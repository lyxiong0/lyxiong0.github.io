<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>CS131 Lecture 1 Course Introduction</title>
    <url>/2018/07/05/Lecture%201%20Course%20Introduction/</url>
    <content><![CDATA[<h1 id="Lecture-1-Course-Introduction"><a href="#Lecture-1-Course-Introduction" class="headerlink" title="Lecture 1 Course Introduction"></a>Lecture 1 Course Introduction</h1><h2 id="什么是计算机视觉？"><a href="#什么是计算机视觉？" class="headerlink" title="什么是计算机视觉？"></a>什么是计算机视觉？</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><ul>
<li><p>从数字图像中提取信息。这些信息可以随着定义不同而改变，可以是空间测量、现实增强等等</p>
</li>
<li><p>构造一个可以理解图片意思并能应用的算法。</p>
<h3 id="一个艰难的问题"><a href="#一个艰难的问题" class="headerlink" title="一个艰难的问题"></a>一个艰难的问题</h3><p>50年来，计算视觉都没有很好的被解决。</p>
</li>
<li><p>可以暴力算出棋类解法，但是无法写诗</p>
</li>
<li><p>目标识别比3D建模更难</p>
</li>
<li><p>计算机视觉难在像素和其意义的不同</p>
<h2 id="理解人类视觉"><a href="#理解人类视觉" class="headerlink" title="理解人类视觉"></a>理解人类视觉</h2><h3 id="视觉定义"><a href="#视觉定义" class="headerlink" title="视觉定义"></a>视觉定义</h3></li>
<li><p>一个可以提取尽可能多的信息的传感器（眼睛、相机）。这方面相机优于人眼，因为可以通过技术看到更远的地方。</p>
</li>
<li><p>处理器需要处理信息并提取其中的含义。这部分计算机视觉仍然落后于人类。</p>
<h3 id="人类视觉系统"><a href="#人类视觉系统" class="headerlink" title="人类视觉系统"></a>人类视觉系统</h3><p>1962年，科学家发现猫的一些视觉神经只有在特殊的线条位置、特别的角度会产生反映。由此引发了对人类视觉的研究。</p>
<h3 id="人类视觉有多强？"><a href="#人类视觉有多强？" class="headerlink" title="人类视觉有多强？"></a>人类视觉有多强？</h3></li>
<li><p>速度<br>人类视觉系统效率极高，下图是人类对动物&#x2F;非动物图片反映时间</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/1.1.png"></p>
</li>
<li><p>错觉<br>人类视觉会因为专注于图片的重要部分而忽视不相关的小细节，如果信号十分接近背景，那就十分难以从图片的相关部分中检测和分割出来。</p>
</li>
<li><p>环境（背景知识）<br>人类依赖先前的知识来识别图片的线索（图片的重点、特殊位置会出现什么），这是计算机视觉难以做到的。环境也能帮助大脑补偿阴影中的颜色，但是有时候环境也会愚弄人类。</p>
<h3 id="从大自然中学习"><a href="#从大自然中学习" class="headerlink" title="从大自然中学习"></a>从大自然中学习</h3><p>计算机视觉不是完全模仿人脑，然而，神经学家希望能深入了解视觉、语言和其他形式的智力背后的概念。 </p>
<h2 id="从图片中提取信息"><a href="#从图片中提取信息" class="headerlink" title="从图片中提取信息"></a>从图片中提取信息</h2><p>可以提取两类信息：度量值、语义信息</p>
<h3 id="度量设备"><a href="#度量设备" class="headerlink" title="度量设备"></a>度量设备</h3><p>自动驾驶到未知地点需要扫描周围环境确定最佳路径，这时计算机视觉就可以作为度量设备测量环境并创造环境地图。立体摄像头通过三角测量提供深度信息，像眼睛一样。如果我们将视角点提高到包含物体所有面，我们就可以创造物体的3D表面，甚至可以通过Google图片重构一个纪念碑的3D模型。同时，计算机视觉还可以帮助机器理解物品的3D几何结构，以便于机器找到好的把握位置。</p>
<h3 id="语义信息来源"><a href="#语义信息来源" class="headerlink" title="语义信息来源"></a>语义信息来源</h3><p>在度量信息之上还包含着密集的语义信息。我们可以标记图片中的各种物体，例如：整个风景、人、动作、姿态、脸等等。医学图片也有很多语义信息，例如：通过皮肤细胞的图片来判断是否有癌症。</p>
<h2 id="计算机视觉的应用"><a href="#计算机视觉的应用" class="headerlink" title="计算机视觉的应用"></a>计算机视觉的应用</h2><p>以下是一份不完全的计算机视觉应用列表</p>
</li>
<li><p>特效：将人类演员的动作表情复制到动画人物上。我们需要检测3D空间内演员脸上标记的准确位置，然后将它们重构到人物上，例如：阿凡达。</p>
</li>
<li><p>3D城市模型：用于将无人机拍到的照片整合到一起，创建城市的3D模型。</p>
</li>
<li><p>风景识别：识别图片的拍摄地点。</p>
</li>
<li><p>面部检测：可以帮助相机识别并专注于人脸，拍出更好的照片。</p>
</li>
<li><p>光学字符检测：用于读取邮政编码之类，最老的应用之一。</p>
</li>
<li><p>移动视觉搜索：加快以图搜图的速度。</p>
</li>
<li><p>自动驾驶</p>
</li>
<li><p>自助收银</p>
</li>
<li><p>基于视觉的互动：Microsoft’s Kinect 和 任天堂的Wii</p>
</li>
<li><p>增强现实： AR(Augmented Reality)</p>
</li>
<li><p>虚拟显示：VR(Virtual Reality)</p>
</li>
</ul>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>CS131</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131 Lecture 10 Semantic Segmentation and Clustering</title>
    <url>/2018/08/11/Lecture%2010%20Semantic%20Segmentation%20and%20Clustering/</url>
    <content><![CDATA[<h1 id="Lecture-10-Semantic-Segmentation-and-Clustering"><a href="#Lecture-10-Semantic-Segmentation-and-Clustering" class="headerlink" title="Lecture 10 Semantic Segmentation and Clustering"></a>Lecture 10 Semantic Segmentation and Clustering</h1><h2 id="Clustering-and-Segmentation"><a href="#Clustering-and-Segmentation" class="headerlink" title="Clustering and Segmentation"></a>Clustering and Segmentation</h2><p>图片分割目的是检测相似和应该在一起的图片区域或像素组。有多种相似度测量方法，以下是一个例子，将属于不同物体的像素分割。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/10.1.PNG"></p>
<p>图像分割可以通过检测像素组，将图像分割为独立的多个物体。这项功能除了能直接运用于物体检测外，还能提高后面的图像处理过程的效率。</p>
<h2 id="Gestalt-School-and-Factors"><a href="#Gestalt-School-and-Factors" class="headerlink" title="Gestalt School and Factors"></a>Gestalt School and Factors</h2><p>Gestalt理论认为总体大于其部分之和，各部分之间的联系可以产生新的性质与特征。这个理论定义了Gestalt因子，用于定义图像中的组。以下是Gestalt因子的一个例子。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/10.2.PNG"></p>
<p>例二。左图的视觉内容看起来没有意义，但是加上灰线条后，右图提供了关于像素分组和图像内容的视觉线索。现在可以看出原图是一些被遮挡的数字9。这是一个通过遮挡反映连续性的例子，灰线条让我们的大脑黑色像素不是分离的，进而识别出数字。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/10.3.PNG"></p>
<p>例三。图中可以看到两张人脸或者一个花瓶，取决于视角不同。这种内容变化来自于我们将物体识别为前景还是背景。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/10.4.PNG"></p>
<h2 id="Agglomerative-Clustering"><a href="#Agglomerative-Clustering" class="headerlink" title="Agglomerative Clustering"></a>Agglomerative Clustering</h2><p>聚类是一种非监督学习：多个数据点$x_1,…,x_n\in R^D$，在不知道正确标签&#x2F;类别的情况下，将它们组合成一类。凝聚聚类(Agglomerative Clustering)是聚类的一种常用算法。</p>
<p>凝聚聚类的基本思想是通过检测点之间的相似度，决定这些点如何以明智的方式组合。首先，我们需要决定如何检测相似度。</p>
<h3 id="Distance-Measures"><a href="#Distance-Measures" class="headerlink" title="Distance Measures"></a>Distance Measures</h3><p>有许多的距离公式，但很难决定什么公式能形成好的距离矩阵。所以我们只列出两个标准、经过研究的距离矩阵。</p>
<h4 id="Euclidean-Distance"><a href="#Euclidean-Distance" class="headerlink" title="Euclidean Distance"></a>Euclidean Distance</h4><p>欧几里德距离考虑两个点$x,x’$的角度和大小来计算距离。<br>$$<br>sim(x,x’)&#x3D;x^Tx’<br>$$<br>这种距离测量没有将矢量正规化，所以它们的大小是相似度计算的一个因素。</p>
<h4 id="Cosine-Similarity-Measure"><a href="#Cosine-Similarity-Measure" class="headerlink" title="Cosine Similarity Measure"></a>Cosine Similarity Measure</h4><p>这种距离计算只考虑两个点之间的角度。注意，和欧几里德距离不同，余弦相似度测量只体现相似度，而不体现距离。且点和自身之间的余弦相似度等于1.<br>$$<br>\begin{align}<br>sim(x,x’)&amp;&#x3D;cos(\theta)\<br>&amp;&#x3D;\frac{x^Tx’}{||x||·||x’||}\<br>&amp;&#x3D;\frac{x^Tx’}{\sqrt{x^Tx}\sqrt{x’^Tx’}}<br>\end{align}<br>$$<br>根据矢量大小的划分导致距离矩阵的正规化，并且保证了测量只取决于两个物体间的角度。</p>
<h3 id="Desirable-Clustering-Properties"><a href="#Desirable-Clustering-Properties" class="headerlink" title="Desirable Clustering Properties"></a>Desirable Clustering Properties</h3><p>当我们选择特定的聚类算法时，需要考虑以下几个性质：</p>
<ol>
<li>可拓展性 - 在计算能力和容量方面</li>
<li>不同的数据类型 - 算法需要支持在$R^d$上的任意数据</li>
<li>输入参数 - 算法的参数调整不能太难。当算法不依赖于我们对数据的精确了解时，会更有用。</li>
<li>可说明性 - 我们要能解释结果。</li>
<li>约束性 - 算法需要有效运用事先设定的约束（例如，我们知道两个点属于或不属于一类）。</li>
</ol>
<h3 id="Agglomerative-Clustering-Implementation"><a href="#Agglomerative-Clustering-Implementation" class="headerlink" title="Agglomerative Clustering Implementation"></a>Agglomerative Clustering Implementation</h3><p>凝聚聚类通过将更近的点分组在一起来计算数据点之间的相似度。新形成的组又可以进一步和靠近它的组合并。这种迭代过程持续至只剩下一个组。这种方式形成了一个层次，最好用树状图(dendrogram)来观察。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/10.5.PNG"></p>
<p>上图第一张显示了所有点，第2-5张显示了聚类算法的步骤：第2步将两个红点聚类，第三步将两个绿点聚类，第四部将绿点集群和附近的蓝点聚类成黄点，最后黄点组和红点组聚类。第六张是最后的树状图。</p>
<h4 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h4><ol>
<li>初始化每个点，作为单独的集群</li>
<li>找到一对最接近的集群</li>
<li>合并这对接近的集群，成为一个父集群</li>
<li>重复步骤2、3，直到剩下一个集群</li>
</ol>
<h4 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h4><p>虽然凝聚聚类很有效，但是当实现它的时候，需要考虑到许多问题。例如：</p>
<ol>
<li><p>我们如何定义集群之间的相似度？我们怎么测量集群之间的距离？</p>
<p>集群之间的距离有多种计算方式：点之间的平均距离、集群中点之间的最小距离、集群中点之间的最大距离。集群距离计算方式对结果有极大的影响。</p>
</li>
<li><p>我们需要选择多少集群？</p>
<p>我们可以通过距离阈值来决定我们需要多少集群。另外，我们可以在树状图的不同层次上水平裁剪，得到我们想要的集群数量。</p>
</li>
</ol>
<h3 id="Different-measures-of-nearest-clusters"><a href="#Different-measures-of-nearest-clusters" class="headerlink" title="Different measures of nearest clusters"></a>Different measures of nearest clusters</h3><p>当我们分割数据集时，有三个主要的模型可以用来决定集群中点之间的距离：</p>
<ol>
<li><p>Single link<br>$$<br>d(C_i,C_j)&#x3D;\min_{x\in C_i,x’\in C_j}d(x,x’)<br>$$<br>通过单链接，我们利用两个集群中点之间的最小距离来实现聚类。</p>
<p>这种方法被称为最小生成树。</p>
<p>我们可以在集群之间的距离超过阈值时停止聚类。这种算法通常生成长、瘦的类（因为我们只考虑集群中有最小距离的点，所以很容易将距离较远的点连接到同一集群中）。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/10.6.PNG"></p>
</li>
<li><p>Complete link<br>$$<br>d(C_i,C_j)&#x3D;\max_{x\in C_i,x’\in C_j}d(x,x’)<br>$$<br>通过完整链接，我们利用两个集群中点之间的最大距离来实现聚类。</p>
<p>这种算法通常生成紧凑、密集的集群（因为它偏向把所有点放在一起）。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/10.7.PNG"></p>
</li>
<li><p>Average link<br>$$<br>d(C_i,C_j)&#x3D;\frac{\sum_{x\in C_i,x’\in C_j}d(x,x’)}{|C_i|·|C_j|}<br>$$<br>通过平均链接，我们利用两个集群中点之间的平均距离来实现聚类。</p>
<p>这种模型对噪音具有强健性，因为距离不像单连接和完整连接一样，只取决于单独的一对点。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/10.8.PNG"></p>
</li>
</ol>
<h3 id="Agglomerative-clustering-conclusions"><a href="#Agglomerative-clustering-conclusions" class="headerlink" title="Agglomerative clustering conclusions"></a>Agglomerative clustering conclusions</h3><p>优点：</p>
<ul>
<li>易于实现与应用</li>
<li>集群形状适应数据集</li>
<li>形成一个集群层次</li>
<li>初始化时不需要指定集群数目。</li>
</ul>
<p>缺点：</p>
<ul>
<li>可能返回不平衡的聚群</li>
<li>必须指定阈值</li>
<li>因为需要时间$O(n^3)$，所以不能很好的测量</li>
<li>贪婪合并会被卡在局部最小值</li>
</ul>
<h2 id="K-Means-Clustering"><a href="#K-Means-Clustering" class="headerlink" title="K-Means Clustering"></a>K-Means Clustering</h2><p>K-means聚类：确定一定数量固定的集群中心，将每个点标记到距离最近的集群中。k-means聚类和凝聚聚类最大的不同在于，k-means要求输入集群数目。</p>
<h3 id="Image-Segmentation-Example"><a href="#Image-Segmentation-Example" class="headerlink" title="Image Segmentation Example"></a>Image Segmentation Example</h3><p>下图上方可以简单的通过像素密度不同分割，但是下方因为包含噪音，所以我们要用K-means分割。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/10.9.PNG"></p>
<p>用K-means的目的是找到三个集群中心作为强度代表，并将每个像素标记到最接近的集群。最佳的集群中心要能够最小化所有点和最近集群中心$c_i$之间的距离平方和(Sum of Square Distance, SSD)：<br>$$<br>SSD&#x3D;\sum_{i\in clusters} \sum_{x\in cluster_i}(x-c_i)^2<br>$$<br>当我们用k-means处理数据集时，我们的目标是最小化每个集群中所有数据点的方差。我们想用一定数目的集群提供尽可能多的信息。可以用以下方程描述：</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/10.10.PNG"></p>
<h3 id="Algorithm-1"><a href="#Algorithm-1" class="headerlink" title="Algorithm"></a>Algorithm</h3><p>我们从随机初始化k个集群开始。接着我们运行一个迭代过程，该过程会计算集群成员和集群中心，直到达到最大迭代次数或集群中心值收敛。该过程如下：</p>
<ol>
<li><p>初始化($t&#x3D;0$)：集群中心$c_1,…,c_K$。</p>
<ul>
<li>通常这些中心都是在数据点中随机选择的。</li>
<li>或者对$k$进行贪婪选择，最小化剩余。</li>
</ul>
</li>
<li><p>计算$\delta^t$：将每个点聚集到最近的中心点。像在凝聚聚类中一样，我们可以用欧几里德距离或者余弦距离来计算。<br>$$<br>\delta^t&#x3D;\min_\delta \frac{1}{N}\sum^N_j \sum^K_i \delta^{t-1}_{ij}(c^{t-1}_i-x_j)^2<br>$$</p>
</li>
<li><p>计算$c^t$：更新集群中心为每个集群的均值点。<br>$$<br>c^t&#x3D;\min_c \frac{1}{N}\sum^N_j \sum^K_i \delta^{t-1}_{ij}(c^{t-1}_i-x_j)^2<br>$$</p>
</li>
<li><p>$t&#x3D;t+1$，重复2-3，直到集群中心点$c^t$停止改变（收敛）或者算法达到最大迭代次数。</p>
</li>
</ol>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/10.11.PNG"></p>
<h3 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h3><p>每次运行，k-means聚集到一个局部最小值。另外，因为中心点是随机初始化的，所以每次运行算法可能会返回不同的结果。因此，应该多次运行算法并选择最好的结果。评估结果的标准是最小化集群的SSD或者每个集群的方差。K-means在球形数据下最有效。</p>
<h3 id="Segmentation-as-Clustering"><a href="#Segmentation-as-Clustering" class="headerlink" title="Segmentation as Clustering"></a>Segmentation as Clustering</h3><p>针对单独的颜色强度（一个颜色对应一个物体），K-means是很有效的。但是像下图，就需要我们定义一个特征空间，选择可以作为输入的像素特征。特征空间的选择直接影响到点之间的相似度测量，也有利于生成区别较大、易于分辨的集群。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/10.12.PNG"></p>
<p>除了像素强度之外，也可以用RGB颜色、纹理、像素位置创建特征空间。其中，纹理可以使用经过特定过滤器过滤后的像素相似度衡量。位置特征包括图中像素坐标。像素的强度和位置都可以一起基于相似度和邻近度聚集像素。</p>
<h3 id="K-Means"><a href="#K-Means" class="headerlink" title="K-Means++"></a>K-Means++</h3><p>K-means优势在于易于实现且速度快，但是精度不高。通过增加一个变量来选择k-means算法的随机种子，可以回避坏的聚类。K-means++算法如下：</p>
<ol>
<li>随机从数据点中选择一个起始中心</li>
<li>计算距离$D(X)$，即每个点$x$和被选择中心之间的距离。通过一个加权概率分布，基于与$D(x)^2$成正比的概率（$x$对总误差的贡献），选择一个新的点作为新中心点。</li>
<li>重复以上步骤，直到选择$k$个中心点。接着用这些中心点作为初始化种子，运行k-means算法。</li>
</ol>
<p>K-means++期望误差&#x3D;$O(logK)$。</p>
<h3 id="Evaluation-of-clusters"><a href="#Evaluation-of-clusters" class="headerlink" title="Evaluation of clusters"></a>Evaluation of clusters</h3><p>聚类结果可以通过多种方法评估。例如，</p>
<ul>
<li>内部评价测量，给出一个单一的质量分数</li>
<li>外部评估，将聚类结果和已有的正确分类进行比较</li>
<li>基于遗传评估：从集群中重建点的效果如何，或者是判断集群中心是否能很好的体现数据。</li>
<li>区别方法：评估集群对应标签的效果如何。集群是否能够合理的分离物体。这项测量只能在监督学习下进行。</li>
</ul>
<h3 id="Pros-amp-Cons"><a href="#Pros-amp-Cons" class="headerlink" title="Pros &amp; Cons"></a>Pros &amp; Cons</h3><p><strong>Pros</strong></p>
<ul>
<li>易于实现</li>
<li>在低维数据下运行较快</li>
<li>能很好的体现数据（聚类中心最小化条件方差）</li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li>不能识别异常值</li>
<li>需要确定$k$的值</li>
<li>不能处理有不同大小和密度的非球形数据</li>
<li>只能在有中心概念的数据上运用</li>
<li>不能保证达到全局最优</li>
</ul>
<p>为了选出$k$的值，可以画出关于k值的目标方程。在目标方程有剧变的位置就是应该选择的$k$值。</p>
<h2 id="Mean-shift-Clustering"><a href="#Mean-shift-Clustering" class="headerlink" title="Mean-shift Clustering"></a>Mean-shift Clustering</h2><p>均值-偏移聚类目的是找到特征空间中最密集的区域。步骤如下：</p>
<ol>
<li>初始化随机种子，以及窗$W$</li>
<li>计算$W$的中心重力(“mean”)：$\sum_{x\in W}xH(x)$</li>
<li>将搜索窗移动到“mean”</li>
<li>重复步骤，直到收敛（窗不再改变）</li>
</ol>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/10.13.PNG"></p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>CS131</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131 Lecture 11 Object Recognition</title>
    <url>/2018/08/13/Lecture%2011%20Object%20Recognition/</url>
    <content><![CDATA[<h1 id="Lecture-11-Object-Recognition"><a href="#Lecture-11-Object-Recognition" class="headerlink" title="Lecture 11 Object Recognition"></a>Lecture 11 Object Recognition</h1><h2 id="Mean-Shift"><a href="#Mean-Shift" class="headerlink" title="Mean-Shift"></a>Mean-Shift</h2><p>一种通过分析密度方程检测它局部最大值的模式寻找(mode-seeking)技术。</p>
<h3 id="Optimizations"><a href="#Optimizations" class="headerlink" title="Optimizations"></a>Optimizations</h3><p>为了提高算法速度，需要平移窗或减少窗的数量。使用吸引盆(basin of attraction)方法达成。</p>
<p><strong>Parallelization</strong> 移动不同窗口的计算是独立的，可以被分割给多个不同的处理器平行运算。因此可以在没有损失精度的前提下，平行计算提高速度。</p>
<p><strong>Basin of Attraction</strong> 因为靠近窗移动路径和停止位置的点很可能被包含进同一集群，所以我们在初始化时就加入这些点，以减少计算时间。</p>
<p>方法：</p>
<ol>
<li><p>在移动完窗的停止点，将半径$r$内的所有点加入。因为停止点密度较大。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/11.1.png"></p>
</li>
<li><p>将窗移位路径上，所有半径在$c,c\leq r$内的点加入。因为窗是往密度大的地区移动的。</p>
</li>
</ol>
<p>需要权衡$r$和$c$值，因为值越小，越少附近点被加入，计算量增加。但值小也减少了错误率，提高了精度。</p>
<h3 id="Technical-Details"><a href="#Technical-Details" class="headerlink" title="Technical Details"></a>Technical Details</h3><p>为了正确移动窗，我们需要确定一个有最高密度的邻近地区，来计算移动向量。这里我们使用多变量核密度估计(the multivariate kernel density estimate)，是估计一个随机变量的概率密度函数的方法。</p>
<p>给定$n$个数据点$x\in R^d$，使用径向对称核(radially symmetric kernel)$K(x)$，多变量核密度估计被定义为：<br>$$<br>\hat{f}<em>K&#x3D;\frac{1}{nh^d}\sum</em>{i&#x3D;1}^nK(\frac{x-x_i}{h})<br>$$<br>其中，$h$是带宽(bandwidth)参数，定义了核的半径。径向对称核$K(x)$定义为<br>$$<br>K(x)&#x3D;c_kk(||x||^2)<br>$$<br>其中，$c_k$代表标准化常数。</p>
<p>选择合适的$h$对精确密度估计十分重要。$h$过小导致半径过小，使得数据受到噪音影响。$h$过大导致包括太多偏远点，集群数减少。</p>
<p>多变量核密度估计的导数为<br>$$<br>\nabla\hat{f}(x)&#x3D;\frac{2c_{k,d}}{nh^{d+2}}[\sum_{i&#x3D;1}^ng(||\frac{x-x_i}{h}||^2)][\frac{\sum_{i&#x3D;1}^{n}x_ig(||\frac{x-x_i}{h}||^2)}{\sum_{i&#x3D;1}^{n}g(||\frac{x-x_i}{h}||^2)}-x]<br>$$<br>其中$g(x)&#x3D;-K’(x)$，代表被选择的核剖面的导数。</p>
<p>第一项$\frac{2c_{k,d}}{nh^{d+2}}[\sum_{i&#x3D;1}^ng(||\frac{x-x_i}{h}||^2)]$，与$x$的密度估计成正比。第二项$[\frac{\sum_{i&#x3D;1}^{n}x_ig(||\frac{x-x_i}{h}||^2)}{\sum_{i&#x3D;1}^{n}g(||\frac{x-x_i}{h}||^2)}-x]$，是指向最大密度的均值-偏移矢量。</p>
<h3 id="Mean-shift-Procedure"><a href="#Mean-shift-Procedure" class="headerlink" title="Mean-shift Procedure"></a>Mean-shift Procedure</h3><p>对于给定的点$x_t$，执行以下步骤找到集群中心。</p>
<ol>
<li><p>计算均值-偏移矢量$m$（公式(13)的第二项）：<br>$$<br>m&#x3D;[\frac{\sum_{i&#x3D;1}^{n}x_ig(||\frac{x-x_i}{h}||^2)}{\sum_{i&#x3D;1}^{n}g(||\frac{x-x_i}{h}||^2)}-x]<br>$$</p>
</li>
<li><p>用均值-偏移矢量转换密度窗：<br>$$<br>x_i^{t+1}&#x3D;x^t_i+m(x_i^t)<br>$$</p>
</li>
<li><p>重复以上两步，直到收敛<br>$$<br>\nabla f(x_i)&#x3D;0<br>$$</p>
</li>
</ol>
<h3 id="Kernel-Functions"><a href="#Kernel-Functions" class="headerlink" title="Kernel Functions"></a>Kernel Functions</h3><p>$K(x)$是一个非负函数，在$x$的所有值上相加为1。这些要求保证了核密度评估会产生概率密度函数。</p>
<p>常用的核函数有：</p>
<ul>
<li><p>均匀（矩形）<br>$$<br>K(x)&#x3D; \left{<br>         \begin{array}{lr}<br>        \frac12, &amp;|x|\leq 1\<br>        0, &amp;otherwise<br>         \end{array}<br>\right.<br>$$</p>
</li>
<li><p>高斯<br>$$<br>K(x)&#x3D; \frac1{\sqrt{2\pi}}e^{-\frac1 2u^2}<br>$$</p>
</li>
<li><p>Epanechnikov（抛物线）<br>$$<br>K(x)&#x3D; \left{<br>         \begin{array}{lr}<br>        \frac34(1-x^2), &amp;|x|\leq 1\<br>        0, &amp;otherwise<br>         \end{array}<br>\right.<br>$$</p>
</li>
</ul>
<h3 id="Mean-Shift-Conclusions"><a href="#Mean-Shift-Conclusions" class="headerlink" title="Mean-Shift Conclusions"></a>Mean-Shift Conclusions</h3><p>优点：</p>
<ul>
<li>非常普遍，可以独立应用。</li>
<li>均值-偏移对数据集群形状无要求</li>
<li>只需要一个因子定义窗的大小。如果需要应用吸引盆，则加上$r$和$c$因子。</li>
<li>找到可变数量的模式。分布函数的模式是局部最大值，这些局部最大值的位置就是集群中心。</li>
<li>对异常值有强健性。如果异常值与其他点的距离大于窗大小，均值-偏移不会将异常值强制加入已有的集群。</li>
</ul>
<p>缺点：</p>
<ul>
<li>输出取决于窗大小，但不容易定义合适的窗大小。</li>
<li>计算量相对来说较大</li>
<li>不能很好的拓展特征空间的维度</li>
</ul>
<h2 id="Object-recognition"><a href="#Object-recognition" class="headerlink" title="Object recognition"></a>Object recognition</h2><h2 id="Object-recognition-tasks"><a href="#Object-recognition-tasks" class="headerlink" title="Object recognition tasks"></a>Object recognition tasks</h2><p>物体识别可以被分为数个不同的视觉识别任务</p>
<p><strong>Classification</strong> 分类任务是让计算机基于物品目录给物体标记标签。专注于“图像内是否有特定物品？”</p>
<p><strong>Image search</strong> 搜索包含特定物体的图像</p>
<p><strong>Organizing photo collections</strong> 物体识别可以基于图像位置、活动相似性、人物等等来帮助组织图像。</p>
<p><strong>Detection</strong> 专注于“这个特定物体在图像的哪个位置？”传统探测方法只把物体在图像上框出。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/11.7.png"></p>
<p>但结合图像分割技术，物体也可被更准确的选出（即标出物体轮廓），被称作准确定位(accurate localization)。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/11.8.png"></p>
<p>检测也可以用于寻找几何和语义特征。例如：物体之间的距离；图像拍摄物体的角度</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/11.9.png"></p>
<p><strong>Single Instance Recognition</strong> 单一实例识别寻求识别图像上特定的物体或地标，而不是通常的物体大类。例如：寻找金毛犬，而不只是识别出狗；寻找某品牌的麦片</p>
<p><strong>Activity or event recognition</strong> 活动或事件识别用于检测图中发生了什么事，例如：图中场景是否发生在婚礼上。</p>
<h3 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h3><p><strong>Category numbers</strong> 目前最好的识别方法只能分类1000种物体，检测200种物体，远低于人类识别物体数量。</p>
<p><strong>Viewpoint variation</strong> 看物体视角不同</p>
<p><strong>Illumination</strong> 不同程度的光照，会导致阴影、物体细节被遮挡</p>
<p><strong>Scale</strong> 一个类下的物体大小多变，不能只识别一个大小。一种解决方法是取得包含多种大小变化的数据集。</p>
<p><strong>Deformation</strong> 同一物体可能有各种看起来不同的形态。例如：人的不同姿势。</p>
<p><strong>Occlusion</strong> 物体可能被遮挡，导致部分几何特征被隐藏。</p>
<p><strong>Background cluster</strong> 由于背景相似，导致物体难以被识别，或者导致物体与同类看起来很不一样。</p>
<p><strong>Intra-class variation</strong> 一类物体可能有很不同的形状。例如：沙发、板凳都属于椅子类。</p>
<h2 id="K-nearest-neighbors"><a href="#K-nearest-neighbors" class="headerlink" title="K-nearest neighbors"></a>K-nearest neighbors</h2><h3 id="Supervised-learning"><a href="#Supervised-learning" class="headerlink" title="Supervised learning"></a>Supervised learning</h3><p>目标：用已有的数据集找到以下方程：<br>$$<br>y&#x3D;f(x)<br>$$<br>其中，$y$是输出，$f$是预测函数，$x$是图像的特征集。</p>
<p>监督学习分训练与测试两个阶段。在第一阶段，使$f$与训练集${(x_1,y_1}$相符。$f$可以用最小化预测误差来评估，即$y$与$f$之间的差异。</p>
<p>在第二阶段，我们用测试集评估方程$y&#x3D;f(x)$。</p>
<p>任意决策规则将输入空间划分为由决策边界分割的决策区域。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/11.10.png"></p>
<h3 id="Nearest-neighbor-classifier"><a href="#Nearest-neighbor-classifier" class="headerlink" title="Nearest neighbor classifier"></a>Nearest neighbor classifier</h3><p>最近邻分类器是基于最近的邻居给物体分类的算法。将测试点标记为最近邻居的标签。</p>
<p>最近邻居通过特征间的欧几里德距离找到。设$X^n$和$X^m$分别是训练集中的第$n$和第$m$个数据点，则距离方程为：<br>$$<br>Dist(X^n,X^m)&#x3D;||X^n-X^m||^2&#x3D;\sqrt{\sum_i (X^n_i-X^m_i)^2}<br>$$<br>最近邻分类器的定义允许在训练集中的每个数据点周围形成复杂的决策边界。</p>
<h3 id="K-nearest-neighbors-classifier"><a href="#K-nearest-neighbors-classifier" class="headerlink" title="K-nearest neighbors classifier"></a>K-nearest neighbors classifier</h3><p>K-最近邻分类器：计算K个最近的邻居，然后通过在最近邻集上计算一个分数，来标记新物体。常用的方法是用大多数邻居属于的类标记新物体。启发法被用于打破联系，并根据最有效的方法进行评估。</p>
<p>例子：下图中”+”为待标记的数据，根据绿圈内的五个邻居占大多数的，这个点将被标记为绿”O”。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/11.2.png"></p>
<h3 id="Pros-of-using-k-nearest-neighbors"><a href="#Pros-of-using-k-nearest-neighbors" class="headerlink" title="Pros of using k-nearest neighbors"></a>Pros of using k-nearest neighbors</h3><ul>
<li>K-NN算法简单，值得作为第一个尝试的模型。</li>
<li>K-NN决策边界灵活</li>
<li>当有无限样本时，1-NN被证明误差最多是贝叶斯最优误差的两倍。</li>
</ul>
<h3 id="Problems-with-K-NN"><a href="#Problems-with-K-NN" class="headerlink" title="Problems with K-NN"></a>Problems with K-NN</h3><h4 id="Choosing-the-value-of-K"><a href="#Choosing-the-value-of-K" class="headerlink" title="Choosing the value of K"></a>Choosing the value of K</h4><p>若$K​$值过小，算法会对噪音点过敏感。如果$K​$值过大，邻居会包括过多的其他类点，精度下降。随着$K​$的增加，决策边界也会更加平滑。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/11.11.png"></p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/11.3.png"></p>
<p><strong>Solution</strong>: 交叉验证</p>
<p>从训练集中分离出交叉验证集。用不同的$K$在训练集上训练，在交叉验证集上验证。最后选择在交叉验证集上精度最高的$K$，用于测试集。</p>
<h4 id="Euclidean-measurement"><a href="#Euclidean-measurement" class="headerlink" title="Euclidean measurement"></a>Euclidean measurement</h4><p>欧几里德测量可能会得出反直觉的结果。例如：</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/11.4.png"></p>
<p><strong>Solution</strong> 标准化</p>
<p>将向量标准化至单位长度。</p>
<h4 id="Curse-of-dimensionality"><a href="#Curse-of-dimensionality" class="headerlink" title="Curse of dimensionality"></a>Curse of dimensionality</h4><p>当维数增长，K-NN算法会越来越慢。这代表我们需要更多的实例用于训练。</p>
<p>目前尚未有最佳的方法解决维数灾难。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/11.5.png"></p>
<p><strong>Problem</strong>: 假设有5000个点均匀分布在单位超立方体中，我们想要应用5-NN算法，假设我们的查询点在原点：</p>
<ul>
<li>在1维，我们平均需要走过$\frac{5}{5000}&#x3D;0.001$距离，来捕捉5-NN</li>
<li>在2维，我们平均需要走过$\sqrt{0.001}$距离，得到一个包含0.001体积的正方形。</li>
<li>在$d$维，我们需要$(0.001)^{\frac{1}{d}}$</li>
</ul>
<p><strong>Note</strong>: K-NN只是众多分类器中的一种。</p>
<h3 id="Bias-variance-trade-off"><a href="#Bias-variance-trade-off" class="headerlink" title="Bias-variance trade-off"></a>Bias-variance trade-off</h3><p>减少泛化误差的关键是找到正确数量&#x2F;类型的参数。泛化误差有两种：偏差(bias)、方差(variance)。偏差：在训练集上平均模型与训练模型有多少不同？方差：不同训练集的模型估计有多少不同？</p>
<p>我们需要在方差与偏差间找到平衡点。太少参数的模型因为高偏差（不灵活）；太多参数的模型因为高方差（对样本过于敏感），而导致精确低。非正确拟合类型如下：</p>
<p><strong>Underfitting</strong>: 模型过于“简单”，无法表示所有相关特征</p>
<ul>
<li>高偏差低方差</li>
<li>高训练、高测试误差</li>
</ul>
<p><strong>Overfitting</strong>: 模型过于“复杂”，对数据中不相关的特征（噪音）进行了拟合。</p>
<ul>
<li>低偏差高方差</li>
<li>高训练、高测试误差</li>
</ul>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/11.6.png"></p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>CS131</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131 Lecture 13 Face Recognition and LDA</title>
    <url>/2018/08/16/Lecture%2013%20Face%20Recognition%20and%20LDA/</url>
    <content><![CDATA[<p>#Lecture 13 Face Recognition and LDA </p>
<h2 id="Introduction-to-Facial-Recognition"><a href="#Introduction-to-Facial-Recognition" class="headerlink" title="Introduction to Facial Recognition"></a>Introduction to Facial Recognition</h2><h3 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h3><ul>
<li>数字摄影：聚焦人脸</li>
<li>监视器</li>
<li>组织相册：将相同的人放在同一相册</li>
<li>人物追踪</li>
<li>动作和表情：构建基于心情互动的智能设备</li>
<li>安全和战争：检测特殊人物、敌军</li>
<li>电话会议：提供正在视频的人的信息</li>
</ul>
<h3 id="A-Key-Distinction-Detection-vs-Recognition"><a href="#A-Key-Distinction-Detection-vs-Recognition" class="headerlink" title="A Key Distinction: Detection vs. Recognition"></a>A Key Distinction: Detection vs. Recognition</h3><p><strong>Face Detection</strong>: 检测照片是否包含人脸和照片上人脸的位置</p>
<p><strong>Face Recognition</strong>: 检测照片包含<em>谁</em>的脸</p>
<h3 id="Space-of-Faces"><a href="#Space-of-Faces" class="headerlink" title="Space of Faces"></a>Space of Faces</h3><p>如果我们考虑一张大小为$m\times n$灰度图，这张图可以用一个在高维空间$R^{mn}$上的点表示。一张图不止包含脸，所以脸只占了相对小的子空间。我们的任务就是对脸的子空间建模。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/13.1.png"></p>
<p>我们计算出K维子空间，这样数据点在子空间的投影在所有子空间中都有最大的方差。这个低维子空间捕捉了脸部关键样貌特点。    </p>
<h2 id="The-Eigenfaces-Algorithm"><a href="#The-Eigenfaces-Algorithm" class="headerlink" title="The Eigenfaces Algorithm"></a>The Eigenfaces Algorithm</h2><h3 id="Key-Ideas-and-Assumptions"><a href="#Key-Ideas-and-Assumptions" class="headerlink" title="Key Ideas and Assumptions"></a>Key Ideas and Assumptions</h3><ul>
<li>假设大多数脸部图像都位于低维子空间，由最大方差的前$k$个方向决定</li>
<li>用PCA检测旋转子空间的向量或特征面</li>
<li>将所有数据集中的脸部图像表现为特征面的线性组合，特征面被定义为SVD分解的主成份</li>
</ul>
<h4 id="What-are-eigenfaces"><a href="#What-are-eigenfaces" class="headerlink" title="What are eigenfaces?"></a>What are eigenfaces?</h4><p>特征面是最大方差方向的特征向量的视觉呈现。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/12.6.png"></p>
<h3 id="Training-Algorithm"><a href="#Training-Algorithm" class="headerlink" title="Training Algorithm"></a>Training Algorithm</h3><ol>
<li><p>排列训练图像：$x_1,…,x_n$</p>
</li>
<li><p>计算平均脸：<br>$$<br>\mu&#x3D;\frac1N \sum x_i<br>$$</p>
</li>
<li><p>计算协方差矩阵：<br>$$<br>\Sigma&#x3D;\frac1N X_cX_c^T<br>$$</p>
</li>
<li><p>用PCA计算协方差矩阵$\Sigma$的特征向量</p>
</li>
<li><p>计算每个训练图像$x_i$的投影<br>$$<br>x_i\rightarrow (x_i^c·\phi_1,x_i^c·\phi_2,…,x_i^c·\phi_k)\equiv(a_1,a_2,…,a_k)<br>$$<br>其中$\phi_i$是第$i$个特征向量。</p>
</li>
<li><p>重构的脸$x_i\approx \mu+a_1·\phi_1+…+a_k·\phi_k$</p>
</li>
</ol>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/12.7.png"></p>
<h4 id="Why-can-we-do-this"><a href="#Why-can-we-do-this" class="headerlink" title="Why can we do this?"></a>Why can we do this?</h4><p>因为特征值（特征向量的方差）随着主成份数目的增加迅速下降</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/12.8.png"></p>
<h4 id="Reconstruction-and-Error"><a href="#Reconstruction-and-Error" class="headerlink" title="Reconstruction and Error"></a>Reconstruction and Error</h4><p>我们只需要前$k$个特征面用于减少维度。特征面越少损失越多，脸部越不明显。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/12.9.png"></p>
<h3 id="Testing-Algorithm"><a href="#Testing-Algorithm" class="headerlink" title="Testing Algorithm"></a>Testing Algorithm</h3><ol>
<li><p>取查询图像$t$</p>
</li>
<li><p>映射至特征向量：<br>$$<br>t\rightarrow((t-\mu)·\phi_1,(t-\mu)·\phi_2,…,(t-\mu)·\phi_k)\equiv(w_1,w_2,..,w_k)<br>$$</p>
</li>
<li><p>比较投影$w$和所有$N$个训练投影。用欧几里德距离和KNN算法输出标签。</p>
</li>
</ol>
<h3 id="Advantages-and-Disadvantages"><a href="#Advantages-and-Disadvantages" class="headerlink" title="Advantages and Disadvantages"></a>Advantages and Disadvantages</h3><p><strong>Advantages</strong></p>
<ul>
<li>这个方法不需要预先知道脸部、表情信息</li>
<li>快速、全局最优</li>
</ul>
<p><strong>Disadvantages</strong></p>
<ul>
<li>要求小心控制的数据<ol>
<li>所有脸部必须集中在框架。否则结果噪音大</li>
<li>图像必须大小相同</li>
<li>对脸的角度敏感</li>
</ol>
</li>
<li>方法不需要预先知识<ol>
<li>脸的类别之间没有差别</li>
<li>PCA不考虑与脸相关的标签。因此它可能将不同的脸对应到相同的子空间，使得分类器难以区别这些脸。</li>
</ol>
</li>
<li>PCA投影在从低维重构上可能是最优的，但在在辨别方面不是最优的。</li>
</ul>
<h3 id="Beyond-Facial-Recognition-Expressions-and-Emotions"><a href="#Beyond-Facial-Recognition-Expressions-and-Emotions" class="headerlink" title="Beyond Facial Recognition: Expressions and Emotions"></a>Beyond Facial Recognition: Expressions and Emotions</h3><p>这项技术也可以用于检测表达和情绪，且算法不改变。</p>
<p>高兴↓</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/12.10.png"></p>
<p>厌恶↓</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/12.11.png"></p>
<h2 id="Linear-Discriminant-Analysis-LDA"><a href="#Linear-Discriminant-Analysis-LDA" class="headerlink" title="Linear Discriminant Analysis (LDA)"></a>Linear Discriminant Analysis (LDA)</h2><h3 id="PCA-vs-LDA"><a href="#PCA-vs-LDA" class="headerlink" title="PCA vs. LDA"></a>PCA vs. LDA</h3><p>PCA与LDA都能减少样本的维度。但是，PCA偏重于重建物体，LDA偏重于分类。LDA会将不同的类相互远离。</p>
<ul>
<li>PCA保持最大方差</li>
<li>LDA找到能够在类之间最大化散射和在类中最小化散射的投影。</li>
</ul>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/13.2.png"></p>
<p>如图，PCA保持了最大方差，并将所有类的点都映射在正斜率方向上，因此难以判别类别。但是，LDA将点映射到负斜率，导致点被映射到接近同类点，与非同类点相反的位置。</p>
<h3 id="General-Idea"><a href="#General-Idea" class="headerlink" title="General Idea"></a>General Idea</h3><p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/13.6.png"></p>
<p>LDA用两个值运行：类间散度、类内散度。类间散度指不同类之间的距离，类内散度指类内点之间的距离。LDA最大化类间散度，最小化类内散度。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/13.3.png"></p>
<h3 id="Mathematical-Formulation-of-LDA-with-2-Variables"><a href="#Mathematical-Formulation-of-LDA-with-2-Variables" class="headerlink" title="Mathematical Formulation of LDA with 2 Variables"></a>Mathematical Formulation of LDA with 2 Variables</h3><p>我们想要找到一个投影$w$在$x\in R^n$空间中映射出0和1的点到一个新的空间$z\in R^m$，例如$z&#x3D;w^Tx$。其中，$m&lt;n$，且投影必须最大化以下公式：<br>$$<br>J(w)&#x3D;\frac{S_B\space when\space projected\space onto\space w}{S_W\space when\space projected\space onto\space w}<br>$$<br>公式中，$S_B$代表类间散度，$S_W$代表类内散度。接下来定义一个代表类内点的平均的变量$\mu_i$：<br>$$<br>\mu_i&#x3D;E_{X|Y}[X|Y&#x3D;i]<br>$$<br>定义变量$\Sigma_i$代表类的协方差矩阵：<br>$$<br>\Sigma_i&#x3D;E_{X|Y}[(X-\mu_i)(X-\mu_i)^T|Y&#x3D;i]<br>$$<br>用以上变量，可以定义$S_B$和$S_W$：<br>$$<br>S_B&#x3D;(\mu_1-\mu_0)^2&#x3D;(\mu_1-\mu_0)(\mu_1-\mu_0)^T\<br>S_W&#x3D;(\Sigma_1+\Sigma_0)<br>$$<br>将变量放回原式$J(w)$可得：<br>$$<br>J(w)&#x3D;\frac{w^T(\mu_1-\mu_0)(\mu_1-\mu_0)^Tw}{w^T(\Sigma_1+\Sigma_0)w}<br>$$<br>我们要最大化$J(w)$，即最大化分子，保持分母为常数：<br>$$<br>\max_{w^T(\Sigma_1+\Sigma_0)w&#x3D;K}w^T(\mu_1-\mu_0)(\mu_1-\mu_0)^Tw<br>$$<br>用拉格朗日乘数法，我们定义拉格朗日算子为：<br>$$<br>L&#x3D;w^TS_Bw-\lambda(w^TS_Ww-K)&#x3D;w^T(S_B-\lambda S_W)w+K<br>$$<br>我们必须最大化$L$对$\lambda$和$w$的值。我们可以通过用其关于$w$的梯度和找到关键点的位置：<br>$$<br>\nabla_wL&#x3D;2(S_B-\lambda S_W)w&#x3D;0<br>$$<br>用这个公式，我们可以得到关键点位置：<br>$$<br>S_Bw&#x3D;\lambda S_Ww<br>$$<br>这是一个广义的特征向量问题。在$S_W^{-1}&#x3D;(\Sigma_1+\Sigma_0)^{-1}$存在的情况下，我们得到：<br>$$<br>S_W^{-1}S_Bw&#x3D;\lambda w<br>$$<br>代入$S_B$得：<br>$$<br>S_W^{-1}(\mu_1-\mu_0)&#x3D;\frac\lambda\alpha w<br>$$<br>$w$的大小并不重要，所以我们可以得到映射$w$：<br>$$<br>w*&#x3D;S_W^{-1}(\mu_1-\mu_0)&#x3D;(\Sigma_1-\Sigma_0)^{-1}(\mu_1-\mu_0)<br>$$</p>
<h3 id="LDA-with-N-Variables-and-C-Classes"><a href="#LDA-with-N-Variables-and-C-Classes" class="headerlink" title="LDA with N Variables and C Classes"></a>LDA with N Variables and C Classes</h3><h4 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h4><p><strong>Variables:</strong></p>
<ul>
<li><p>N个样本：${x_1,…,x_N}$</p>
</li>
<li><p>C个类:${Y_1,Y_2,…,Y_C}$。每一类都有N个样本。</p>
</li>
<li><p>每个类的平均：类$i$的平均为<br>$$<br>\mu_i&#x3D;\frac 1{N_i}\sum_{x_k\in Y_i}x_k<br>$$</p>
</li>
<li><p>所有数据的平均：<br>$$<br>\mu&#x3D;\frac 1N\sum_{k&#x3D;1}^N x_k<br>$$</p>
</li>
</ul>
<p><strong>Scatter Matrices:</strong></p>
<ul>
<li>类$i$散度：$S_i&#x3D;\sum_{x_k\in Y_i}(x_k-\mu_i)(x_k-\mu_i)^T$</li>
<li>类内散度：$S_w&#x3D;\sum^{c}_{i&#x3D;1}S_i$</li>
<li>类间散度：$S_b&#x3D;\sum_{i&#x3D;1}^cN_i(\mu_i-\mu)(\mu_i-\mu)^T$</li>
</ul>
<h4 id="Mathematical-Formulation"><a href="#Mathematical-Formulation" class="headerlink" title="Mathematical Formulation"></a>Mathematical Formulation</h4><p>我们需要一个将所有点从$x\in R^m$映射到$z\in R^n$的投影：<br>$$<br>z&#x3D;w^Tx,x\in R^m,z\in R^n<br>$$<br><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/13.5.png"></p>
<h3 id="Results-Eigenface-PCA-vs-Fisherface-LDA"><a href="#Results-Eigenface-PCA-vs-Fisherface-LDA" class="headerlink" title="Results: Eigenface(PCA) vs. Fisherface(LDA)"></a>Results: Eigenface(PCA) vs. Fisherface(LDA)</h3><p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/13.4.png"></p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>CS131</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131 Lecture 15 Detecting Objects by Parts</title>
    <url>/2018/08/21/Lecture%2015%20Detecting%20Objects%20by%20Parts/</url>
    <content><![CDATA[<h1 id="Lecture-15-Detecting-Objects-by-Parts"><a href="#Lecture-15-Detecting-Objects-by-Parts" class="headerlink" title="Lecture 15 Detecting Objects by Parts"></a>Lecture 15 Detecting Objects by Parts</h1><h2 id="Introduction-to-Object-Detection"><a href="#Introduction-to-Object-Detection" class="headerlink" title="Introduction to Object Detection"></a>Introduction to Object Detection</h2><p>之前我们介绍了物体检测，本章将介绍物体检测和定位物体。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/15.1.png"></p>
<h3 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h3><p>环境改变（光照、视角、物体变形）使得同一类的物体变得不同，以至于难以正确检测并分类。另外，此处介绍的算法只适用于2D空间，例如：无法检测一个物体是否在另一个物体的旁边。还有，以下的算法不能检测物体的具体边缘，只是像上图一样的边界框。</p>
<h2 id="Current-Object-Detection-Benchmarks-基准点"><a href="#Current-Object-Detection-Benchmarks-基准点" class="headerlink" title="Current Object Detection Benchmarks(基准点)"></a>Current Object Detection Benchmarks(基准点)</h2><p>为了评估一个物体检测器的表现，研究者用标准化的物体检测器基准点。基准点用于保证我们比之前的表现好。</p>
<h3 id="PASCAL-VOC"><a href="#PASCAL-VOC" class="headerlink" title="PASCAL VOC"></a>PASCAL VOC</h3><p>第一个广泛使用的基准点是PASCAL VOC Challenge, 模式分析、统计建模和计算学习 (the Pattern Analysis, Statistical Modeling, and Computational Learning, PASCAL) Visual Object Classes(VOC) Challenge。PASCAL VOC Challenge测试了20个类，因其测试集的每个类内多变性，所以PASCAL被认为是高质量的基准点。每个测试集对所有物体都有边界框。</p>
<h3 id="ImageNet-Large-Scale-Visual-Recognition-Challenge"><a href="#ImageNet-Large-Scale-Visual-Recognition-Challenge" class="headerlink" title="ImageNet Large Scale Visual Recognition Challenge"></a>ImageNet Large Scale Visual Recognition Challenge</h3><p>代替PASCAL的基准点是ImageNet大规模视觉识别 (ImageNet Large Scale Visual Recognition Challenge, ILSVR)。ILSVR测试了200种物体，且物体种类更加多样化，一张图中常常有多个物体。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/15.2.png"></p>
<h3 id="2-3-Common-Objects-in-Context"><a href="#2-3-Common-Objects-in-Context" class="headerlink" title="2.3 Common Objects in Context"></a>2.3 Common Objects in Context</h3><p>另一个基准点直到今天人仍在使用，在上下文中常见的物体(Common Objects in Context, COCO)。COCO测试了80种物体，但另外测试了物体位置的边界区域。它也进行了物体分割，即物体更加细节的边界。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/15.3.png"></p>
<h2 id="Evaluating-Object-Detection"><a href="#Evaluating-Object-Detection" class="headerlink" title="Evaluating Object Detection"></a>Evaluating Object Detection</h2><p>我们通过对比预测和真实情况来评估物体检测算法，其中真实情况是人工标记的。下图中黄框表示真实情况，绿框表示预测。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/15.4.png"></p>
<p>比较时，有四种情况：</p>
<ol>
<li><p>True Positive (TP)</p>
<p>预测值与真实值均成功定位，见下图(a)。TP在图中被认为是预测值和真实值的重叠部分大于0.5。重叠部分被定义为预测值和真实值集合的交集。也被称作击中(hits)。</p>
</li>
<li><p>False Positive (FP)</p>
<p>真实值未定位，预测值定位。即预测值和真实值重叠部分小于0.5，见下图(b)。也被称作假警报(false alarms)。</p>
</li>
<li><p>True Negative (TN)</p>
<p>模型找不到真实物体，见下图(c)。也被称作漏掉(misses)。</p>
</li>
<li><p>False Negative (FN)</p>
<p>真实值未定位，预测值也未定位。也被称作正确拒绝(misses)。</p>
</li>
</ol>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/15.5.png"></p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/15.6.png"></p>
<p>如上图，通常我们想要最小化FP和FN，同时最大化TP和TN。</p>
<p>我们定义两种衡量尺度：精度(precision)和召回率(recall)。<br>$$<br>Precision&#x3D;\frac{TP}{TP+FP}<br>$$<br>精度可以被视作模型检测到的所有对象中，正确预测的概率。<br>$$<br>Recall&#x3D;\frac{TP}{TP+FN}<br>$$<br>召回率可以被视作所有真实值中被正确检测到的概率。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/15.7.png"></p>
<p>上图中所有黄框真实值都被正确检测到了，所以召回率很高。但是，因为有太多FP，所以精度很低。</p>
<p>对每个用于定义TP的阈值(之前的例子中是0.5)，我们可以衡量精度和召回率，并画出精度-召回率曲线(Precision-Recall, PR curve)。通常，我们想要将PR都最大化。在比较不同模型时可以用到PR曲线，模型越好在曲线在的区域就越大。如下图，Faster-RCNN的效果最好。</p>
<p>实际应用中，我们常指定一个精度&#x2F;召回率值，然后在这个值下使另外一个值（召回率&#x2F;精度）最大化。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/15.8.png"></p>
<h2 id="A-Simple-Sliding-Window-Detector"><a href="#A-Simple-Sliding-Window-Detector" class="headerlink" title="A Simple Sliding Window Detector"></a>A Simple Sliding Window Detector</h2><p>检测问题可以被当作分类问题。我们将窗滑过整张图片，找出窗在的每个位置是否包含物体。如下图，窗滑过的四个位置，只有(d)中包含人。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/15.9.png"></p>
<h3 id="Feature-Extraction-and-Object-Representation"><a href="#Feature-Extraction-and-Object-Representation" class="headerlink" title="Feature Extraction and Object Representation"></a>Feature Extraction and Object Representation</h3><p>在Lecture 8中，我们提到过HOG描述子。本章继续用HOG描述子检测物体。首先，我们需要训练一个模板HOG描述子来检测新图像中的物体。一个方法是基于多张已标记的图像的HOG描述子，训练一个分类器，例如：线性支持向量机(Support Vector Machine, SVM)。然后将分类器对新图像的窗进行识别。</p>
<p>另一个简单的方法是，取得一张平均图像，基于平均图像提取HOG描述子，来创建一个模板。如下图。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/15.10.png"></p>
<h3 id="Classifying-Windows"><a href="#Classifying-Windows" class="headerlink" title="Classifying Windows"></a>Classifying Windows</h3><p>在创造一个模板后，接下来我们需要将物体模板和窗的每个位置对比。这里我们直接将模板作为过滤器，在图上滑动。在每个位置，提取该位置的HOG描述子，和模板的描述子进行计算，得到一个相似度分数。若分数超过预先设置的阈值，则含有该物体。</p>
<p>相似度分数可以简单的用窗HOG描述子和模板描述子的点乘计算。</p>
<p>现在唯一的问题在于滑动的模板窗的大小。如下图，窗太小只能检测到左图，而漏掉了右图。</p>
<p><img src="/15.11.png" alt="15.11"><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/15.11.png">Multi Scale Sliding Window</p>
<p>为了解决上述窗大小问题，我们建立不同尺寸图片的特征金字塔（如下图）。这样我们无需调整模板的大小，在某个尺寸上产生最大相似度值的窗即为物体所在位置。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/15.12.png"></p>
<h2 id="The-Deformable-Parts-Model-DPM"><a href="#The-Deformable-Parts-Model-DPM" class="headerlink" title="The Deformable Parts Model (DPM)"></a>The Deformable Parts Model (DPM)</h2><p>简单的滑窗检测器仍然对物体形状的小变化不够强健，例如：人脸五官的改变，汽车轮子间距等等。我们需要一个新模型解决这个问题。类似于词袋模型，我们可以检测物体的部分而不是整体。最后将部分组合，可以得到一个有些许方差的正确位置。</p>
<h3 id="Early-Deformation-Model-for-Face-Detection"><a href="#Early-Deformation-Model-for-Face-Detection" class="headerlink" title="Early Deformation Model for Face Detection"></a>Early Deformation Model for Face Detection</h3><p>1973年，研究者创建了下图可变形的装置来检测人脸，中间用弹簧连接。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/15.13.png"></p>
<p>这些弹簧标志着两个部分（人脸器官）间有一个相对位置。就像弹簧拉的越长越紧一样，相对于理想位置偏移越大，我们给予的惩罚越大。</p>
<h3 id="More-General-Deformable-Parts-Models"><a href="#More-General-Deformable-Parts-Models" class="headerlink" title="More General Deformable Parts Models"></a>More General Deformable Parts Models</h3><p>上图可变形装置只适用于人脸检测，对更加通用的可变形部分模型，一个流行的方法是星形模型。如下图，将一些检测器作为根(如$x_1$)，然后将其他部分和根用“弹簧”连接。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/15.14.png"></p>
<p>上图右为一个人形检测的例子。蓝框是整个人检测的边界框，也是我们作为根的框。剩下的黄框是人的各个部分检测的边界框，作为和根连接的框。</p>
<p>在这个例子中，我们已经知道每个部分的大致位置，例如：头应该在人整体位置的上中部。但是在机器学习中，我们可能需要学习应该用到哪些部分（例如检测人时的头手脚），以便于最佳的物体检测。</p>
<h3 id="Examples-of-Deformable-Parts-Models"><a href="#Examples-of-Deformable-Parts-Models" class="headerlink" title="Examples of Deformable Parts Models"></a>Examples of Deformable Parts Models</h3><p>通常用物体的整体作为根，更小更细节的过滤器来检测每个部分，如下图。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/15.15.png"></p>
<p>通常我们会对一个物体的多方向采用一个多成分模型，即有一个整体过滤器和每个方向的多个部分过滤器。但这样只能抵抗轻微的角度改变，对于大幅度的角度改变，我们需要不同的检测器。如下图的汽车检测，每行代表一个方向，左列代表车的整体过滤器，中列代表车的多个部分过滤器。右列代表对每个部分的相对惩罚，其中颜色越浅代表惩罚越大。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/15.16.png"></p>
<h3 id="Calculating-Score-for-Deformable-Parts-Models"><a href="#Calculating-Score-for-Deformable-Parts-Models" class="headerlink" title="Calculating Score for Deformable Parts Models"></a>Calculating Score for Deformable Parts Models</h3><p>为了建立一个可变形部分模型，我们需要一个计算分数的方法。首先，我们给整体物体检测器计算一个分数，然后每个部分的分数取决于其变形惩罚。最终分数为整体分数减去所有变形惩罚，这样一个物体整体检测较强但部分变形过大会被大幅度惩罚，具体步骤如下。</p>
<p>一个有$n$个部分的整体模型被一个$(n+2)$的元组表达：<br>$$<br>\begin{align*}<br>(F_0,P_1,P_2,…,P_n,b)<br>\end{align*}<br>$$<br>其中$F_0$是根过滤器，$P_n$是模型的第$n$个部分，$b$是基元。每个模型部分$P_i$定义为一个元组：<br>$$<br>\begin{align*}<br>(F_i,v_i,d_i)<br>\end{align*}<br>$$<br>其中$F_i$是第$i$个部分的过滤器，$v_i$是部分$i$联系根位置的“锚”的位置，$d_i$定义了每个“锚”的可能位置的变形花费。</p>
<p>我们在HOG金字塔上计算整体和每个部分过滤器的位置，如下图。我们对每个尺度都运用HOG过滤器以抵抗尺度变化。每个过滤器的位置是响应最大的位置，为了保持尺度不变，我们需要将每个部分的最大响应映射到原始尺度的图像位置上。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/15.17.png"></p>
<p>检测分数计算式如下：<br>$$<br>\prod_{i&#x3D;0}^n F_i·\phi(p_i,H)-\sum_{i&#x3D;1}^n d_i(dx_i,dy_i,dx_i^2,dy_i^2)<br>$$</p>
<p>上式左侧部分是根和其它部分检测分数的累乘。其中$\theta(p_i,H)$定义为过滤器在$p_i$位置的窗的HOG特征向量。$F_i$是第$i(i&gt;0)$个部分过滤器，$i&#x3D;0$代表整体过滤器。</p>
<p>上式右侧部分是所有部分变形惩罚之和。其中$d_i$表示第$i$部分的惩罚权重，对应大小$dx_i$（相对于“锚”在$x$方向的偏移量），$dy_i$（相对于“锚”在$y$方向的偏移量）。例如：若$d_i&#x3D;(0,0,1,0)$，那么惩罚为$dx_i^2$，相对于“锚”在$x$方向的偏移量的平方。</p>
<h2 id="The-DPM-Detection-Pipeline"><a href="#The-DPM-Detection-Pipeline" class="headerlink" title="The DPM Detection Pipeline"></a>The DPM Detection Pipeline</h2><p>可变形部分模型流程有许多步骤，如下图。我们必须先用整体过滤器来检测一个物体，接着用部分过滤器来计算检测的整体分数。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/15.18.png"></p>
<ol>
<li><p>产生多个不同尺度的原始图像的拷贝。对这些拷贝储存HOG，以便于后续过滤器应用。</p>
</li>
<li><p>对这些图像应用整体过滤器。在整体过滤器检测之上，运用部分过滤器。这部分在整体流程中的位置如下图。<br>$$<br>\prod_{i&#x3D;0}^n F_i·\phi(p_i,H)<br>$$<br><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/15.19.png"></p>
</li>
<li><p>计算空间花费（例如：部分相对于整体的变形惩罚），这部分在整体流程中的位置如下图。：<br>$$<br>\sum_{i&#x3D;1}^n d_i(dx_i,dy_i,dx_i^2,dy_i^2)<br>$$<br><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/15.20.png"></p>
</li>
<li><p>计算检测分数：<br>$$<br>F_0+\prod_{i&#x3D;1}^n F_i·\phi(p_i,H)-\sum_{i&#x3D;1}^n d_i(dx_i,dy_i,dx_i^2,dy_i^2)<br>$$</p>
</li>
<li><p>这些分数代表了在图上每个坐标检测到物体的强度。所以根据分数，画出整张图上的响应分数：</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/15.21.png"></p>
</li>
</ol>
<h2 id="DPM-Detection-Results"><a href="#DPM-Detection-Results" class="headerlink" title="DPM Detection Results"></a>DPM Detection Results</h2><p>DPM模型有几个重要假设：</p>
<ul>
<li>一个物体由整体和与之相连的部分之间的关系定义</li>
<li>检测分数随着根和部分之间的变形减少而提高</li>
<li>不管物体是否可能是另一个类的，只要检测分数高，就认为检测到物体</li>
</ul>
<p>因此，DPM在这些假设不满足时会产生错误。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/15.22.png"></p>
<p>在右上图，DPM成功检测到汽车的多个部分和整体（背后的车）。但由于两辆汽车的部分距离过近，所以DPM将两辆汽车认为是一个整体过滤器，产生错误。</p>
<p>在右下图，DPM成功检测到汽车的整体和部分。但由于DPM没有考虑到不属于汽车的特征（校车顶部），也没有考虑到物体可能属于另一相似类（校车），产生错误。</p>
<h2 id="DPM-Summary"><a href="#DPM-Summary" class="headerlink" title="DPM Summary"></a>DPM Summary</h2><p><strong>Approach</strong></p>
<ul>
<li>手动选择部分集：对每个部分训练一个特殊的检测器</li>
<li>在部分上训练空间模型</li>
<li>用连接可能性评估这些部分</li>
</ul>
<p><strong>Advantages</strong></p>
<ul>
<li>部分有意义</li>
<li>标准的检测方法可以应用于每个部分</li>
<li>在许多类中有效运行</li>
</ul>
<p><strong>Disadvantages</strong></p>
<ul>
<li>需要手动选择部分</li>
<li>语义驱动部分有时不容易发现</li>
<li>不能保证没有遗漏重要部分</li>
<li>切换到别的类时，需要重建模型</li>
</ul>
<p>现在DPM已经过时，它的全连接版本被更广泛的使用：</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/15.23.png"></p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>CS131</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131 Lecture 12 Face Recognition &amp; Dimensionality Reduction</title>
    <url>/2018/08/15/Lecture%2012%20Face%20Recognition%20&amp;%20Dimensionality%20Reduction/</url>
    <content><![CDATA[<p>#Lecture 12 Face Recognition &amp; Dimensionality Reduction </p>
<h2 id="Overview-and-Motivation"><a href="#Overview-and-Motivation" class="headerlink" title="Overview and Motivation"></a>Overview and Motivation</h2><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>维度减少是用于减少特征数的一个过程，可以提高效率。主要有两种方法：奇异值分解(Singular Value Decomposition, SVD)和主成分分析(Principal Component Analysis, PCA)。</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ol>
<li>减少计算成本。减少不重要的特征，保留关键成分。</li>
<li>减少“维度灾难”的影响。lecture 11中提及，维度增加，需要更多数据点，分析耗费的时间更多。因此，减少维度能够缓解维度灾难。</li>
<li>压缩数据。极大的降低数据储存要求。</li>
</ol>
<h2 id="Singular-Value-Decomposition-SVD"><a href="#Singular-Value-Decomposition-SVD" class="headerlink" title="Singular Value Decomposition (SVD)"></a>Singular Value Decomposition (SVD)</h2><h3 id="Overview-1"><a href="#Overview-1" class="headerlink" title="Overview"></a>Overview</h3><p>直观地，SVD是允许将数据呈现在一个新子特征空间的程序，使得数据的大多数变化能被捕捉。这是通过原特征空间的旋转轴，形成与原轴&#x2F;原特征（例如：客户的年龄、收入···）线性组合的新轴。新轴可以基于每个方向对方差的贡献，系统地分解数据点的方差（数据的分散程度）。</p>
<p>SVD的结果是一个关于特征空间的“方向”表，根据方差由高到低排序。有最高方差的方向称为“（数据方差的）主成份”。关注这些维度的数据分布，就可以捕捉到大多数信息。</p>
<p>特征选择和维度减少有所不同，见下。</p>
<h3 id="Technical-Details-of-Singular-Value-Decomposition"><a href="#Technical-Details-of-Singular-Value-Decomposition" class="headerlink" title="Technical Details of Singular Value Decomposition"></a>Technical Details of Singular Value Decomposition</h3><ul>
<li><p>SVD代表矩阵$A&#x3D;U\Sigma V^T$，其中$U:m\times m$和$V:n\times n$是旋转矩阵，$\Sigma:m\times n$是对角尺度矩阵。例如：</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/12.1.png"></p>
</li>
<li><p>python代码：<code>[U, S, V] = numpy.linalg.svd(A)</code>。计算机计算SVD步骤如下：</p>
<ol>
<li>计算$AA^T$的特征向量。这些向量构成$U$的列。特征向量的平方根为奇异值(构成$\Sigma$)</li>
<li>计算$A^TA$的特征向量。这些向量构成$V$的列。</li>
</ol>
</li>
<li><p>因为SVD依赖于特征向量的计算，所以即使矩阵很大，计算也很快。</p>
</li>
<li><p>更详细的实现细节：<a href="http://www.ams.org/samplings/feature-column/fcarc-svd">http://www.ams.org/samplings/feature-column/fcarc-svd</a>.</p>
</li>
</ul>
<h4 id="Eigenvector-definition"><a href="#Eigenvector-definition" class="headerlink" title="Eigenvector definition"></a>Eigenvector definition</h4><ul>
<li>$Ax&#x3D;\lambda x$，$x$为特征向量，$\lambda$为放缩因子。</li>
<li>换句话说，用$x$来转换$A$只会放缩但不会改变方向。</li>
</ul>
<h3 id="Applications-of-Singular-Value-Decomposition"><a href="#Applications-of-Singular-Value-Decomposition" class="headerlink" title="Applications of Singular Value Decomposition"></a>Applications of Singular Value Decomposition</h3><ul>
<li>计算逆转矩阵。如果任意矩阵$A$可以被分解为$A&#x3D;U\Sigma V^T$，那么$A$的逆可以定义为$A^+&#x3D;V^T\Sigma ^{-1}U$。即使这只是一个近似值，但它允许计算许多非平方矩阵的逆。</li>
<li>SVD也可以用于计算矩阵的主成份。主成份大量用于数据分析和机器学习中，因此SVD是很多程序的核心。</li>
</ul>
<h2 id="Principal-Component-Analysis-PCA"><a href="#Principal-Component-Analysis-PCA" class="headerlink" title="Principal Component Analysis (PCA)"></a>Principal Component Analysis (PCA)</h2><h3 id="What-are-Principal-Component"><a href="#What-are-Principal-Component" class="headerlink" title="What are Principal Component"></a>What are Principal Component</h3><p>继续SVD的例子，注意$U$的第一列被$\Sigma$的第一个值缩放了。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/12.2.png"></p>
<p>接着，$U\Sigma$被$V^T$的第一行缩放，对$A$的列产生了一个贡献$A_{partial}$。每个($U$的列$i$)*($\Sigma$的值$i$)*($V^T$的行$i$)都是$A$的一个成分。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/12.3.png"></p>
<p>在这个过程中，我们把矩阵$A$作为$U$的行的线性组合，如上图。但是，现实中我们可以只有$U$的几列来构造出一个$A$的好的近似。这是由于$\Sigma$的性质。$\Sigma$是一个最大值位于左上角，其余值由左上往右下递减的对角矩阵。因此，$U$的前列对$A$的贡献最大。这前几列就称为主成分。</p>
<p>我们通过分析协方差矩阵，移除贡献小的维度。协方差矩阵的值并没有那么重要，但是值的符号很重要，正号代表正相关，负号代表负相关，0代表相互独立。</p>
<h4 id="Covariance"><a href="#Covariance" class="headerlink" title="Covariance"></a>Covariance</h4><p>方差和协方差是一组点在质量中心（均值）的“扩散”的度量。方差：衡量一个维度上的点的偏差的度量，例如高度。协方差：衡量每个维度之间的差异的一种度量。在两个维度之间测量协方差，看看两个维度之间是否有关系，例如研究的小时数和获得的分数。一个维度和自身之间的协方差是方差。<br>$$<br>COV(x,y)&#x3D;\frac{\sum^n_{i&#x3D;1}(\overline x_i-x)(\overline y_i-y)}{n-1}<br>$$</p>
<h3 id="Performing-PCA"><a href="#Performing-PCA" class="headerlink" title="Performing PCA"></a>Performing PCA</h3><p>PCA可以用sklearn package实现：<code>sklearn.decomposition.PCA</code>。非正式方法实现步骤如下：</p>
<ol>
<li><p>将数据转化为$m\times n$格式，$m$代表样本数，$n$表示特征数</p>
</li>
<li><p>使$X$置中<br>$$<br>\frac{X-平均值}{每行标准差}<br>$$</p>
</li>
<li><p>通过SVD对角化$X$：$X&#x3D;U\Sigma V^T$</p>
</li>
<li><p>特征向量是主要方向，这些轴上的阴影是组成成分。这意味着最终我们要计算$XV$</p>
</li>
<li><p>因为$V$包含特征向量，所以是标准正交的，$XV&#x3D;U\Sigma V^TV&#x3D;US$</p>
</li>
<li><p>步骤5说明我们只需要$US$的列，均由SVD产生。</p>
</li>
</ol>
<h3 id="Applications-of-Principal-Component"><a href="#Applications-of-Principal-Component" class="headerlink" title="Applications of Principal Component"></a>Applications of Principal Component</h3><ul>
<li><p>图像压缩。图像矩阵中的大多数信息都可以被低阶矩阵提取。所以在质量没有明显损失下，可以使用PCA压缩图像。如图：</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/12.4.png"></p>
<p>只用16个主成份，原图像就能被很好的重现。相对误差如下：</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/12.5.png"></p>
</li>
<li><p>用于搜索引擎。搜索空间中有许多都与搜索关键词无关，所以搜索引擎常用PCA缩小搜索空间。这对即使搜索十分重要，也体现了SVD的能力。</p>
</li>
</ul>
<p>实际上，PCA代表了样本作为不同成分的权重 – 允许用一个成分代表样本间的差异。这大大减少了数据冗余，使得算法更加高效有用。</p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>CS131</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131 Lecture 18 Tracking</title>
    <url>/2018/08/30/Lecture%2018%20Tracking/</url>
    <content><![CDATA[<h1 id="Lecture-18-Tracking"><a href="#Lecture-18-Tracking" class="headerlink" title="Lecture 18 Tracking"></a>Lecture 18 Tracking</h1><h2 id="Introduction-What-is-tracking"><a href="#Introduction-What-is-tracking" class="headerlink" title="Introduction: What is tracking?"></a>Introduction: What is tracking?</h2><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>定位在连续时间下移动的物体的过程</p>
<h3 id="Objective"><a href="#Objective" class="headerlink" title="Objective"></a>Objective</h3><p>在连续视频帧下关联目标物体并评估目标状态</p>
<h3 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h3><ul>
<li>人机互动</li>
<li>安全</li>
<li>AR</li>
<li>交通</li>
<li>医疗图像</li>
</ul>
<h3 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h3><p>因为视频的大量数据，耗时较长。并依赖于物体识别算法，在以下情况可能会失败：</p>
<ul>
<li>几何改变，例如物体的尺度改变</li>
<li>光方面的改变</li>
<li>图像帧中遮挡</li>
<li>非线性移动</li>
<li>分辨率低的模糊视频</li>
<li>同框的相似物体</li>
</ul>
<h2 id="Feature-Tracking"><a href="#Feature-Tracking" class="headerlink" title="Feature Tracking"></a>Feature Tracking</h2><h3 id="Definition-1"><a href="#Definition-1" class="headerlink" title="Definition"></a>Definition</h3><p>特征追踪是检测并追踪在连续时间下视觉特征点</p>
<h3 id="Challenges-of-feature-tracking"><a href="#Challenges-of-feature-tracking" class="headerlink" title="Challenges of feature tracking"></a>Challenges of feature tracking</h3><ul>
<li>找出哪些特征可被追踪</li>
<li>随着帧追踪——一些点可能产生明显变化（例如，因为旋转，光照改变）</li>
<li>平移：随着模型升级，小错误可能积累成大错误</li>
<li>点可能会消失，需要能够删除、添加点</li>
</ul>
<h3 id="What-are-good-features-to-track"><a href="#What-are-good-features-to-track" class="headerlink" title="What are good features to track?"></a>What are good features to track?</h3><p>通常我们会避开选择光滑区域和边缘作为特征。为了选出“高质量”的特征，一种方法是检测Harris Corners作为关键点，这样能够保证较小的错误敏感性。</p>
<p>一旦选好了特征，接下来就可以使用光流算法来解决移动测量问题。</p>
<h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/18.1.png"></p>
<h3 id="Tracking-methods"><a href="#Tracking-methods" class="headerlink" title="Tracking methods"></a>Tracking methods</h3><h4 id="Simple-Kanade–Lucas–Tomasi-feature-tracker"><a href="#Simple-Kanade–Lucas–Tomasi-feature-tracker" class="headerlink" title="Simple Kanade–Lucas–Tomasi feature tracker"></a>Simple Kanade–Lucas–Tomasi feature tracker</h4><p>The Kanade–Lucas–Tomasi (KLT)特征追踪器是一种特征提取方法。KLT使用空间强度信息来引导位置的搜索，从而找到最佳匹配位置。算法如下：</p>
<ol>
<li><p>找到一个好的点来追踪(Harris corner)</p>
<p>Harris corner点有较大特征值，所以光流方程可解</p>
</li>
<li><p>对每个Harris corner计算帧之间的运动（平移或仿射）</p>
</li>
<li><p>在连续帧内连接运动向量来得到每个Harris corner的轨迹。</p>
<p>如果新点周围和旧点差距太大，则抛弃这些点</p>
</li>
<li><p>对每10或15帧应用Harris检测器，引入新的Harris点。</p>
</li>
</ol>
<p>下图的箭头表示Harris corners的运动追踪。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/18.2.png"></p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/18.3.png"></p>
<h2 id="2D-Transformations"><a href="#2D-Transformations" class="headerlink" title="2D Transformations"></a>2D Transformations</h2><h3 id="Types-of-2D-Transformations"><a href="#Types-of-2D-Transformations" class="headerlink" title="Types of 2D Transformations"></a>Types of 2D Transformations</h3><p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/18.4.png"></p>
<p>有多种类型的2D转换。可以通过摄像机（放置位置、移动、视角…）和物体来选择正确的2D转换。上图是几种2D转换的例子：</p>
<ul>
<li>平移(translation)变换（例如，固定吊顶相机）</li>
<li>相似变换（例如，篮球赛固定相机）</li>
<li>仿射变换（例如，行人检测中的人）</li>
<li>射影变换（例如，移动相机）</li>
</ul>
<h3 id="Translation"><a href="#Translation" class="headerlink" title="Translation"></a>Translation</h3><p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/18.5.png"></p>
<p>平移运动是一个点平移到另一个点。假设我们有一个在左边$(x,y)$的点$m$。应用平移运动将$m$从点$(x,y)$移动到$(x’,y’)$<br>$$<br>x’&#x3D;x+b_1\<br>y’&#x3D;y+b_2<br>$$<br>可以写成用其次坐标的矩阵变换：<br>$$<br>\left(\begin{matrix}<br>x’\<br>y’<br>\end{matrix}\right)<br>&#x3D;\left(\begin{matrix}<br>1&amp;0&amp;b_1\<br>0&amp;1&amp;b_2<br>\end{matrix}\right)<br>\left(\begin{matrix}<br>x\<br>y\<br>1<br>\end{matrix}\right)<br>$$<br>定义$W$<br>$$<br>W(x;p)&#x3D;\left(\begin{matrix}<br>1&amp;0&amp;b_1\<br>0&amp;1&amp;b_2<br>\end{matrix}\right)<br>$$<br>其中因子向量$p&#x3D;\left(\begin{matrix}<br>b_1\<br>b_2<br>\end{matrix}\right)$</p>
<p>则$W$对$p$的偏导数为<br>$$<br>\frac{\partial W}{\partial p}(x;p)&#x3D;\left(\begin{matrix}<br>1&amp;0\<br>0&amp;1<br>\end{matrix}\right)<br>$$<br>上式被称作雅克比行列式(Jacobian)</p>
<h3 id="Similarity-Motion"><a href="#Similarity-Motion" class="headerlink" title="Similarity Motion"></a>Similarity Motion</h3><p>相似运动是一种刚体运动，包括缩放和平移。</p>
<p>我们可以定义相似性为<br>$$<br>x’&#x3D;ax+b_1\<br>y’&#x3D;ay+b_2<br>$$<br>定义$W,p$<br>$$<br>W&#x3D;\left(\begin{matrix}<br>a&amp;0&amp;b_1\<br>0&amp;a&amp;b_2<br>\end{matrix}\right)\<br>p&#x3D;\left(\begin{matrix}<br>a&amp;b_1&amp;b_2<br>\end{matrix}\right)^T<br>$$<br>则$W$对$p$的偏导数，即雅克比行列式(Jacobian)为<br>$$<br>\frac{\partial W}{\partial p}(x;p)&#x3D;\left(\begin{matrix}<br>x&amp;1&amp;0\<br>y&amp;0&amp;1<br>\end{matrix}\right)<br>$$</p>
<h3 id="Affine-motion"><a href="#Affine-motion" class="headerlink" title="Affine motion"></a>Affine motion</h3><p>仿射运动包括放缩，旋转，平移。我们可以表达为<br>$$<br>x’&#x3D;a_1x+a_2y+b_1\<br>y’&#x3D;a_3x+a_4y+b_1<br>$$<br>定义$W,p$<br>$$<br>W&#x3D;\left(\begin{matrix}<br>a_1&amp;a_2&amp;b_1\<br>a_3&amp;a_4&amp;b_2<br>\end{matrix}\right)\<br>p&#x3D;\left(\begin{matrix}<br>a_1&amp;a_2&amp;b_1&amp;a_3&amp;a_4&amp;b_2<br>\end{matrix}\right)^T<br>$$<br>则$W$对$p$的偏导数，即雅克比行列式(Jacobian)为<br>$$<br>\frac{\partial W}{\partial p}(x;p)&#x3D;\left(\begin{matrix}<br>x&amp;y&amp;1&amp;0&amp;0&amp;0\<br>0&amp;0&amp;0&amp;x&amp;y&amp;1<br>\end{matrix}\right)<br>$$</p>
<h2 id="Iterative-KLT-tracker"><a href="#Iterative-KLT-tracker" class="headerlink" title="Iterative KLT tracker"></a>Iterative KLT tracker</h2><h3 id="Problem-formulation"><a href="#Problem-formulation" class="headerlink" title="Problem formulation"></a>Problem formulation</h3><p>给定视频顺序，找到对应每帧相连的变化顺序。要求能够处理任意类型的运动。</p>
<h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><p>与KLT追踪器不同之处在于连接帧的方式：用特征数据和线性相似直接解决相关的变换，而不是用光流连接运动向量并追踪运动。这种方式允许我们处理更加复杂（例如，仿射和投射）转换，并更加稳健的连接物体。</p>
<p>步骤：</p>
<ol>
<li><p>用Harris corner检测找到特征</p>
</li>
<li><p>对于每个在位置$x&#x3D;[x,y]^T$的特征：选择一个特征描述子，并用这个描述子创建一个特征的初始模板（常用附近的像素）：$T(x)$</p>
</li>
<li><p>求出一个$p$，能够最小化下一帧中在$x_2&#x3D;W(x;p)$周围（假设是在特征的新位置）的特征描述的错误。用公式描述则是<br>$$<br>\sum_x[T(W(x;p))-T(x)]^2<br>$$</p>
</li>
<li><p>迭代的重复以上步骤，将帧之间相连接，随着转换的不断应用，储存特征的坐标。这样能够得到关于物体如何在帧之间移动的测量。</p>
</li>
<li><p>像之前一样，每10-15帧引入一个新Harris corner来加入新特征，去除之前不好的特征。</p>
</li>
</ol>
<h3 id="Math"><a href="#Math" class="headerlink" title="Math"></a>Math</h3><p>在上面第三步中，我们实际上可以近似计算$p$。假设有一个关于$p,p_0$的初始猜测，其中$p&#x3D;p_0+\Delta p$</p>
<p>现在<br>$$<br>E&#x3D;\sum_x[T(W(x;p))-T(x)]^2&#x3D;\sum_x[T(W(x;p_0+\Delta p))-T(x)]^2<br>$$<br>使用泰勒逼近，我们可以看出错误项约等于<br>$$<br>E\approx \sum_x[T(W(x;p_0))+\nabla T\frac{\partial W}{\partial p}\Delta p-T(x)]^2<br>$$<br>为了最小化此项，我们对$p_0$求偏导，并设为0，求出$p_0$<br>$$<br>\frac{\partial E}{\partial p}\approx\sum_x[\nabla T(\frac{\partial W}{\partial p})^T][T(W(x;p_0))+\nabla T\frac{\partial W}{\partial p}\Delta p-T(x)]&#x3D;0\<br>\Delta p&#x3D;H^{-1}\sum_x[\nabla T(\frac{\partial W}{\partial p})^T][T(x)-T(W(x;p_0))]\<br>H&#x3D;\sum_x[\nabla T\frac{\partial W}{\partial p}]^T[\nabla T\frac{\partial W}{\partial p}]<br>$$<br>迭代地设置$p_0&#x3D;p_0+\Delta p$，我们可以最终收敛于一个精确的、最小化错误值的$p$，告诉我们转换的类型。</p>
<h3 id="Link-to-Harris-Corner-Detection"><a href="#Link-to-Harris-Corner-Detection" class="headerlink" title="Link to Harris Corner Detection"></a>Link to Harris Corner Detection</h3><p>对平移运动，<br>$$<br>\frac{\partial W}{\partial p}(x;p)&#x3D;\left(\begin{matrix}<br>1&amp;0\<br>0&amp;1<br>\end{matrix}\right)<br>$$<br>容易得到，<br>$$<br>H&#x3D;\left(\begin{matrix}<br>I_x^2&amp;I_xI_y\<br>I_xI_y&amp;I_y^2<br>\end{matrix}\right)<br>$$<br>但是，Harris corner检测器假设只要$H​$有大特征值（即稳定可逆），那么该点就是一个角。因此，角可能是一个用于计算平移的好特征，恰恰因为角计算出的矩阵是稳定可逆的。</p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>CS131</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131 Lecture 14 Visual Bag of Words</title>
    <url>/2018/08/19/Lecture%2014%20Visual%20Bag%20of%20Words/</url>
    <content><![CDATA[<h1 id="Lecture-14-Visual-Bag-of-Words"><a href="#Lecture-14-Visual-Bag-of-Words" class="headerlink" title="Lecture 14 Visual Bag of Words"></a>Lecture 14 Visual Bag of Words</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>首先我们要将图像表现为特征向量形式。接下来创建图像图像的空间形式，观察在低维的图像值。然后将每个图像转换成一组系数，并投影到PCA空间。转换后的数据用分类器分类，例如：K-means、HAC。</p>
<h3 id="Idea-of-Bag-of-Words"><a href="#Idea-of-Bag-of-Words" class="headerlink" title="Idea of Bag of Words"></a>Idea of Bag of Words</h3><p>词袋模型是一个简化目标表示的方式，将其作为它们子部分的集合，以便于分类之类的操作。例如：一个段落里的单词列表和单词的频率就可以构成一个词袋，我们用词袋来表现段落以便于后续分析。</p>
<p>在计算机视觉领域，我们将图像考虑为图像特征的集合。同样，结合特征的频率，我们也可以运用词袋模型来进行预测任务，例如：分类、脸部检测。</p>
<p>词袋模型主要分为两步：</p>
<ol>
<li><p>从多张图片上构造特征“字典”或者“单词表”——图像中存在什么通用特征？例如：房间的色系、脸的部分。</p>
<ul>
<li><p>提取特征</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/14.8.png"></p>
</li>
<li><p>学习视觉词典</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/14.9.png"></p>
</li>
<li><p>用视觉词典量化特征</p>
</li>
</ul>
</li>
<li><p>给定一张新图像，将它们转换为我们收集过的特征直方图——我们在1中构建的特征频率。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/14.10.png"></p>
</li>
</ol>
<h3 id="Origins"><a href="#Origins" class="headerlink" title="Origins"></a>Origins</h3><p>词袋可以看作一个体现在一系列图像或者文档上建立的字典的频率——新数据可以建立这个模型并用于后续预测任务。</p>
<h2 id="Algorithm-Summary"><a href="#Algorithm-Summary" class="headerlink" title="Algorithm Summary"></a>Algorithm Summary</h2><h3 id="Extracting-Interesting-Features"><a href="#Extracting-Interesting-Features" class="headerlink" title="Extracting Interesting Features"></a>Extracting Interesting Features</h3><p>我们可以选择我们想要的任何特征。例如：简单的将图片分割，用子图片作为特征（如下图）。后者我们可以用SIFT特征的角检测。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/14.1.png"></p>
<h3 id="Learning-Visual-Vocabulary"><a href="#Learning-Visual-Vocabulary" class="headerlink" title="Learning Visual Vocabulary"></a>Learning Visual Vocabulary</h3><p>一旦我们找到特征，我们必须将大特征集转换为一套小“主题”。“主题”等于在自然语言分析中的“单词”，也等于计算机视觉中的纹理基元(texton)。</p>
<p>我们可以用任何聚类技术（通常为K-means，Mean Shift或者HAC也可用）来聚类特征。接下来用每个集群的中心作为纹理基元。纹理集被称作视觉词典，如下图。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/14.2.png"></p>
<h3 id="Quantize-Features"><a href="#Quantize-Features" class="headerlink" title="Quantize Features"></a>Quantize Features</h3><p>码矢量(codevector)是纹理基元在这种情况下的同义词，码矢量集构成码本(codebook)。我们用码本量化特征：用相同的方法从新图像中提取特征，接下来用码本将特征向量映射到相近的码矢量标签。</p>
<p>码本的大小（等于集群的个数）是很重要的超参数。太小会导致码矢量没有代表性；太大会导致码本过拟合。</p>
<h3 id="Represent-Images-by-Frequencies"><a href="#Represent-Images-by-Frequencies" class="headerlink" title="Represent Images by Frequencies"></a>Represent Images by Frequencies</h3><p>首先，我们可以将数据集中的每张图表现为码矢量频率的直方图（如下图），我们通过特征量化完成。接着，我们有两个选择，取决于问题类型。如果是监督学习，可以基于直方图训练一个分类器。因为这个分类器是在纹理基元上训练的，所以对类之间的区分是很稳定的。如果是非监督学习，我们可以对直方图进一步聚类来找到数据集中的视觉集合。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/14.3.png"></p>
<h3 id="Large-Scale-Image-Search"><a href="#Large-Scale-Image-Search" class="headerlink" title="Large-Scale Image Search"></a>Large-Scale Image Search</h3><p>词袋模型在大规模图像检索上十分有效。词袋模型可以帮助构造一个有效检索的数据集。首先，从数据中提取特征。接着用k-means（通常k&#x3D;100000）构造一个词典。接下来我们为每个词计算一个权重。赋予权重可以帮助我们降低特定词的重要性。例如：把”is”, “are”之类的词降低权重。在图像中就是把无用特征赋予低权重。</p>
<p>词频逆文档频率(Term Frequency Inverse Document Frequency, TF-IDF)通过单词在文件中的频率赋予权重。</p>
<p>一个单词$j$的逆文档频率(IDF)为：<br>$$<br>IDF&#x3D;log(\frac{Num\space Docs}{Num\space Docs_{j\space appears}})<br>$$<br>图像中bin $j$的值表示为：<br>$$<br>Bin_j&#x3D;frequency_{j\space in\space I}*IDF<br>$$<br>我们可以对文档构造一个词映射的逆文档，以便于快速寻找新图像和数据集中所有图像的相似性。我们只考虑bins和新图像重叠的图像。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/14.11.png"></p>
<p>大规模数据检索的缺点在于算法的表现随着数据集的增大而削弱。因为量化误差和不完美的特征检测，词袋模型有时会产生噪声图像相似度。</p>
<h2 id="Spatial-Pyramid-Matching"><a href="#Spatial-Pyramid-Matching" class="headerlink" title="Spatial Pyramid Matching"></a>Spatial Pyramid Matching</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>空间金字塔匹配(Spatial Pyramid Matching)可以将空间信息结合到模型中。</p>
<h3 id="Pyramids"><a href="#Pyramids" class="headerlink" title="Pyramids"></a>Pyramids</h3><p>一个金字塔通过多张源图像的拷贝构建。每层金字塔大小是上一层的$1&#x2F;4$。层数越低，分辨率越高。从几何角度看，整个多尺度表现看起来像一个金字塔， 原始图像位于最底部，每个周期导致更小的图像叠加在另外一个图像上。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/14.4.png"></p>
<h3 id="Bags-of-Words-BoW-Pyramids"><a href="#Bags-of-Words-BoW-Pyramids" class="headerlink" title="Bags of Words(BoW) + Pyramids"></a>Bags of Words(BoW) + Pyramids</h3><p>空间金字塔匹配将图像分割为越来越细的子区域(sub-region)，并允许我们计算每个子区域的局部特征直方图(BoW)。</p>
<p>如下图，如果金字塔上方的BoW包含天空特征，中部包含植被和山特征，底层包含山特征，那么整张图很可能被分类为山。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/14.5.png"></p>
<h3 id="Some-results"><a href="#Some-results" class="headerlink" title="Some results"></a>Some results</h3><p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/14.6.png"></p>
<h2 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naïve Bayes"></a>Naïve Bayes</h2><h3 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h3><p>一旦我们产生了一个视觉词直方图，我们可以用朴素贝叶斯(Naïve Bayes)来对直方图进行分类。我们简单的衡量一个给定的视觉词是否存在，然后假定一个视觉词的存在&#x2F;缺失和给定类的每个视觉词条件独立。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/14.7.png"></p>
<p>如上图，考虑一些视觉词直方图$X$，$x_i$是视觉词$i$在直方图中的总和。我们只在意视觉词$i$是否存在，即$x_i\in {0,1}$</p>
<h3 id="Prior"><a href="#Prior" class="headerlink" title="Prior"></a>Prior</h3><p>$P(c)$是在所有类中出现类$c$的概率。所以对总共$m$个物体类，我们有<br>$$<br>\sum_{i&#x3D;1}^m P(c)&#x3D;1<br>$$<br>对于一个用直方图$x$表现的图像和一些物体类$c$，我们可以计算<br>$$<br>P(x|c)&#x3D;\prod_{i&#x3D;1}^m P(x_i|c)<br>$$</p>
<h3 id="Posterior"><a href="#Posterior" class="headerlink" title="Posterior"></a>Posterior</h3><p>现在我们可以用贝叶斯理论(Bayes Theorem)来计算图像$x$属于类$c_j$的概率：<br>$$<br>P(c|x)&#x3D;\frac{P(c)P(x|c)}{\sum_{c’} P(c’)P(x|c’)}<br>$$<br>$c’$表示所有类。拓展分子和分母，我们可以重写方程：<br>$$<br>P(c|x)&#x3D;\frac{P(c)\prod_{i&#x3D;1}^m P(x_i|c)}{\sum_{c’} P(c’)\prod_{i&#x3D;1}^m P(x_i|c’)}<br>$$</p>
<h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><p>为了将用直方图$x$表现的图像分类，我们简单的找到能够最大化方程(6)的类$c^*$：<br>$$<br>c^*&#x3D;\arg\max_c P(c|x)<br>$$<br>因为公式中含有大量的小概率累乘，所以可能会产生一个接近于0的不稳定值。因此，我们改用logs计算概率：<br>$$<br>c^*&#x3D;\arg\max_clogP(c|x)<br>$$<br>现在我们考虑两个类$c_1$和$c_2$：<br>$$<br>P(c_1|x)&#x3D;\frac{P(c_1)\prod_{i&#x3D;1}^m P(x_i|c_1)}{\sum_{c’} P(c’)\prod_{i&#x3D;1}^m P(x_i|c’)}<br>$$<br>和<br>$$<br>P(c_2|x)&#x3D;\frac{P(c_2)\prod_{i&#x3D;1}^m P(x_i|c_2)}{\sum_{c’} P(c’)\prod_{i&#x3D;1}^m P(x_i|c’)}<br>$$<br>因为分母是固定的，所以我们计算最大值时可以忽视分母。即($\propto$是正比于的意思)<br>$$<br>P(c_1|x)\propto P(c_1)\prod_{i&#x3D;1}^m P(x_i|c_1)<br>$$<br>和<br>$$<br>P(c_2|x)\propto P(c_2)\prod_{i&#x3D;1}^m P(x_i|c_2)<br>$$<br>对于类$c$<br>$$<br>P(c|x)\propto P(c)\prod_{i&#x3D;1}^m P(x_i|c)<br>$$<br>用上logs：<br>$$<br>logP(c|x)\propto logP(c)+\sum_{i&#x3D;1}^m logP(x_i|c)<br>$$<br>现在，分类任务变成<br>$$<br>\begin{align*}<br>c^*&amp;&#x3D;\arg\max_c P(c|x) \<br>&amp;&#x3D;\arg\max_c logP(c|x)\<br>&amp;&#x3D;\arg\max_c logP(c) +\sum_{i&#x3D;1}^m logP(x_i|c)<br>\end{align*}<br>$$</p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>CS131</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131 Lecture 16 Recognizing Objects by Parts</title>
    <url>/2018/08/22/Lecture%2016%20Recognizing%20Objects%20by%20Parts/</url>
    <content><![CDATA[<h1 id="Lecture-16-Recognizing-Objects-by-Parts"><a href="#Lecture-16-Recognizing-Objects-by-Parts" class="headerlink" title="Lecture 16 Recognizing Objects by Parts"></a>Lecture 16 Recognizing Objects by Parts</h1><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>计算机视觉不止检测物体的分类，还要检测物体的相关信息。例如：</p>
<ul>
<li>淘宝以图搜购买链接</li>
<li>某种蘑菇能不能吃</li>
<li>…</li>
</ul>
<p>通常，计算机视觉需要能够提供物体基本特征之外的信息。</p>
<h2 id="What-can-computer-recognize-today"><a href="#What-can-computer-recognize-today" class="headerlink" title="What can computer recognize today?"></a>What can computer recognize today?</h2><p>找到特定类别的物体（品牌、地标、书本…），不过只能精确匹配。对于当今的系统来说，找到一个通用的对象要困难得多。</p>
<h2 id="What’s-next-to-work-on"><a href="#What’s-next-to-work-on" class="headerlink" title="What’s next to work on?"></a>What’s next to work on?</h2><p>计算机视觉尚未达到的主要目标是通用类别识别。即能识别图中所有物体类别。例如：对于一个橘色杯子，我们想要找到咖啡杯，但图像搜索只能找到有橘色物体在同一位置的图片。</p>
<p>现在的系统无法识别训练集之外的任意物体，而现在有太多的物体类别，很难决定专注于哪个物体。</p>
<h2 id="Big-Data-from-the-Internet"><a href="#Big-Data-from-the-Internet" class="headerlink" title="Big Data from the Internet"></a>Big Data from the Internet</h2><p>现在互联网上86%的数据都是可视数据，而可视数据不能被自动分析。很多识别问题对机器来说较难，但对于人类来说很简单。当人类找不到答案时也可以简单的求助于他人寻找答案，而机器无法做到，所以当今互联网提供了一个将人类帮助和机器识别结合的环境。</p>
<h2 id="ImageNet-and-Confusion-Matrices"><a href="#ImageNet-and-Confusion-Matrices" class="headerlink" title="ImageNet and Confusion Matrices"></a>ImageNet and Confusion Matrices</h2><p>既然模型受限于训练集中类的数目，一个可行的方法是拓展训练集中的类的数目。比较有名的数据集有：ImageNet(13M images, 22000 categories), Caltech101(9K images, 101 categories), LabelMe (30k images), SUN (131K images)。</p>
<p>Deng从PASCAL VOC到ImageNet应用了四种分类模型，发现随着类别数目的提升，精度也随之大幅下降。他将结果画成以下的混淆矩阵(confusion matrix)：</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/16.1.png"></p>
<p>混淆矩阵在xy轴上画出类别，并衡量类中物体正确分类与否的程度。主要看对角线上格子颜色，颜色越浅，分类器效果越好。</p>
<p>从图中我们可以看出，分类器需要区分的类越“细”（或越相似），分类错误就越多。例如：区分狗与鸟的错误率比区分鸟的种类要低。</p>
<h2 id="Challenges-and-Solutions"><a href="#Challenges-and-Solutions" class="headerlink" title="Challenges and Solutions"></a>Challenges and Solutions</h2><h2 id="Semantic-Hierarchy"><a href="#Semantic-Hierarchy" class="headerlink" title="Semantic Hierarchy"></a>Semantic Hierarchy</h2><p>一个解决正确区分相似类的方法是语义层次(semantic hierarchy)。下图即为一个例子。我们创建了一个树结构，它的每个孩子都是父亲的子类。系统将尝试尽量识别到树的深处，即更精确的类。当类更精确时，不确定度也会增加。这个概念称作”hedging”——系统试图识别不确定树中的位置，根据位置做出猜测以得到足够信息，减少错误。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/16.2.png"></p>
<p>为了正式定义这个问题，我们假设训练和测试集有同样的数据分布。另外，我们假设我们可以得到一个给出层次结构的后验概率的基本类$g$。接下来，定义一个奖励函数$R(f)$，它给在树更深处的类（更精确的类）更高的分数。再定义一个预计精度函数$A(f)$，它随着我们沿书向下移动（不确定性增加）而降低。我们的问题定义为<br>$$<br>\max_{A(f)\geq 1-\epsilon} R(f)<br>$$<br>其中$\epsilon$是预先设置的常数，表示对所有例子而言，分类器所允许误差。</p>
<p>为了保证最后方案是最优方案，我们定义一个全局的，固定的，标量参数$\lambda\geq 0$。对每个节点，我们将$\lambda$加入奖励值，然后正规化后验分布。流程如下：</p>
<ol>
<li>选择一个$\lambda$</li>
<li>找到与$\lambda$相关的决策规则$f$</li>
<li>在验证集上衡量表现</li>
<li>检查是否$A\approx 1-\epsilon$。不是则重复</li>
</ol>
<p>我们可以用二分搜索快速找到$\lambda $</p>
<h3 id="Fine-grained-Classes"><a href="#Fine-grained-Classes" class="headerlink" title="Fine-grained Classes"></a>Fine-grained Classes</h3><p>现有的方法从图中所有可能的位置选择特征，但是它可能找不到正确的特征。例如：两种相似的鸟区别在于尾巴，但是计算机可能无法找到这是个辨别特征。解决方案是众包(crowd-sourcing)。</p>
<p>那么，什么是最好的方法来询问一个人群：哪些特征可以区分图像的类别？</p>
<p><strong>Crowd-sourced bubble games</strong>： 用泡沫来代表特征，利用游戏特性来吸引大量人群为图像标记出主要特征，因为有奖惩系统，所以标记质量也较高。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/16.3.png"></p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>CS131</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131 Lecture 19 Introduction to Deep Learning</title>
    <url>/2018/09/03/Lecture%2019%20Introduction%20to%20Deep%20Learning/</url>
    <content><![CDATA[<h1 id="Lecture-19-Introduction-to-Deep-Learning"><a href="#Lecture-19-Introduction-to-Deep-Learning" class="headerlink" title="Lecture 19 Introduction to Deep Learning"></a>Lecture 19 Introduction to Deep Learning</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>目前为止，本课程已经包含了一系列经典的计算机视觉技术，包括边缘检测(edge detection)、聚类(clustering)方法、分类器(classifiers)、特征检测器&#x2F;描述子(feature detectors&#x2F;descriptors)。然而这些方法都有一个主要缺点：它们依赖于人类手动设计特征并选择表现良好的分类器。结果是，用于解决一个问题的计算机视觉方法往往无法很好的解决另一个问题，构建一个合理且精确的视觉流程需要大量人力和实验。甚至最佳的流程也受限于它能取得的最高精度，因为他们是由人类设计的，所以只能选取人类首先选取的模式。</p>
<p>深度学习(deep learning)已经成为现代计算机视觉研究的一个基本模块，因为它能够避免上述缺点。深度学习不再依赖于人工选择的特征和分类器，而是自身学习如何最好的消化和解释数据，来解决它们被训练的任何问题。关键点在于它们执行端到端学习(end-to-end learning)，即学习如何将原始数据（图像）直接映射到想要的输出（例如，标签、分割图像、预测）。深度学习常常包含将一系列简单的工具组合来构建一个网络，接着在数据上对想要解决的特定问题训练网络。它们常常能只靠替换训练数据来解决不同的问题。深度学习方法受到人类大脑的启发，它们尝试去模仿人类大脑新皮层中神经元的活动。</p>
<h2 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h2><h3 id="Image-Classification"><a href="#Image-Classification" class="headerlink" title="Image Classification"></a>Image Classification</h3><p>图像分类的任务是辨别所有输入图像中物体的类别。一个常见的基准是ImageNet，之前的lecture中有简单的介绍过。</p>
<p>最初应用于ImageNet的流行算法，是手动调整且用了仔细挑选、硬编码(hardcode)的特征标记测试图像，再讲标记过的图像输入分类器，像用SVM来完成最后的分类。例如，2010年的时候，ImageNet优胜算法是先用HOG和LBP描述子，两者结合后再提供给SVM。但是这种叫做“shallow”的方法很快就被“deep”方法打败了。接下来的几年内，分别出现了AlexNet、VGG、GooLeNet，直到2015年的ResNet，错误率已经低于人眼识别。</p>
<p>通常来说，这意味着深度学习算法在计算机视觉领域的前景极佳。深度学习算法的表现远超过传统的计算机视觉算法，甚至在某些精细的、较难的视觉任务上甚至超过了人类的表现，特别是当网络中层数较多时。</p>
<h3 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h3><p>物体检测实际上是图像分类的一种拓展，在问“什么”（例如，识别图中物体）之上又提出了“哪里”（例如，定位被识别的物体）。在深度卷积神经网络(deep convolutional neural networks, CNN)出现之前，“shallow”计算机视觉方法在PASCAL VOC数据集上的mAP(mean average precision)在40%左右。之后出现的一系列深度学习方法，将mAP提升到了85.6%。</p>
<h3 id="Object-Segmentation"><a href="#Object-Segmentation" class="headerlink" title="Object Segmentation"></a>Object Segmentation</h3><p>物体分割包括首先将一个物体划分为多个不同的语义区，这些语义区通常代表不同物体，接着检测每个区域包含的物体类型。这在传统分割算法（像我们在本门课中学的）是极具挑战性的——我们需要对我们想检测的物体有特定的知识，且必须手工设计符合物体的特征（例如，甜甜圈是中间带有孔的圆形物体，所以我么需要一个能够优先考虑这些形状的特征）。但是，最近的深度学习方法在解决物体分割方面展示了令人影响深刻的结果，例如：用三个独立的深度网络可得到下图的结果。</p>
<p>![1535934150492](D:&#x2F;Machine Learning&#x2F;cs131&#x2F;Lecture18_19&#x2F;19.1.png)</p>
<h3 id="Pose-Estimation"><a href="#Pose-Estimation" class="headerlink" title="Pose Estimation"></a>Pose Estimation</h3><p>姿态估计包含通过一系列图片，追踪人体结构骨架来理解人类正在做什么。利用CNN共同学习定位身体部位，然后将部位与图中独立的人相关联。下图是一个姿态估计的例子。</p>
<p>![1535934565445](D:&#x2F;Machine Learning&#x2F;cs131&#x2F;Lecture18_19&#x2F;19.2.png)</p>
<h3 id="Image-Captioning"><a href="#Image-Captioning" class="headerlink" title="Image Captioning"></a>Image Captioning</h3><p>图像说明是通过算法理解图中物体、动作、关系，并附上与图像描述相关的说明。同样，深度学习能很好的解决这方面的问题：用一个卷积和递归结合的神经网络来给图像标记一个单独的说明，并用一个相似的结构加上一个额外的神经网络层用于定位图中独立的物体&#x2F;行为和密集的给每一个物体&#x2F;行为标记一个说明。</p>
<p>![1535935585337](D:&#x2F;Machine Learning&#x2F;cs131&#x2F;Lecture18_19&#x2F;19.3.png)</p>
<h3 id="Other-Computer-Vision-Tasks"><a href="#Other-Computer-Vision-Tasks" class="headerlink" title="Other Computer Vision Tasks"></a>Other Computer Vision Tasks</h3><p>视觉问题回答(Visual question answering)是利用算法回答图中描述的相关物体&#x2F;行为的问题，例如，给定一张披萨的图像，算法可能需要回答“披萨有几块？”或者“这是一块素披萨吗？”。用一个基于递归神经网络，利用LSTM(长短记忆网络，long short-term memory)模块来回答关于图像主题和行为的多选题。</p>
<p>图像超分辨率(Image super-resolution)包括通过缩小的输入图像，推导出高分率版本——这是一个较难的任务，因为它包含产生未知的细节，并保证这些产生的细节与整张图片是相关的。用一个生成对抗网络(generative adversarial network)来从缩小的图像中恢复真实的细节，并产生接近原始高分辨率验证图像接近的插值结果。</p>
<h3 id="Outside-Computer-Vision"><a href="#Outside-Computer-Vision" class="headerlink" title="Outside Computer Vision"></a>Outside Computer Vision</h3><p>深度学习同样也对在计算机视觉之外的方面有着特别好的效果，包括机器翻译(machine translation)，文本生成(text generation)，语音识别(speech recognition)和语音合成(speech synthesis)。</p>
<p>另外，深度强化学习(deep reinforcement learning)方法也在一系列任务取得了成功。深度强化学习包括反复使用试凑法学习一种端到端的最优控制策略（也就是在特定情况下采取一系列行动达到最终目标），最终目标是直接映射状态信息（例如，一个视频游戏的截图）。深度强化学习在许多基于游戏的任务上有着显著的结果，例如：Alpha Go。</p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>大多数深度学习问题被制定为监督学习任务：给定数据$X$和标签$y$，目标是学习精确的预测标签$\hat y$。图像分类是一个监督学习问题，通常用深度学习方法解决。我们提及的其他解决图像分类问题的方法是首先提取手工设定的特征，再用机器学习方法（例如：PCA、KNN）去学习在指定特征集下的模型。在测试的时候，我们用分类器输出$\hat y$作为我们的预测分类。问题在于手工设定的特征提取器在训练时不能被优化。下图是传统计算机视觉对图像分类的流程</p>
<p>![1535937955378](D:&#x2F;Machine Learning&#x2F;cs131&#x2F;Lecture18_19&#x2F;19.4.png)</p>
<p>本门课所包含的大多数策略专注于将图像减少至手工设定的特征集。这些特征集将图像原始信息的一部分编码，可以用于大多数机器学习方法训练分类器。</p>
<p>深度学习允许我们学习特征与分类器的联合，以取得更好的模型。深度学习由从类似第一准则获得可提供信息的特征，并以此作为动力，即同时最优化特征提取和分类器模块。下图是深度学习作用于图像分类的例子。</p>
<p>![1535949650711](D:&#x2F;Machine Learning&#x2F;cs131&#x2F;Lecture18_19&#x2F;19.5.png)</p>
<p>层次特征如下：</p>
<p>![1535949732993](D:&#x2F;Machine Learning&#x2F;cs131&#x2F;Lecture18_19&#x2F;19.6.png)</p>
<p>深度学习模型包含许多层，可以学习到层次特征。低层学习简单特征，而高层学习简单特征的结合。</p>
<h2 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h2><h3 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h3><p>监督学习是从被标记的训练数据中学习一个方程的过程。给定一个训练样本集，模型通过调整权重(weights)来最小化代价函数(cost function)的值。代价函数是用于衡量预测值和实际值之间不同的程度，例如：均方误差(mean squared error)。通常，损失函数会包含一个正规化项$R(w)$，用于惩罚较大的权重值并限制模型的复杂度。接下来模型就可以用这些权重来对测试数据进行预测。随后可以用测试误差来对模型进行表现分析。<br>$$<br>W^*&#x3D;\arg\min_W \frac1N \sum_{i&#x3D;1}^N l(f(x_x,w),y_i)+R(w)<br>$$</p>
<h3 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h3><p>线性回归是一种监督学习方法，我们尝试将输入向量$x_i\in R^{D_{in}}$通过模型关联至输出向量$y_i\in R^{D_{out}}$。线性回归通过一个简单的线性转换或者矩阵乘法来建模：<br>$$<br>f(x,W)&#x3D;Wx<br>$$<br>我们可以将其视作一个监督学习问题，我们需要学习权重$W$。我们通过最优化一个将欧几里德距离$l(\hat y,y)&#x3D;\frac12||\hat y-y||^2_2$和正规化项$R(W)$结合的损失函数。利用frobenius正规化项，可以用以下方程描述：<br>$$<br>W^*&#x3D;\arg\max_W \frac1{2N}\sum_{i&#x3D;1}^N||Wx_i-y||^2_2+\lambda||W||^2_{fro}<br>$$<br>下图是一个简单的线性回归模型用于对图像分类。$W$的每行代表用于计算输入到每个输出类总和的权重。</p>
<h3 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h3><p>在一些情况下，简单的线性转换已经不能满足我们的需求，所以我们需要引入一个更复杂的模型。首先，我们可能想只添加矩阵乘法，如下：<br>$$<br>f(x,W_1,W_2)&#x3D;W_2W_1x<br>$$<br>但是，这种方法实际上与之前的线性转换没有差别。因为$W_1W_2$可以写作矩阵乘法$W&#x3D;W_1W_2$，这样又变成了之前的线性转换$f(x,W)&#x3D;Wx$。作为替代，我们需要在矩阵乘法之间引入非线性部分。进而产生以下的新模型：<br>$$<br>f(x,W_1,W_2)&#x3D;W_2(\sigma(W_1,x))<br>$$<br>式中，$W_1\in R^{H\times D_{in}},W_2\in R^{D_{out}\times H}$。 在这个模型中，一个矩阵$W_1$和一个非线性方程$\sigma:R^H\rightarrow R^H$点乘。这就是一个一层神经网络的例子。$\sigma$函数让这个模型比简单的线性模型更加有效。</p>
<p>现在较为流行的激活函数$\sigma (x)$有两种：</p>
<ul>
<li>sigmoid函数，定义$\sigma (x)&#x3D;\frac{1}{1+e^{-x}}$，因为它是连续的且可微。同时也限制了输入。</li>
<li>修正线性单元(rectified linear unit, ReLu)，定义$\sigma (x)&#x3D;max(0,x)$，在文学和效率方面都有出色的表现</li>
</ul>
<p>![1535968957626](D:&#x2F;Machine Learning&#x2F;cs131&#x2F;Lecture18_19&#x2F;19.7.png)</p>
<p>由激活函数产生的向量被称作一个隐藏层。可以通过包含更多的矩阵乘法，即更多的隐藏层来拓展这种模型。如下式，是一个二层神经网络：<br>$$<br>f(x,W_1,W_2,W_3)&#x3D;W_3\sigma(W_2\sigma(W_1,x))<br>$$<br>给定一个可微的损失函数（正如我们在线性回归所用的一样），这种新的神经网络结构可以用一系列最优化技术进行最优化。当神经网络训练时，隐藏层开始表现关于输入的“学习特征”。重要的是，这是这种类型的模型和之前手工产生特征的最大进步。</p>
<p>![1535969285904](D:&#x2F;Machine Learning&#x2F;cs131&#x2F;Lecture18_19&#x2F;19.8.png)</p>
<h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><p>如上所述，我们最终想要最小化以下的方程以检测最佳的权重：<br>$$<br>g(w)&#x3D;\frac1N \sum_{i&#x3D;1}^{\infty} l(f(x_x,w),y_i)+R(w)<br>$$<br>一个优化上式的方法是直接解方程，但花费极大。类似的，对最佳权重的随机搜索效率也较低，特别是维度较大时。</p>
<p>这里我们采用梯度下降的方法。梯度下降通过计算权重向量$w$发生较小变化时，给定的函数$g(w)$会发生多少变化。一个经典的检测变化量的方法是计算导数$g’(w)$，这里假设$w$是一个标量。但如果$w$是一个向量，我们旧必须计算$g(w)$在$w$每个部分的偏导数。</p>
<p>基于以上梯度下降的想法，我们可以写出一个最直接的算法</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Initialize w randomly</span><br><span class="line">while true do:</span><br><span class="line">	read current</span><br><span class="line">	g = compute gradient of g(w)</span><br><span class="line">	w = w - a * g</span><br></pre></td></tr></table></figure>

<p>用以上算法，我们可以迭代的更新权重。学习率$\alpha$是一个超参数，代表在搜索最小值时梯度每一步下降的速度。</p>
<h2 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h2><p>卷积神经网络是一个常用且成功的深度学习模型子集，它包含了之前提过的卷积滤波器。从一张原始图像(大小为32x32，3个颜色通道)开始，图像和多个滤波器卷积。在课上的例子中，使用了一个5x5的滤波器，只对图中的“有效区域”卷积——所以不包含图外区域，输出的长度为32-5+1&#x3D;28，如下图。至关重要的是，这些滤波器通过反向传播(backpropagation)集体被优化——我们不再受限于手动选择的特征。</p>
<p>![1535973112300](D:&#x2F;Machine Learning&#x2F;cs131&#x2F;Lecture18_19&#x2F;19.9.png)</p>
<p>卷积的方法给了我们平移不变性——一个特征不管在图像的哪里出现，表达的意思都相同。许多独立的卷积滤波器可以应用于原图，然后其堆叠的输出相当于得到一个有更多通道的图像。像其他深度学习模型一样，非线性在卷积神经网络中也有应用，且卷积层可以再之前输出的基础上再训练，得到高层次的特征（即地层次特征的结合）。这个特性允许我们从简单的成分中提取有效的模型，这正是深度学习的特征。</p>
<p>![1535973824823](D:&#x2F;Machine Learning&#x2F;cs131&#x2F;Lecture18_19&#x2F;19.10.png)</p>
<p>关于GoogLeNet的研究通过”开始模块”(Inception modules)——一个接下来会被连接的层的特殊模式，推动了这个方法的发展。为了训练这个深度网络，Szegedy et al.使用“成长分类器”(auxillary classifiers)仅基于一些GoogLeNet的层，有效的尝试预测图像的类别。在训练中，损失函数是所有输出权重之和。</p>
<p>![1535974121293](D:&#x2F;Machine Learning&#x2F;cs131&#x2F;Lecture18_19&#x2F;19.11.png)</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>深度学习是一个在计算机视觉和机器学习领域较新的方法，它同时训练特征提取器和分类器，而不是依赖于人类去检测有效的特征。深度学习方法在大多数计算机视觉问题中都有极佳的表现。但是，在训练样本庞大或者过小的时候，本课程介绍的其他方法可能会更有效，因为人类的直觉在一些领域可以选出良好的特征集。</p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>CS131</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131 Lecture 17 Motion</title>
    <url>/2018/08/25/Lecture%2017%20Motion/</url>
    <content><![CDATA[<h1 id="Lecture-17-Motion"><a href="#Lecture-17-Motion" class="headerlink" title="Lecture 17 Motion"></a>Lecture 17 Motion</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>本节课我们会将之前的技术与新方法结合，追踪多张图片下像素的移动，应用方面有自动驾驶汽车、机器人、安全系统等</p>
<h2 id="Optical-Flow-and-Key-Assumptions"><a href="#Optical-Flow-and-Key-Assumptions" class="headerlink" title="Optical Flow and Key Assumptions"></a>Optical Flow and Key Assumptions</h2><h3 id="Optical-Flow"><a href="#Optical-Flow" class="headerlink" title="Optical Flow"></a>Optical Flow</h3><p>光流即像素随时间的运动。光流的目的是通过观察两张图片$I_0$和$I_1$，对每个在时间$t_0$到$t_1$之间的每个像素产生一个运动矢量。但是，光流只能呈现图像模式的<strong>明显</strong>运动，这在下一节Assumptions and Limitations有解释，</p>
<h3 id="Assumptions-and-Limitations"><a href="#Assumptions-and-Limitations" class="headerlink" title="Assumptions and Limitations"></a>Assumptions and Limitations</h3><h4 id="Apparent-Motion"><a href="#Apparent-Motion" class="headerlink" title="Apparent Motion"></a>Apparent Motion</h4><p>在二维图像中，光流只能呈现亮度模式的明显运动，意味着移动矢量是由各种行动产生的。例如：变化的光照可以使得静止的物体产生强运动矢量，但进入或者移出屏幕的运动无法被2D光流的运动矢量捕捉。光流处理问题的一个例子是孔径问题。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/17.1.png"></p>
<p>孔径问题指光流无法代表边缘运动的结果，可能导致移动测量错误。例如图中线条实际上是向左下移动，但是由于孔径，看起来像是向右移动。</p>
<h4 id="Brightness-Consistency"><a href="#Brightness-Consistency" class="headerlink" title="Brightness Consistency"></a>Brightness Consistency</h4><p>光流只能呈现明显的运动，为了正确的检测图中点的运动，我们必须假设这些点在帧之间的亮度相等。亮度一致性(Brightness Consistency)方程如下：<br>$$<br>I(x,y,t-1)&#x3D;I(x+u(x,y),y+v(x,y),t)<br>$$<br>其中$u(x,y)$表示点的水平运动，$v(x,y)$表示点的垂直运动。</p>
<h4 id="Small-Motion"><a href="#Small-Motion" class="headerlink" title="Small Motion"></a>Small Motion</h4><p>光流假设点在连续的图像之间不会移动太多。因为帧之间的时间极短，所以假设基本成立。但是在物体距离摄像机过远或过近的时候，这个假设不再成立。接下来我们用亮度一致性方程来证明假设的必要性。首先用泰勒展开式线性化等式右侧：<br>$$<br>I(x+u(x,y),y+v(x,y),t)\approx I(x,y,t-1) +I_x·u(x,y)+I_y·v(x,y)+I_t<br>$$<br>线性化允许我们解得运动向量$u$和$v$，但在这种情况下我们只包含泰勒展开式的第一项。当帧之间的运动幅度较大时，式子无法步骤整个运动情况，导致不精确的$u$和$v$，因此假设有必要。</p>
<h4 id="Spatial-Coherence"><a href="#Spatial-Coherence" class="headerlink" title="Spatial Coherence"></a>Spatial Coherence</h4><p>空间相干性是一个关于相同物体内邻近的像素会移动到一起的假设。其必要性证明如下：<br>$$<br>\begin{align}<br>I(x+u(x,y),y+v(x,y),t)\approx I(x,y,t-1) +I_x·u(x,y)+I_y·v(x,y)+I_t \<br>I(x+u(x,y),y+v(x,y),t)- I(x,y,t-1) &#x3D;I_x·u(x,y)+I_y·v(x,y)+I_t<br>\end{align}<br>$$<br>可得到<br>$$<br>\begin{align}<br>I_x·u+I_y·v+I_t\approx 0\<br>\nabla I·[u\space v]^T +I_t &#x3D;0<br>\end{align}<br>$$<br>现在我们有足够的方程解每个单像素中的$u$和$v$。假设像素可以移动到一起允许我们用有相同的$[u\space v]$的多个公式，使得我们能够解出邻居像素的移动。</p>
<h2 id="Lucas-Kanade"><a href="#Lucas-Kanade" class="headerlink" title="Lucas-Kanade"></a>Lucas-Kanade</h2><p>从上式中得到图像的移动，至少每个像素需要两个方程。Lucas-Kanade技术依赖于一个附加约束–空间相干性来完成图像追踪。</p>
<p>通过一个大小为$k\times k$的窗来对像素应用空间相干性约束。假设窗内的邻居像素有相同的$(u,v)$。例如，在$5\times5$窗内应用以下式子：</p>
<p>![17.2](D:\Machine Learning\cs131\Lecture16_17\17.2.png)</p>
<p>这产生了一个$Ad&#x3D;b$形式的线性方程的过约束系统。用最小二乘法解过拟合系统，我们减少了解$(A^TA)d&#x3D;A^Tb$中的$d$的复杂度。更准确的说，需要求解的系统系统被简化至</p>
<p>![17.3](D:\Machine Learning\cs131\Lecture16_17\17.3.png)</p>
<h3 id="Condition-for-an-Existing-Solution"><a href="#Condition-for-an-Existing-Solution" class="headerlink" title="Condition for an Existing Solution"></a>Condition for an Existing Solution</h3><p>为了能解这个系统，需要满足以下条件：</p>
<ul>
<li><p>$A^TA$要求可逆</p>
</li>
<li><p>$A^TA$不能太小以至于引起噪音。</p>
<p>$A^TA$的特征向量$\lambda_1$和$\lambda_2$不能过小</p>
</li>
<li><p>$A^TA$应该适中</p>
<p>例如：$A^TA$的特征向量$\lambda_1$和$\lambda_2$不能过大</p>
</li>
</ul>
<h3 id="Geometric-Interpretation"><a href="#Geometric-Interpretation" class="headerlink" title="Geometric Interpretation"></a>Geometric Interpretation</h3><p>很明显，之前的最小二乘方程组产生了一个二阶矩阵$M&#x3D;A^TA$。事实上，这是角探测的哈里斯矩阵。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/17.4.png"></p>
<p>我们可以把上面的条件联系起来，以解出运动场$[u\space v]$来追踪哈里斯矩阵$M$检测到的角。$M&#x3D;A^TA$的特征向量和一个区域可能的边缘特征值合方向、大小联系。</p>
<p>综上，很明显Lucas-Kanade的光流估计的最理想区域是一个角。若两个$\lambda$均过小，则这个区域过于平坦。若一个$\lambda$远大于另一个，则会产生孔径问题，无法得到正确的光流。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/17.5.png"></p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/17.6.png"></p>
<p>以上树的三张图从左到右分别代表：$\lambda_1$大而$\lambda_2$小、两个$\lambda$都小（低语义区）、两个$\lambda$都大（高语义区）</p>
<h3 id="Error-in-Lucas-Kanade"><a href="#Error-in-Lucas-Kanade" class="headerlink" title="Error in Lucas-Kanade"></a>Error in Lucas-Kanade</h3><p>Lucas-Kanade受限于光流假设。假设$A^TA$可逆且图中没有太多噪音，错误仍然在以下情况产生：</p>
<ul>
<li>亮度一致性不满足，意味着随着时间改变像素的强度可能改变</li>
<li>移动量过大或者随着时间不会逐渐改变</li>
<li>未满足空间相干性，意味着相邻像素没有随之改变。这个可能是由不合适大小的窗造成的（即选择了不好的$k$）。</li>
</ul>
<h3 id="Improving-Accuracy"><a href="#Improving-Accuracy" class="headerlink" title="Improving Accuracy"></a>Improving Accuracy</h3><p>从上面所做的许多假设中，Lucas-Kanade可以通过包含之前在亮度一致方程的泰勒展开近似中得到的更高阶项来提高其准确性。这个放松了之前的假设。现在，待解决的问题是：<br>$$<br>I(x+u,y+v)&#x3D;I(x,y)+I_xu+I_yv+higher\space order\space terms-I_{t-1}<br>$$<br>这是一个找多项式根的问题，可以用牛顿迭代方法解决。</p>
<p>总的来说，精确的迭代Lucas-Kanade算法可以被应用：</p>
<ol>
<li>解Lucas-Kanade方程，估计每个像素的速度</li>
<li>用被估计的光流区域和图像变形技术，将$I(t-1)$变化到$I(t)$</li>
<li>重复至收敛</li>
</ol>
<h2 id="Horn-Schunk"><a href="#Horn-Schunk" class="headerlink" title="Horn-Schunk"></a>Horn-Schunk</h2><h3 id="Horn-Schunk-Method-for-Optical-Flow"><a href="#Horn-Schunk-Method-for-Optical-Flow" class="headerlink" title="Horn-Schunk Method for Optical Flow"></a>Horn-Schunk Method for Optical Flow</h3><p>Horn-Schunk方法将光流公式化为以下全局能量函数，并尽量最小化$u(x,y)$和$v(x,y)$。<br>$$<br>E&#x3D;\iint[(I_xu+I_yv+I_t)^2+\alpha^2(||\nabla u||^2+||\nabla v||^2)]dxdy<br>$$<br>上式第一项反应了光照恒定假设，根据假设$I_xu+I_yv+I_t$应该等于0。这项存在于式子中是为了保证这个值尽可能接近于0。</p>
<p>第二项反应了小移动假设。这项存在于式子中为了鼓励在位置改变较小的情况下，更加光滑的流。$\alpha$是正规化常数，用于控制光滑度，值越大流越光滑。</p>
<p>为了最小化能量函数，我们对$u$和$v$求导并等于0。得到以下两个等式<br>$$<br>\begin{align}<br>I_x(I_xu+I_yv+I_t)+\alpha^2\Delta u&#x3D;0\<br>I_y(I_xu+I_yv+I_t)+\alpha^2\Delta v&#x3D;0<br>\end{align}\<br>\Delta&#x3D;\frac{\partial}{\partial x^2}+\frac{\partial}{\partial y^2}<br>$$<br>其中$\Delta$为拉格朗日算子，在实际计算中为<br>$$<br>\Delta u(x,y)&#x3D;\overline u(x,y)-u(x,y)<br>$$<br>其中$\overline u(x,y)$是$u$在$(x,y)$附近的加权平均。用(11)式带入得到：<br>$$<br>\begin{align}<br>(I_x^2+\alpha^2)u+I_xI_yv&#x3D;\alpha^2\overline u-I_xI_t\<br>(I_y^2+\alpha^2)u+I_xI_yu&#x3D;\alpha^2\overline v-I_yI_t<br>\end{align}\<br>$$<br>是一个关于$u$和$v$的线性方程。</p>
<h3 id="Iterative-Horn-Schunk"><a href="#Iterative-Horn-Schunk" class="headerlink" title="Iterative Horn-Schunk"></a>Iterative Horn-Schunk</h3><p>因为$u$和$v$的值取决于$(x,y)$邻近的光流值，所以我们需要在每次邻居更新后，重新计算$u$和$v$：<br>$$<br>\begin{align}<br>u^{k+1}&#x3D;\overline u^k-\frac{I_x(I_x\overline u^k+I_y\overline v^k+I_t)}{\alpha^2+I_x^2+I_y^2}\<br>v^{k+1}&#x3D;\overline v^k-\frac{I_y(I_x\overline u^k+I_y\overline v^k+I_t)}{\alpha^2+I_x^2+I_y^2}<br>\end{align}\<br>$$<br>其中上标$k$代表迭代次数</p>
<h3 id="Smoothness-Regularization"><a href="#Smoothness-Regularization" class="headerlink" title="Smoothness Regularization"></a>Smoothness Regularization</h3><p>光滑度正规化项$||\nabla u||^2+||\nabla v||^2$推进最小化邻近点之间的光流变化。这样在边缘区域，点会流向最临近的点，解决孔径问题。</p>
<h3 id="Dense-Optical-Flow-with-Michael-Black’s-Method"><a href="#Dense-Optical-Flow-with-Michael-Black’s-Method" class="headerlink" title="Dense Optical Flow with Michael Black’s Method"></a>Dense Optical Flow with Michael Black’s Method</h3><p>Michael Black拓展了Horn-Schunk方法。原来的正规化项$||\nabla u||^2+||\nabla v||^2$如下图，是一个二次函数。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/17.7.png"></p>
<p>现在用以下函数替代</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/17.8.png"></p>
<h2 id="Pyramids-for-Large-Motion"><a href="#Pyramids-for-Large-Motion" class="headerlink" title="Pyramids for Large Motion"></a>Pyramids for Large Motion</h2><p>返回之前的假设，我们要求在帧之间像素只能有小位移。所以当位移较大时，算法可能会崩溃。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/17.9.png"></p>
<p>注意上图，Lucas-Kanade无法找到树干流的一致向量。为了解决这个问题，我们可以和之前的滑动窗特征检测器一样，建立一个图像尺度逐渐下降的金字塔。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/17.10.png"></p>
<p>现在，当我们想找到流向量，可以采用帧之间位移更小的低分辨率采样版本。以下是使用了金字塔后的树干流向量：</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/17.11.png"></p>
<p>注意现在流向量大多数都指向同一方向，即树干整体移动方向一致。</p>
<h2 id="Common-Fate"><a href="#Common-Fate" class="headerlink" title="Common Fate"></a>Common Fate</h2><p>我们可以通过图像中部分的共同性来获得更多信息，共同性即在图像给定的切割中，这部分像素移动方式都相似。我们的目标就是找出有共同性的分割或层。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/17.12.png"></p>
<h3 id="Identify-Layers"><a href="#Identify-Layers" class="headerlink" title="Identify Layers"></a>Identify Layers</h3><p>我们通过将图像分割至块，并基于块之间放射运动因子的相似性，将块组合来计算图像的层。对于每个块，找到一个$\alpha$，对于所有在块中的像素$(x,y)$，能够最小化下式：<br>$$<br>Err(\alpha)&#x3D;\sum[I_x(\alpha_1+\alpha_2x+\alpha_3y)+I_x(\alpha_4+\alpha_5x+\alpha_6y)+I_t]^2<br>$$<br>上面的方程由两部分推导而出：</p>
<ol>
<li><p>亮度一致方程<br>$$<br>I(x,y,t-1)&#x3D;I(x+u(x,y),y+v(x,y),t)<br>$$</p>
</li>
<li><p>仿射运动成分<br>$$<br>I_xu(x,y)+I_xv(x,y)+I_t\approx0<br>$$<br>$I_x,I_y,I_t$是图像关于两个方向和实践的梯度。$u(x,y),v(x,y)$是仿射运动在两个方向上的成分<br>$$<br>\begin{align}<br>u(x,y)&#x3D;\alpha_1+\alpha_2x+\alpha_3y\<br>v(x,y)&#x3D;\alpha_4+\alpha_5x+\alpha_6y<br>\end{align}<br>$$</p>
</li>
</ol>
<p>从此，我们将因子向量$\alpha_i$映射到运动因子空间，并对仿射运动因子向量采用k-means聚类。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/17.13.png"></p>
<p>最后k-means聚类的中心是因子$\alpha_1,…,\alpha_6​$，它们最小化了上述错误方程（$Err(\alpha)​$），且代表了元图像块应该聚类为一个层。</p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>CS131</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131 Lecture 3 Linear Algebra Primer Part 2</title>
    <url>/2018/07/08/Lecture%203%20Linear%20Algebra%20Primer%20Part%202/</url>
    <content><![CDATA[<h1 id="Lecture-3-Linear-Algebra-Primer-Part-2"><a href="#Lecture-3-Linear-Algebra-Primer-Part-2" class="headerlink" title="Lecture 3 Linear Algebra Primer Part 2"></a>Lecture 3 Linear Algebra Primer Part 2</h1><h2 id="Transformation-Matrices"><a href="#Transformation-Matrices" class="headerlink" title="Transformation Matrices"></a>Transformation Matrices</h2><p>矩阵可以用多种方式将向量转换，最简单的例子是缩放：</p>
<p>$\left [ \begin{matrix} s_{x} &amp; 0 \ 0 &amp; s_{y} \end{matrix} \right ]  $ x $\left [ \begin{matrix} x \ y \end{matrix} \right ]$ &#x3D; $\left [ \begin{matrix} s_{x} x \  s_{y}y \end{matrix} \right ]$</p>
<h3 id="Rotate-旋转"><a href="#Rotate-旋转" class="headerlink" title="Rotate 旋转"></a>Rotate 旋转</h3><p>也可以用矩阵来逆时针旋转向量$\theta$角：</p>
<p>$x’&#x3D;cos\theta x-sin\theta y\ y’&#x3D;cos\theta y+sin\theta x\ R&#x3D;\left [ \begin{matrix} cos\theta &amp; -sin\theta \ sin\theta &amp; cos\theta \end{matrix} \right ]  $</p>
<h3 id="Scaling-缩放"><a href="#Scaling-缩放" class="headerlink" title="Scaling 缩放"></a>Scaling 缩放</h3><p><strong>Homogeneous Coordinates</strong> 齐次坐标，简而言之，用$N+1$维向量表示$N$维向量。</p>
<p><strong>Cartesian</strong> 笛卡尔坐标，也就是常说的xy坐标系</p>
<p>笛卡尔坐标于齐次坐标的转换：$(x,y,w)&#x3D;(x&#x2F;w,y&#x2F;w)$</p>
<p>所以，为了向原有向量添加常数，以下运用到齐次坐标的原理</p>
<p>$P&#x3D;\left [ \begin{matrix} x \ y \1 \end{matrix} \right ],S&#x3D; \left [ \begin{matrix} s_{x}&amp;0&amp;0 \ 0&amp; s_{y} &amp;0 \0&amp;0&amp;1 \end{matrix} \right ] \ P’&#x3D;S·P$</p>
<h3 id="Translating-移动"><a href="#Translating-移动" class="headerlink" title="Translating 移动"></a>Translating 移动</h3><p>$P&#x3D;\left [ \begin{matrix} x \ y \1 \end{matrix} \right ],T&#x3D; \left [ \begin{matrix} 1&amp;0&amp;t_{x} \ 0&amp; 1 &amp;t_{y} \0&amp;0&amp;1 \end{matrix} \right ] \ P’&#x3D;T·P$</p>
<p>以上旋转、缩放、移动可以同时应用，即$P’&#x3D;T·R·S·P$。作用顺序从右往左（可以交换），前式表示先移动后旋转再缩放。</p>
<h2 id="Matrix-Inverse-逆"><a href="#Matrix-Inverse-逆" class="headerlink" title="Matrix Inverse(逆)"></a>Matrix Inverse(逆)</h2><ul>
<li>$AA^{-1}&#x3D;I$</li>
<li>$(AB)^{-1}&#x3D;B^{-1}A^{-1}$</li>
<li>$A^{-T}&#x3D;(A^{T})^{-1}&#x3D;(A^{-1})^{T}$</li>
<li>非方矩阵的逆矩阵不存在</li>
</ul>
<h2 id="Pseudoinverse-伪逆矩阵"><a href="#Pseudoinverse-伪逆矩阵" class="headerlink" title="Pseudoinverse(伪逆矩阵)"></a>Pseudoinverse(伪逆矩阵)</h2><p>通常为了解方程$AX&#x3D;B$，会使用$X&#x3D;A^{-1}B$，在python上命令为<code>np.linalg.inv(A)*B</code>。但是有时候计算$A^{-1}$十分耗时，甚至$A^{-1}$可能不存在。</p>
<p>所以这时我们需要伪逆矩阵，具体原理暂时不了解，使用python命令<code>np.linalg.solve(A,B)</code>会返回方程$AX&#x3D;B$的最优解。</p>
<h2 id="Matrix-Rank-秩"><a href="#Matrix-Rank-秩" class="headerlink" title="Matrix Rank(秩)"></a>Matrix Rank(秩)</h2><ul>
<li>一个变换矩阵A的秩告诉你它会将矩阵转换到几个维度上，例如，$r(A)&#x3D;1$，则$p’&#x3D;Ap$返回一条线</li>
<li>col-rank：最大线性无关列数目</li>
<li>row-rank：最大线性无关行数目</li>
<li>满秩矩阵：$mxm\space &amp; \space r(A)&#x3D;m$</li>
</ul>
<h2 id="Eigenvalues-特征值-and-Eigenvectors-特征向量"><a href="#Eigenvalues-特征值-and-Eigenvectors-特征向量" class="headerlink" title="Eigenvalues(特征值) and Eigenvectors(特征向量)"></a>Eigenvalues(特征值) and Eigenvectors(特征向量)</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>当A点乘特征向量时，不会改变方向，只会放缩。即$Ax&#x3D;\lambda x,x\not &#x3D; 0$，$x$为特征向量，$\lambda$为特征值。</p>
<h3 id="求解"><a href="#求解" class="headerlink" title="求解"></a>求解</h3><ol>
<li>解$|\lambda I-A|&#x3D;0$得到特征值$\lambda$</li>
<li>带入原式得特征向量</li>
</ol>
<h3 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h3><ul>
<li>矩阵的迹&#x3D;特征值之和</li>
<li>行列式(determinant)A&#x3D;特征值之积</li>
<li>矩阵的秩&#x3D;非零特征值的个数</li>
<li>对角矩阵的特征值&#x3D;对角线上的值</li>
</ul>
<h3 id="Spectral-谱-Theory"><a href="#Spectral-谱-Theory" class="headerlink" title="Spectral(谱) Theory"></a>Spectral(谱) Theory</h3><p><strong>特征对(eigenpair)</strong> 特征值和其对应的特征向量</p>
<p><strong>谱(spectrum)</strong> 包含所有特征值的集合</p>
<p><strong>谱半径(spectrum radius)</strong> 绝对值最大的特征值，它的上界为该矩阵的无限范数</p>
<h3 id="Diagonalization-对角化"><a href="#Diagonalization-对角化" class="headerlink" title="Diagonalization 对角化"></a>Diagonalization 对角化</h3><p>若$nxn$矩阵A有n个线性无关特征值，则A可以对角化</p>
<ul>
<li>满足$A^{<em>}A&#x3D;AA^{</em>}$的矩阵可以对角化，其中$A^{*}$是A的复共轭矩阵</li>
<li>有n个相异(distinct)特征值的矩阵可对角化</li>
</ul>
<p>计算：对角化矩阵$A&#x3D;VDV^{T}$，其中V为特征向量组合成的矩阵，D为对角值为特征值的对角矩阵，DV相对应</p>
<h3 id="Symmetric-对称-Matrix"><a href="#Symmetric-对称-Matrix" class="headerlink" title="Symmetric(对称) Matrix"></a>Symmetric(对称) Matrix</h3><p>如果A是对称的，则其特征值为真，且特征向量标准正交(orthonormal)。</p>
<h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><ul>
<li>PageRank</li>
<li>薛定谔方程</li>
<li>PCA</li>
</ul>
<h2 id="矩阵计算"><a href="#矩阵计算" class="headerlink" title="矩阵计算"></a>矩阵计算</h2><h3 id="Gradient"><a href="#Gradient" class="headerlink" title="Gradient"></a>Gradient</h3><p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/3.2.png"></p>
<h3 id="Gradient-性质"><a href="#Gradient-性质" class="headerlink" title="Gradient 性质"></a>Gradient 性质</h3><p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/3.3.png"></p>
<h3 id="Hessian"><a href="#Hessian" class="headerlink" title="Hessian"></a>Hessian</h3><p>即梯度的梯度</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/3.4.png"></p>
<h3 id="Hessian-性质"><a href="#Hessian-性质" class="headerlink" title="Hessian 性质"></a>Hessian 性质</h3><ul>
<li>Hessian是对称的</li>
</ul>
<h3 id="矩阵计算例子"><a href="#矩阵计算例子" class="headerlink" title="矩阵计算例子"></a>矩阵计算例子</h3><h4 id="梯度例子"><a href="#梯度例子" class="headerlink" title="梯度例子"></a>梯度例子</h4><p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/3.5.png"></p>
<h4 id="Hessian例子"><a href="#Hessian例子" class="headerlink" title="Hessian例子"></a>Hessian例子</h4><p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/3.6.png"></p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>CS131</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131 Lecture 5 Edge Detection</title>
    <url>/2018/07/10/Lecture%205%20Edge%20detection/</url>
    <content><![CDATA[<h1 id="Lecture-5-Edge-Detection"><a href="#Lecture-5-Edge-Detection" class="headerlink" title="Lecture 5 Edge Detection"></a>Lecture 5 Edge Detection</h1><h2 id="Edge-Detection-in-Mammals"><a href="#Edge-Detection-in-Mammals" class="headerlink" title="Edge Detection in Mammals"></a>Edge Detection in Mammals</h2><h3 id="Hubel-amp-Wiesel"><a href="#Hubel-amp-Wiesel" class="headerlink" title="Hubel &amp; Wiesel"></a>Hubel &amp; Wiesel</h3><p>两个人的实验表明，猫的视觉神经在碰到特定方向的边缘时，会产生反应。</p>
<h3 id="Biederman"><a href="#Biederman" class="headerlink" title="Biederman"></a>Biederman</h3><p>Biederman的实验表明，人们在只看到物品一半的轮廓时，仍能识别物体且速度不受影响。这给计算机视觉提供了一个想法：即使只展现图像的一部分，一个系统理论上应该能够识别整个物品。</p>
<h3 id="Walther-Chai-Caddigan-Beck-amp-Fei-Fei"><a href="#Walther-Chai-Caddigan-Beck-amp-Fei-Fei" class="headerlink" title="Walther, Chai, Caddigan, Beck &amp; Fei-Fei"></a>Walther, Chai, Caddigan, Beck &amp; Fei-Fei</h3><p>一群研究者发现大脑的低层次在识别轮廓方面更强，而高层次对颜色识别能力更强。</p>
<h2 id="Edge-Detection-for-Computer-Vision"><a href="#Edge-Detection-for-Computer-Vision" class="headerlink" title="Edge Detection for Computer Vision"></a>Edge Detection for Computer Vision</h2><p>边缘检测的目的是检测图像中的不连续部分。直观来讲，图像的大部分语义学和形状信息可以在图像边缘被编码。边缘可以帮助我们提取信息、识别物体、恢复几何和视角。</p>
<h3 id="Types-of-Discrete-Derivative-离散导数-in-1D"><a href="#Types-of-Discrete-Derivative-离散导数-in-1D" class="headerlink" title="Types of Discrete Derivative(离散导数) in 1D"></a>Types of Discrete Derivative(离散导数) in 1D</h3><p>主要有三类，它们的公式和对应的滤波器为：</p>
<ul>
<li><p>Backward<br>$$<br>\dfrac{df}{dx}&#x3D;f(x)-f(x-1)&#x3D;f’(x) \<br>[0,1,-1]<br>$$</p>
</li>
<li><p>Forward<br>$$<br>\dfrac{df}{dx}&#x3D;f(x)-f(x+1)&#x3D;f’(x)\<br>[-1,1,0]<br>$$</p>
</li>
<li><p>Central<br>$$<br>\dfrac{df}{dx}&#x3D;f(x+1)-f(x-1)&#x3D;f’(x)\<br>[1,0,-1]<br>$$</p>
</li>
</ul>
<h3 id="Discrete-Derivative-in-2D"><a href="#Discrete-Derivative-in-2D" class="headerlink" title="Discrete Derivative in 2D"></a>Discrete Derivative in 2D</h3><ul>
<li><p>Gradient vector<br>$$<br>\nabla f(x,y)&#x3D; \left[<br> \begin{matrix}<br>  f_{x}\<br>   f_{y}<br>  \end{matrix}<br>  \right]<br>$$</p>
</li>
<li><p>Gradient magnitude<br>$$<br>|\nabla f(x,y)|&#x3D;\sqrt{f_{x}^{2}+f_{y}^{2}}<br>$$</p>
</li>
<li><p>Gradient direction<br>$$<br>\theta&#x3D;tan^{-1}(\frac{\frac{df}{dy}}{\frac{df}{dx}})<br>$$</p>
</li>
</ul>
<h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p>矩阵的梯度可以近似为用基于中心离散倒数方程的相邻像素拓展到2D。一个水平滤波器如下：<br>$$<br>\frac{1}{3}<br>\left[<br> \begin{matrix}<br>  -1&amp;0&amp;1\<br>   -1&amp;0&amp;1\<br>   -1&amp;0&amp;1<br>  \end{matrix}<br>  \right]<br>$$<br>当这个滤波器覆盖在像素$x[m,n]$上，可以产生一个输出。这个输出近似于于像素$(m,n)$在水平方向上的梯度，这个滤波器检测水平边缘，同样需要一个单独的内核检测垂直边缘。</p>
<h2 id="Simple-Edge-Detectors"><a href="#Simple-Edge-Detectors" class="headerlink" title="Simple Edge Detectors"></a>Simple Edge Detectors</h2><h3 id="Characterizing-Edges"><a href="#Characterizing-Edges" class="headerlink" title="Characterizing Edges"></a>Characterizing Edges</h3><p>描述边缘是检测边缘的第一步，是为了边缘可以被识别。首先，定义边缘为图片的强度函数快速变化的位置，也是导数较大的位置。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/5.1.png"></p>
<h3 id="Image-Gradient"><a href="#Image-Gradient" class="headerlink" title="Image Gradient"></a>Image Gradient</h3><p>图片的梯度可以被定义为<br>$$<br>\nabla f(x,y)&#x3D;[\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}]<br>$$<br>同时，方向与边缘强度可以被定义为<br>$$<br>\theta&#x3D;tan^{-1}(\frac{\partial f}{\partial y}&#x2F;\frac{\partial f}{\partial x})\<br>||\nabla f(x,y)||&#x3D;\sqrt{(\frac{\partial f}{\partial y})^{2}+(\frac{\partial f}{\partial y})^{2}}<br>$$<br><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/5.2.png"></p>
<p>梯度向量的方向指向强度变化最快的方向，如上图所示。下图为将梯度应用于图像后的结果。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/5.3.png"></p>
<h3 id="Effects-of-Noise"><a href="#Effects-of-Noise" class="headerlink" title="Effects of Noise"></a>Effects of Noise</h3><p>如果边缘噪音过多，偏导数可能无法很好的检测边缘，如下图。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/5.4.png"></p>
<p>为了减少噪音影响，图片首先要光滑化。光滑化是重新计算像素值，使得该像素值和周围像素更相似的过程。光滑化是通过将滤波器和图像卷积来实现的（例如：高斯核）。</p>
<p>当然，光滑化也有损失：模糊了边缘；较大的滤波器会导致边缘损失和图片有用的细节损失。</p>
<p>总而言之，合适的光滑化可以促进边缘检测。在光滑化$f$后，就可以计算$f*\frac{d}{dx}g$，顶峰处为边缘。</p>
<h3 id="Gaussian-Blur"><a href="#Gaussian-Blur" class="headerlink" title="Gaussian Blur"></a>Gaussian Blur</h3><p>高斯模糊是用高斯函数减少图片噪音的结果。它是一个低通滤波器，用于降低高频率信号。</p>
<p>一维<br>$$<br>G(x)&#x3D;\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{x^{2}}{2\sigma^{2}}}<br>$$<br>二维<br>$$<br>G(x,y)&#x3D;\frac{1}{2\pi\sigma}e^{-\frac{x^{2}+y^{2}}{2\sigma^{2}}}<br>$$</p>
<h2 id="Designing-a-Good-Edge-Detector"><a href="#Designing-a-Good-Edge-Detector" class="headerlink" title="Designing a Good Edge Detector"></a>Designing a Good Edge Detector</h2><p>一个可行的边缘检测器必须有以下性质：</p>
<ol>
<li><p>检测效果好</p>
<p>必须最小化检测为误报（false positives,由噪音导致的假性边缘）和漏报率（false negatives，漏掉真的边缘）的可能性。</p>
</li>
<li><p>定位好</p>
<p>检测的边缘位置必须与原图实际边缘吻合。检测器还必须在检测哪些像素位于边缘上保持一致。</p>
</li>
<li><p>Silent response</p>
<p>检测器必须最小化真正边缘附近的局部极大值，即每个真正的边缘点只返回一个点。它应该要告知这里有一个特定的边缘，而不是将一个边缘分散为数个边缘。换句话说，只有真正的边缘会被捕捉到。</p>
<p>下图是正确的边缘；1稳健性差；2定位差；3过多响应</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/5.5.png"></p>
</li>
</ol>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>CS131</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131 Lecture 4 Pixels and Filters</title>
    <url>/2018/07/09/Lecture%204%20Pixels%20and%20Filters/</url>
    <content><![CDATA[<h1 id="Lecture-4-Pixels-and-Filters"><a href="#Lecture-4-Pixels-and-Filters" class="headerlink" title="Lecture 4 Pixels and Filters"></a>Lecture 4 Pixels and Filters</h1><h2 id="Image-Sampling-and-Quantization"><a href="#Image-Sampling-and-Quantization" class="headerlink" title="Image Sampling and Quantization"></a>Image Sampling and Quantization</h2><h3 id="Image-Type"><a href="#Image-Type" class="headerlink" title="Image Type"></a>Image Type</h3><ul>
<li><p>Binary Type 每个像素不是黑(0)就是白(1)</p>
</li>
<li><p>Grayscale Images 比起Binary Type，每个像素在黑白之间有更大的范围(0-255)，即多了中间的灰色部分</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/4.5.png"></p>
</li>
<li><p>Color Images 有多个颜色通道，每张图片可以在不同的颜色模型(RGB, LAB, HSV)上呈现，每个颜色通道值的范围取决于所选的颜色模型。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/4.6.png"></p>
</li>
</ul>
<h3 id="Sampling-and-Resolution"><a href="#Sampling-and-Resolution" class="headerlink" title="Sampling and Resolution"></a>Sampling and Resolution</h3><p>图片是采样的，由离散的像素组成，不是连续的，所以可能因为像素密度造成图片有颗粒感。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/4.1.png"></p>
<ul>
<li>Resolution 一个取样参数，由dots per inch(DPI)确定。标准DPI&#x3D;72</li>
<li>Pixels 像素是可量化的，用一系列(通常为0-255)的值表示。量化和采样会因为有限的精度而丢失信息</li>
</ul>
<h2 id="Image-Histograms-直方图"><a href="#Image-Histograms-直方图" class="headerlink" title="Image Histograms(直方图)"></a>Image Histograms(直方图)</h2><p>直方图用于测试灰度图的强度：一个特定的像素值(0-255)在图像中出现了几次。下图为在两个水平方向上的取样</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/4.2.png"></p>
<p>直方图可以帮助我们删除图中某些特征，例如：</p>
<ul>
<li>天空：光滑的曲线表示图像着色的一致性，和图中的天空一样</li>
<li>草：锯齿状的曲线表示着色变化范围大，和图中草的阴影部分一样</li>
</ul>
<p>直方图提供了一个物品的量化描述，所以可以作为分类器(classifiers)的输入。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">histogram</span>(<span class="params">im</span>):</span><br><span class="line">    h = np.zeros(<span class="number">255</span>)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> im.shape[<span class="number">0</span>]:</span><br><span class="line">        <span class="keyword">for</span> col <span class="keyword">in</span> im.shape[<span class="number">1</span>]:</span><br><span class="line">            val = im[row, col]</span><br><span class="line">            h[val] += <span class="number">1</span></span><br></pre></td></tr></table></figure>



<h2 id="Images-as-Functions"><a href="#Images-as-Functions" class="headerlink" title="Images as Functions"></a>Images as Functions</h2><p>计算机视觉处理的图像大部分是数字的，即为一种离散的形式。这种离散化是通过对常规网格上的二维空间采样得到的，最后用一个整数数值的矩阵表示图像。</p>
<p>处理图片时，我们可以将所有图片视作一个无限宽高的矩阵。但是，放置的图片只是一个有限的子矩阵。因此我们可以将图片写作矩阵的坐标。</p>
<p>也可以用$f[m,n]$表示坐标(m,n)上的像素的强度，方括号表示$f[m,n]$是离散函数。</p>
<p>例如：</p>
<ul>
<li>灰阶图：$f:[a,b]\times [c,d] \rightarrow [0,255]$ ，表示区域宽在ab之间，区域长在cd之间，值的范围在0-255</li>
<li>彩图：$g[x,y]&#x3D;\left [ \begin{matrix} r[x,y] \ g[x,y] \b[x,y] \end{matrix} \right ] $ ，其中$r,g,b:[a,b]\times [c,d] \rightarrow 255$</li>
</ul>
<h2 id="Linear-Systems-Filters"><a href="#Linear-Systems-Filters" class="headerlink" title="Linear Systems (Filters)"></a>Linear Systems (Filters)</h2><p>$filtering$：转换像素值形成新图片的过程，目的是为了提取有用信息（例：边缘检测）或调整图片的视觉特性（例：降噪）。</p>
<p>过滤器是系统的一个例子，也是将输入函数$f[m,n]$转换到输出函数$g[m,n]$的单位。</p>
<p>符号$S$指系统操作员(system operator)，它将一组可能的输出映射到一组可能的输入，可以写作<br>$$<br>f[m,n] \rightarrow System\space S \rightarrow g[m,n]\<br>S[g]&#x3D;f \<br>S{f[m,n]}&#x3D;g[m,n]\<br>f[m,n] \xrightarrow[]{S} g[m,n]<br>$$</p>
<h3 id="Examples-of-Filters"><a href="#Examples-of-Filters" class="headerlink" title="Examples of Filters"></a>Examples of Filters</h3><h4 id="Moving-Average"><a href="#Moving-Average" class="headerlink" title="Moving Average"></a>Moving Average</h4><p>这种过滤器将一个像素值设置为周围像素的平均值，在2D上即为3x3的9个像素整合为1个，数学表达为<br>$$<br>g[m,n]&#x3D;\frac{1}{9}  \sum\limits_{i&#x3D;-1}^1 \sum\limits_{j&#x3D;-1}^1 f[m-i,n-j]<br>$$<br>这种加权平均(Weighted Average)过滤器可以将尖锐的边缘光滑化，产生模糊或者光滑的效果。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/4.7.png"></p>
<h4 id="Image-Segmentation"><a href="#Image-Segmentation" class="headerlink" title="Image Segmentation"></a>Image Segmentation</h4><p>基于简单的阈值(Threshold)系统可以做到基本的图像分割，数学上表示为<br>$$<br>g[m,n]&#x3D; \left{<br>             \begin{array}{lr}<br>            255, &amp;f[m,n]\geq t\<br>            0, &amp;otherwise<br>             \end{array}<br>\right.<br>$$<br>这种基本的图像分割过滤器将像素划分为二元分类器，非黑即白，取决于阈值函数。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/4.8.png"></p>
<h3 id="Properties-of-Systems"><a href="#Properties-of-Systems" class="headerlink" title="Properties of Systems"></a>Properties of Systems</h3><p>以下是系统<strong>可能有</strong>的性质</p>
<h4 id="Amplitude-振幅-Properties"><a href="#Amplitude-振幅-Properties" class="headerlink" title="Amplitude(振幅) Properties"></a>Amplitude(振幅) Properties</h4><p>可加性、同次性、叠加性、稳定性、可逆性</p>
<p><img src="D:/Machine%20Learning/cs131/Lecture3_4/4.3.png" alt="4.3"></p>
<h4 id="Spatial-Properties"><a href="#Spatial-Properties" class="headerlink" title="Spatial Properties"></a>Spatial Properties</h4><p>因果性、移位不变性</p>
<p><img src="D:/Machine%20Learning/cs131/Lecture3_4/4.4.png" alt="4.4"></p>
<h3 id="Linear-Systems"><a href="#Linear-Systems" class="headerlink" title="Linear Systems"></a>Linear Systems</h3><p>线性系统指满足叠加性的系统。当应用一个线性系统进行过滤，我们对每个原始像素值使用相同的权重集进行加权求和，得到新的像素值。一个线性移位不变系统(Linear shift invariant systems,LSI)是具有移位不变性的线性系统。</p>
<p>线性系统还具有脉冲响应(impulse response)。函数$\delta_{2}[m,n]$由以下定义：<br>$$<br>\delta_{2}[m,n]&#x3D;\left{<br>             \begin{array}{lr}<br>            1, &amp; m&#x3D;0\space and \space n&#x3D;0\<br>            0, &amp;otherwise<br>             \end{array}<br>\right.<br>$$<br>则脉冲响应$r&#x3D;S[\delta_{2}]$</p>
<p>而一个简单的线性移位不变系统根据delta函数的移位特性来移动像素。<br>$$<br>f[m,n]&#x3D;\sum\limits_{i&#x3D;- \infty}^\infty \sum\limits_{j&#x3D;-\infty}^\infty f[i,j]\delta_{2}[m-i,n-j]<br>$$<br>接下来，我们可以定义线性移位不变系统的过滤器$h$<br>$$<br>h[m,n]&#x3D;\alpha_{1}\delta_{2,1}[m-i,n-j] +\alpha_{2}\delta_{2,2}[m-i,n-j]+\ldots<br>$$<br>对于所有线性系统，Delta[m,n]函数：在某个特定像素值为1，给予一个回应。h[m,n]函数：一个移动的delta函数，即基于一个移动回应。</p>
<p>LSI例子：</p>
<ol>
<li><p>一个移动平均过滤器即为LSI，它是脉冲响应的总和，并且满足以下条件：</p>
<ul>
<li>系统满足叠加性</li>
<li>有脉冲响应：$S[\delta_{2}[m,n]]&#x3D;\delta_{2}[m,n]$</li>
<li>离散卷积(convolution)：$f[n,m]*h[n,m]$(原始函数脉冲响应进行移位)</li>
</ul>
</li>
<li><p>阈值系统不是线性系统。因为以下反例<br>$$<br>f_{1}[m,n]+f_{2}[m,n]&gt;T \<br>f_{1}[m,n]&lt;T\<br>f_{2}[m,n]&lt;T<br>$$</p>
</li>
</ol>
<h2 id="Convolution-and-Correlation"><a href="#Convolution-and-Correlation" class="headerlink" title="Convolution and Correlation"></a>Convolution and Correlation</h2><h3 id="Convolution-卷积"><a href="#Convolution-卷积" class="headerlink" title="Convolution 卷积"></a>Convolution 卷积</h3><p>最简单解释卷积的方式是把它当作一个用周围像素的信息来整合成目标像素的系统，如：平均移动系统。</p>
<p>卷积的符号为$*$，例如：$f[m,n]*h[m,n]$表示函数和移动脉冲响应相乘。</p>
<p>卷积让我们可以通过考虑系统脉冲响应，简单的计算任何经过系统的输入信号的输出。首先，我们要理解图和将信号化为一系列的脉冲函数。</p>
<p>前文已经提过，任何信号都可以分解为脉冲函数$\delta[n]$的加权和，即任何信号$x$都可以写作<br>$$<br>x[n]&#x3D;\sum\limits_{k&#x3D;- \infty}^\infty x[k]\delta[n-k]<br>$$<br>系统的脉冲响应为$h[n]$，是将脉冲函数输入系统产生的输出。缩放一个线性系统的脉冲函数将导致脉冲响应以同样的大小缩放。若系统是移动不变的，移动脉冲函数也会同样的移动脉冲响应。下图体现了这些性质。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/4.9.png"></p>
<p>所以，将信号$x[n]&#x3D;\sum_{k&#x3D;- \infty}^\infty x[k]\delta[n-k]$输入一个线性、移动不变的系统，得到输出为$y[n]&#x3D;\sum_{k&#x3D;- \infty}^\infty x[k]h[n-k]$。</p>
<p>同样，卷积可以在二维上执行，有以下几种计算方法。</p>
<ul>
<li>将信号$x[n，m]&#x3D;\sum_{i&#x3D;- \infty}^\infty \sum_{j&#x3D;- \infty}^\infty x[i,j]\delta[n-i,m-j]$输入一个线性、移动不变的系统，得到输出为$y[n，m]&#x3D;\sum_{i&#x3D;- \infty}^\infty \sum_{j&#x3D;- \infty}^\infty x[i,j]h[n-i,m-j]$。</li>
</ul>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/4.10.png"></p>
<ul>
<li>将卷积核(Kernel)旋转180°，并将原始图像进行zero-padding（用0在四周添加边缘），最后在pad的图像上滑动卷积核进行加权求和。</li>
</ul>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/4.11.png"></p>
<ul>
<li>利用傅里叶变换<br>$$<br>F(f<em>h)&#x3D;F(f)·F(h) \<br>f</em>h&#x3D;F^{-1}(F(f)·F(h))<br>$$</li>
</ul>
<p>实例：</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/4.12.png"></p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/4.13.png"></p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/4.14.png"></p>
<h4 id="Sharpening-Filter-锐化过滤器"><a href="#Sharpening-Filter-锐化过滤器" class="headerlink" title="Sharpening Filter 锐化过滤器"></a>Sharpening Filter 锐化过滤器</h4><p>强调和原图平均水平的差异</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/4.15.png"></p>
<h3 id="Correlation-相关性"><a href="#Correlation-相关性" class="headerlink" title="Correlation 相关性"></a>Correlation 相关性</h3><p>等于卷积计算时内核没有翻转的结果。二维交叉相关为：<br>$$<br>r[k，l]&#x3D;\sum\limits_{i&#x3D;- \infty}^\infty \sum_{j&#x3D;- \infty}^\infty f[m+k,n+l]g[m,n]  \<br>&#x3D;f[n,m]<em>g^{</em>}[-n,-m]<br>$$<br>其中$g^{<em>}$表示$g$的共轭复数，在本节课中，$g(n,m)$为实数，所以$g^{</em>}&#x3D;g$</p>
<p>和卷积的区别：</p>
<ul>
<li>卷积是一个积分，它表示当一个函数在另一个函数上移动的时候的重叠部分。也就是说，卷积是一个过滤操作。</li>
<li>相关性比较了两个数据集的相似性。相关性计算了两个输入函s数相互移动时的相似性测量值。两个函数匹配都越高，它的结果值越大。也就是说，相关性是两个信号关联性的测量值。</li>
</ul>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/4.16.png"></p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>CS131</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131 Lecture 2 Color and Linear Algebra</title>
    <url>/2018/07/07/Lecture%202%20Color%20and%20Linear%20Algebra/</url>
    <content><![CDATA[<h1 id="Lecture-2-Color-and-Linear-Algebra"><a href="#Lecture-2-Color-and-Linear-Algebra" class="headerlink" title="Lecture 2 Color and Linear Algebra"></a>Lecture 2 Color and Linear Algebra</h1><h2 id="颜色物理学"><a href="#颜色物理学" class="headerlink" title="颜色物理学"></a>颜色物理学</h2><h3 id="什么是颜色？"><a href="#什么是颜色？" class="headerlink" title="什么是颜色？"></a>什么是颜色？</h3><p>颜色是环境中的物理光和我们视觉系统交互的结果。它是一种我们在看物品和光时的视觉体验的心理性质，而不是这些物品和光的物理性质。</p>
<h3 id="颜色和光"><a href="#颜色和光" class="headerlink" title="颜色和光"></a>颜色和光</h3><p>白光由几乎平均能量的可视光谱的波长组成</p>
<h3 id="电磁光谱"><a href="#电磁光谱" class="headerlink" title="电磁光谱"></a>电磁光谱</h3><p>光由不同波长的波组成，可视光谱范围从400nm到700nm，人类对可视光谱中间部分最敏感。人类只能看到可视光谱的原因的太阳发射黄光最多，而且温度较高。</p>
<h3 id="可视光"><a href="#可视光" class="headerlink" title="可视光"></a>可视光</h3><p>Plank’s Law基于星球的表面温度测量电磁辐射发出的波长。</p>
<h3 id="物理光"><a href="#物理光" class="headerlink" title="物理光"></a>物理光</h3><p>任何光都可以用它的光谱描述（每秒钟发射的波长在400-700nm之间的能量）。物品反射的光时常集中于可视光谱的特定部分，因此物品呈现出不同颜色，例如：香蕉的黄色、番茄的红色。</p>
<h3 id="光和表面的交互"><a href="#光和表面的交互" class="headerlink" title="光和表面的交互"></a>光和表面的交互</h3><p>反射的颜色是光源范围和物品表面反射交互的结果。通常，单位和定义是 per unit wavelengths，关系是光谱而不是单独的波长。光照度被量化为：光照度*反射度&#x3D;颜色信号</p>
<h2 id="人类对颜色的编码"><a href="#人类对颜色的编码" class="headerlink" title="人类对颜色的编码"></a>人类对颜色的编码</h2><h3 id="Rods-and-Cones"><a href="#Rods-and-Cones" class="headerlink" title="Rods and Cones"></a>Rods and Cones</h3><p>视网膜上主要包含两种光敏感细胞：rods and cones。Rods数量更多且更敏感，用于低光环境下的物品检测。而cones则相反，数量少且不敏感，用于高光环境下的检测。这两种细胞帮助我们通过一种机制感知颜色。</p>
<h3 id="Cones和颜色"><a href="#Cones和颜色" class="headerlink" title="Cones和颜色"></a>Cones和颜色</h3><p>rods和cones的主要不同处在于cones以三种不同形式出现，每种以一个独特的对光不同波长的反应曲线为特征。每个反应曲线高峰位于440（蓝）, 530（绿）, 560nm（红）。然而，rods和cones运作方式像过滤器，输出的结果是光谱上所有波长的整合，所以一些信息会丢失。这意味着一些光谱的子集会被错误的整合成一样的，这种光谱称称为metamers</p>
<h3 id="颜色匹配"><a href="#颜色匹配" class="headerlink" title="颜色匹配"></a>颜色匹配</h3><p>因为我们要设计一个对所有人都提供一致视觉体验的系统，所以理解可以创造所有可感知颜色的最小数量的颜色很有帮助。通过一系列研究，红黄蓝是最充分的编码颜色。</p>
<h2 id="色彩空间"><a href="#色彩空间" class="headerlink" title="色彩空间"></a>色彩空间</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>色彩空间是一个抽象的数学模型，它用数字元组描述了一些列颜色，例如RGB。一个颜色空间可以是任意的或者有数学结构的，几乎所有颜色模型都能匹配到一个绝对的和全球的颜色解释理解系统。</p>
<h3 id="线性颜色空间"><a href="#线性颜色空间" class="headerlink" title="线性颜色空间"></a>线性颜色空间</h3><p>由三原色定义，其数值代表某种原色的权重。因为两种原色只能在直线上表示颜色，但三种原色可以在空间上表示颜色。</p>
<ul>
<li><p>RGB Space</p>
<ul>
<li>原色是单色（对于监视器，它们对应三种不同的磷光体）</li>
<li>减法匹配需要光的特定波长</li>
<li>下图RGB的原色和匹配函数。匹配函数表示在水平尺度上显示的波长的单色测试颜色所需的原色数量。</li>
</ul>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/2.1.png"></p>
</li>
<li><p>CIE XYZ Color Space</p>
<ul>
<li>原色是虚构的，但匹配方程处处为正</li>
<li>Y向量对应一个颜色的亮度</li>
<li>通过线性转化和RGB Space相关联，在Grassmann’s Law之上</li>
</ul>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/2.2.png"></p>
</li>
</ul>
<h3 id="非线性颜色空间：HSV"><a href="#非线性颜色空间：HSV" class="headerlink" title="非线性颜色空间：HSV"></a>非线性颜色空间：HSV</h3><ul>
<li><p>用于反射更多传统和直觉色彩混合模型（例如：颜料混合）</p>
</li>
<li><p>基于颜色如何在人的视觉中组织和概念化。</p>
</li>
<li><p>维度：色彩、饱和度、强度</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/2.3.png"></p>
</li>
</ul>
<h2 id="白平衡"><a href="#白平衡" class="headerlink" title="白平衡"></a>白平衡</h2><h3 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h3><p>白平衡是将传感器接收到的图片数据调整至合适的呈现中性的颜色（灰、白等等）的过程。</p>
<h3 id="白平衡的重要性"><a href="#白平衡的重要性" class="headerlink" title="白平衡的重要性"></a>白平衡的重要性</h3><ol>
<li>相机的传感器和人眼感受不同</li>
<li>不同的显示媒体渲染图像的方式不同</li>
<li>照片被拍下时的观察条件和照片的观察条件不同</li>
</ol>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/2.4.png"></p>
<h3 id="Von-Kries-Method"><a href="#Von-Kries-Method" class="headerlink" title="Von Kries Method"></a>Von Kries Method</h3><p>按增益系数将每个通道缩放至匹配灰度中性物品的外观。通过Gray Card Method（灰卡方法）实现：假设一个卡的中性值为$r_{w},g_{w},b_{w}$，那么我们将每个通道通过$1&#x2F;r_{w},1&#x2F;g_{w},1&#x2F;b_{w}$缩放。</p>
<h3 id="其他方法"><a href="#其他方法" class="headerlink" title="其他方法"></a>其他方法</h3><p>如果不用灰卡方法，我们需要猜测哪个像素对应白色物体。</p>
<ul>
<li><p>Gray World Assumption</p>
<p>假设平均像素值为灰值$r_{ave},g_{ave},b_{ave}$，那么我们将每个通道通过$1&#x2F;r_{ave},1&#x2F;g_{ave},1&#x2F;b_{ave}$缩放。</p>
</li>
<li><p>Brightest Pixel Assumption</p>
<p>适用于不饱和图像，对每个通道依据最亮的像素以反比例加权。</p>
</li>
<li><p>Gamut Mapping </p>
<p>Gamut是图像上所有像素颜色的一个集合，也是所有颜色可能结合的子集。接下来我们可以将图片的gamut映射到标准白光下的gamut</p>
</li>
</ul>
<h3 id="颜色在计算机视觉中的其他应用"><a href="#颜色在计算机视觉中的其他应用" class="headerlink" title="颜色在计算机视觉中的其他应用"></a>颜色在计算机视觉中的其他应用</h3><p>皮肤检测、图像分割</p>
<h2 id="线性代数引入：向量与矩阵"><a href="#线性代数引入：向量与矩阵" class="headerlink" title="线性代数引入：向量与矩阵"></a>线性代数引入：向量与矩阵</h2><h3 id="向量"><a href="#向量" class="headerlink" title="向量"></a>向量</h3><p>列向量(column vector)$v$, 行向量(row vector)$v^{T}$ ，CS131默认使用列向量。</p>
<h3 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h3><p><strong>matrix</strong> m行n列，若m&#x3D;n则为square</p>
<p><strong>image</strong> 在python中表现为一个像素亮度矩阵，左上角表示为[y,x]。</p>
<p><strong>Grayscale images</strong> 储存于$mxn$矩阵中</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/2.5.png"></p>
<p><strong>Color images</strong> 储存于$mxnx3$矩阵中，多了一个维度表示RGB三个值</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/2.6.png"></p>
<h3 id="基本矩阵运算"><a href="#基本矩阵运算" class="headerlink" title="基本矩阵运算"></a>基本矩阵运算</h3><p><strong>Norm</strong> $||x||<em>{2}&#x3D;\sqrt{\sum\limits</em>{i&#x3D;1}^{n}x^{2}_{i}}$</p>
<p>更广泛来讲，norm是任何满足以下条件的函数：</p>
<ol>
<li>非负性</li>
<li>当且仅当$x&#x3D;0$时，$f(x)&#x3D;0$</li>
<li>$f(tx)&#x3D;|t|f(x)$</li>
<li>$f(x+y)\leq f(x)+f(y)$</li>
</ol>
<p>例如：</p>
<p><strong>One Norm</strong> $||x||<em>{1}&#x3D;\sum\limits</em>{i&#x3D;1}^{n}|x_{i}|$</p>
<p><strong>Infinity Norm</strong> $||x||<em>{inf}&#x3D;max</em>{i}|x_{i}|$</p>
<p><strong>General P Norm</strong> $||x||<em>{p}&#x3D;(\sum\limits</em>{i&#x3D;1}^{n}x_{i}^{p})^{1&#x2F;p}$</p>
<p><strong>Matrix Norm</strong>  $||A||<em>{F}&#x3D;\sqrt{\sum\limits</em>{i&#x3D;1}^{n}\sum\limits_{j&#x3D;1}^{n}A^{2}_{ij}}&#x3D;\sqrt{tr(A^{T}A)}$</p>
<p><strong>Inner or Dot Product</strong> 内积或点乘，返回 <strong>Projection</strong>。如果B是一个单位向量，则A·B返回A在B方向上的长度</p>
<p><strong>Determinant</strong> $det(A)$返回一个标量，$A&#x3D;\left [ \begin{matrix} a &amp; b \ c &amp; d \end{matrix} \right ]  $ 则$det(A)&#x3D;ad-bc$</p>
<p>性质：</p>
<ul>
<li>$det(A^{-1})&#x3D;1&#x2F;det(A)$</li>
<li>$det(A^{T})&#x3D;det(A)$</li>
</ul>
<p><strong>Trace</strong> $tr(A)$为对角线上元素和</p>
<p>性质：$tr(A+B)&#x3D;tr(A)+tr(B)$</p>
<p><strong>Transpose</strong> 转置矩阵，表示为$A^{T}$，且$(ABC)^{T}&#x3D;C^{T}B^{T}A^{T}$</p>
<p><strong>Identity Matrix</strong> 缩写为$I​$ ，表示单位矩阵</p>
<p><strong>Diagonal Matrix</strong> 和单位矩阵区别在于对角线上不一定是1，可以为其他数字</p>
<p><strong>Symmetric Matrix</strong> 对称矩阵$A^{T}&#x3D;A$</p>
<p><strong>Skew-symmetric Matrix</strong> $A^{T}&#x3D;-A$</p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>CS131</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131 Lecture 6-7 Features and Fitting/Feature Descriptor</title>
    <url>/2018/07/13/Lecture%206-7%20Features%20and%20FittingFeature%20Descriptor/</url>
    <content><![CDATA[<h1 id="Lecture-6-7-Features-and-Fitting-x2F-Feature-Descriptor"><a href="#Lecture-6-7-Features-and-Fitting-x2F-Feature-Descriptor" class="headerlink" title="Lecture 6-7 Features and Fitting&#x2F;Feature Descriptor"></a>Lecture 6-7 Features and Fitting&#x2F;Feature Descriptor</h1><h2 id="Local-Invariant-局部不变-Features"><a href="#Local-Invariant-局部不变-Features" class="headerlink" title="Local Invariant(局部不变) Features"></a>Local Invariant(局部不变) Features</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>使用局部不变的特征的目的出于它在广泛环境下的有用性，之前讨论的方法在这些环境下均产生问题，例如：交叉相关。这个方法通过找到图像中局部、独特的结构，并用周围区域来作为小块，而不是使用全局作为代表来找到相对应的结构。这样做可以得到一个强健性更高的图片检测策略，该策略对物体旋转、视角改变、尺度变换等具有不变性。</p>
<h3 id="General-Approach"><a href="#General-Approach" class="headerlink" title="General Approach"></a>General Approach</h3><ol>
<li>找到一系列特别的关键点</li>
<li>在关键点附近定义一个局部区域</li>
<li>从该区域提取并归一化局部内容</li>
<li>从归一化的区域中计算一个局部描述子(local descriptor)，例如：像素颜色函数</li>
<li>匹配局部描述子</li>
</ol>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/6.7.png"></p>
<h3 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h3><p>好的局部特征应该具有以下性质：</p>
<ol>
<li><p>重复性</p>
<p>同一个物体或场景在不同摄像情况下（例如，光照或者视角改变），应该都能检测到大量的特征。换句话说，该特征需要对光线变化、噪音、模糊等具有稳健性，同时对旋转和视角改变也保持不变性。</p>
</li>
<li><p>局部性</p>
<p>特征需要是局部的以避免背景遮挡(occlusion)和混淆(clutter)导致的问题</p>
</li>
<li><p>数量</p>
<p>需要足够多的特征被选择去有效的检测物体</p>
</li>
<li><p>特殊性</p>
<p>特征需要有能展现出大量变换的特点，这样才能保证可以区分不同特征。</p>
</li>
<li><p>效率</p>
<p>新图像的特征匹配需要有利于实时应用</p>
</li>
</ol>
<h2 id="Keypoint-Localization"><a href="#Keypoint-Localization" class="headerlink" title="Keypoint Localization"></a>Keypoint Localization</h2><h3 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h3><p>关键点定位目标是持续且重复地检测区域、达到更精确的定位、在图像中找到游泳的内容。</p>
<h3 id="General-Approach-1"><a href="#General-Approach-1" class="headerlink" title="General Approach"></a>General Approach</h3><p>我们寻找角点，因为它们在大量图像中都是可重复且有特点的。为了找到角点，我们需要寻找在所有维度上强度剧烈变化的地方，也是梯度有两个以上主要方向的地方。为了提供上下文，一个“平整”的区域在任何方向上都不会改变且边沿的方向不会产生变化。我们用Harris技术找到这些角点。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/6.10.png"></p>
<h3 id="Harris-Detector"><a href="#Harris-Detector" class="headerlink" title="Harris Detector"></a>Harris Detector</h3><p>计算移动$[u,v]$强度的改变量$E(u,v)$，其中$I(x,y)$表示强度函数，$w(x,y)$表示窗函数（用于对信号进行截断，也叫截断函数）：<br>$$<br>E(u,v)&#x3D;\sum _{x,y}w(x,y)[I(x+u,y+v)-I(x,y)]^{2}<br>$$<br><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/6.11.png"></p>
<p>为了找到角点，我们需要最大化函数$E(u,v)$。用Taylor Expansion可以得到以下方程：<br>$$<br>E(u,v)&#x3D;\left[ \begin{matrix}<br>u &amp;v<br>\end{matrix} \right]<br>M<br>\left[ \begin{matrix}<br>u \v<br>\end{matrix} \right]<br>$$<br>其中M定义为<br>$$<br>M&#x3D;\sum <em>{x,y}w(x,y)\left[ \begin{matrix}<br>I</em>{x}I_{x} &amp;I_{x}I_{y}\<br>I_{x}I_{y}  &amp; I_{y}I_{y}<br>\end{matrix} \right] \<br>\left[ \begin{matrix}<br>I_{x}I_{x} &amp;I_{x}I_{y}\<br>I_{x}I_{y}  &amp; I_{y}I_{y}<br>\end{matrix} \right]&#x3D;x的梯度\times y的梯度<br>$$<br>这个矩阵显示<br>$$<br>M&#x3D;\left[ \begin{matrix}<br>\sum I_{x}I_{x} &amp; \sum I_{x}I_{y}\<br>\sum I_{x}I_{y}  &amp; \sum I_{y}I_{y}<br>\end{matrix} \right]&#x3D;<br>\left[ \begin{matrix}<br>\lambda_{1} &amp; 0\<br>0  &amp; \lambda_{2}<br>\end{matrix} \right]<br>$$<br>角点的两个特征值$\lambda_{1}$、$\lambda_{2}$都是大且相近的，而边缘只有其中一个特征值较大，平坦区域两个特征值均较小。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/6.12.png"></p>
<p>角点响应函数(Corner Response Function)为每个窗计算一个值：<br>$$<br>\theta&#x3D;det(M)-\alpha trace(M)^{2}<br>$$<br>其中$\alpha$范围为$[0.04,0.06]$。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/6.8.png"></p>
<p>为了增加旋转不变性，我们用经过加权和的高斯函数进行光滑化：<br>$$<br>M&#x3D;g(\sigma) * \left[ \begin{matrix}<br>I_{x}I_{x} &amp;I_{x}I_{y}\<br>I_{x}I_{y}  &amp; I_{y}I_{y}<br>\end{matrix} \right]<br>$$<br>最后，下图展示了Harris detector找到的关键点：</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/6.9.png"></p>
<h2 id="Scale-Invariant-Keypoint-Detection"><a href="#Scale-Invariant-Keypoint-Detection" class="headerlink" title="Scale Invariant Keypoint Detection"></a>Scale Invariant Keypoint Detection</h2><h3 id="Motivation-2"><a href="#Motivation-2" class="headerlink" title="Motivation"></a>Motivation</h3><p>之前我们用Harris detector找到角点的关键点。Harris detector为了维持良好的定位，用的窗较小。因为用的是小窗，所以会当图片缩放后，窗会受到影响，其梯度会发生改变。下图展现了同样大小的框在图片放大后的改变。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/7.1.png"></p>
<h3 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h3><p>我们可以设计一个可伸缩的函数，即窗对应的区域不受尺度变换影响（例如，平均强度）。我们可以用一个圈表示这个可伸缩函数。圆上一个点表示一个圆半径对应区域大小的函数，所以只要选取一个特征比较明显的点（如峰值），所对应的函数就能在不同的图中取得不同的窗大小。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/7.4.png"></p>
<h3 id="General-Approach-2"><a href="#General-Approach-2" class="headerlink" title="General Approach"></a>General Approach</h3><p>我们可以找到一个函数的局部最大值。相对于局部最大值，区域大小应该不随尺度改变。这意味着区域大小和图像尺寸应该共同变化。一个好的函数应该有有且仅有一个明显的局部最大值。换句话说，我们应该用在强度上有鲜明对比的函数。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/7.2.png"></p>
<p>我们将函数定义为：$f&#x3D;kernel*image$，可以用Laplacian（拉普拉斯算子）或者Difference of Gaussians（DoG，差分高斯算子）作为核<br>$$<br>L&#x3D;\sigma^{2}(G_{xx}(x,y,\sigma)+G_{yy}(x,y,\sigma))\<br>DoG&#x3D;G(x,y,k\sigma)-G(x,y,\sigma)\<br>where\space G(x,y,\sigma)&#x3D;\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x^{2}+y^{2}}{2\sigma^{2}}}<br>$$<br>这些核都具有旋转与伸缩不变性。</p>
<p>附加：拉普拉斯算子的推导</p>
<p>对于图像$f$，首先进行高斯平滑处理滤去噪点：<br>$$<br>G(x,y,\sigma)<em>f(x,y)<br>$$<br>对上式求微分，进行边缘检测：<br>$$<br>\frac{d}{dx}(G</em>f)&#x3D;\frac{dG}{dx}*f<br>$$<br>其中，$dG&#x2F;dx$即为拉普拉斯算子。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/7.3.png"></p>
<p>基于 DoG 算子，可以采用 SIFT 算法进行特征匹配，在空间和尺度上找到DOG局部最大值。基于拉普拉斯算子，可以采用 Harris-Laplacian 算法进行特征匹配，在空间上找到Harris角点检测器的局部最大值、在尺度上找到拉普拉斯的局部最大值。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/7.5.png"></p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>CS131</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131 Lecture 8 Feature Descriptors and Resizing</title>
    <url>/2018/08/07/Lecture%208%20Feature%20Descriptors%20and%20Resizing/</url>
    <content><![CDATA[<h1 id="Lecture-8-Feature-Descriptors-and-Resizing"><a href="#Lecture-8-Feature-Descriptors-and-Resizing" class="headerlink" title="Lecture 8 Feature Descriptors and Resizing"></a>Lecture 8 Feature Descriptors and Resizing</h1><h2 id="Scale-invariant-keypoint-detection"><a href="#Scale-invariant-keypoint-detection" class="headerlink" title="Scale invariant keypoint detection"></a>Scale invariant keypoint detection</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>给定有较大尺度差别的两张相同情景下的图片，目标是独立的检测每张图中相同关键点。解决方案是寻找再空间和尺度上合适函数的极大值。</p>
<h3 id="General-methodology"><a href="#General-methodology" class="headerlink" title="General methodology"></a>General methodology</h3><p>通过合理放缩窗口，我们可以捕捉到相同的内容。如下图，右侧的窗要放大至左侧窗才能获得相同信息。  </p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/8.1.png"></p>
<p>从尺度不变角度上，我们需要找到一个描述什么是“捕捉相同内容”的方法。这里我们考虑一个以区域内容作为输入，对该区域的所有尺度范围相同值作为输出的函数$f(window)$</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/8.2.png"></p>
<p>在每两张图中，我们可以独立的检测区域极值作为关键点。对应区域极值的窗大小（上图$s_1$和$s_2$）反映了两张图片之间的尺度不同。</p>
<h4 id="Average-intensity"><a href="#Average-intensity" class="headerlink" title="Average intensity"></a>Average intensity</h4><p>第一种方法，用窗内像素的平均强度作为$f(window)$，因为平均强度不随尺度变化而变化。但是，平均强度不能很好的捕捉窗内的对比度改变。要捕捉对比度，我们需要加入导数。</p>
<h4 id="Difference-of-Gaussians-DoG"><a href="#Difference-of-Gaussians-DoG" class="headerlink" title="Difference of Gaussians (DoG)"></a>Difference of Gaussians (DoG)</h4><p>第二种方法，用高斯差分。</p>
<p>考虑一个图片$I$。首先，我们将$I$与有不同$\sigma$的高斯滤波器卷积。接下来，我们用尺度收缩的$I$重复第一步的卷积工作。至此我们得到了有不同的$\sigma$和图片尺度的高斯金字塔（如图三左），接着将临近的高斯卷积图像相减得到高斯差分(Difference of Gaussians)，如图三右：<br>$$<br>DOG(\sigma)&#x3D;(G(k\sigma)-G(\sigma))*I<br>$$<br><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/8.3.png"></p>
<p>直观来讲，高斯差分捕捉了$I$在不同尺度下的细节，也即保留了$\sigma_1$和$\sigma_2$之间不同的细节。$\sigma$越大，高斯差分捕捉到的细节越粗糙。如下图</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/8.6.png"></p>
<p>有了x-y尺度空间上的高斯差分金字塔， 现在我们可以在3D空间上检测局部极值以检测关键点和相关尺度。为了检测局部极值，我们将一个坐标和它的26个邻居（3D空间上，见下图），若该坐标大于或者小于26个邻居，则为局部极值。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/8.4.png"></p>
<h4 id="Harris-Laplacian"><a href="#Harris-Laplacian" class="headerlink" title="Harris-Laplacian"></a>Harris-Laplacian</h4><p>第三种方法用哈里斯-拉普拉斯算子方法。相比于高斯差分，这种方法更高效，但是计算量也更大。</p>
<p>首先，对不同尺度的$I$运行哈里斯检测器，局部化每个尺度上的关键点。接下来，选择在所有尺度中可以最大化拉普拉斯算子的关键点。</p>
<p><strong>转变为尺度不变检测器：</strong>现在我们有多种方法检测不同尺度上的固定关键点，我们可以继续研究在一个尺度不变方式上描述这些关键点的方法，以便匹配关键点。</p>
<h2 id="SIFT-an-image-region-descriptor"><a href="#SIFT-an-image-region-descriptor" class="headerlink" title="SIFT: an image region descriptor"></a>SIFT: an image region descriptor</h2><p>尺度不变特征转换 (Scale-invariant feature transform, SIFT)  </p>
<h3 id="Invariant-Local-Features"><a href="#Invariant-Local-Features" class="headerlink" title="Invariant Local Features"></a>Invariant Local Features</h3><p> 点描述子需要有不变性和特殊性。我们将图片内容转换为局部特征坐标，这些坐标具有移动、旋转、放缩以及其他成像因素不变性，以达到点描述子的鲁棒性。</p>
<p>局部不变特征有以下优点：</p>
<ul>
<li>局部性：特征描述部分，且对混乱和遮挡有鲁棒性。</li>
<li>特殊性：特征可以从一个大型的物体数据库中识别出来。</li>
<li>数量：即使物体很小，也能得到许多特征。</li>
<li>效率：接近实时性能</li>
<li>延展性：可以轻松地拓展至大量不同的特征类型，且每个拓展都提高了对改变的鲁棒性。</li>
</ul>
<h4 id="Scale-invariance"><a href="#Scale-invariance" class="headerlink" title="Scale invariance"></a>Scale invariance</h4><p>要求有一个可以重复的在空间和尺度上选择点的方法：</p>
<ul>
<li>唯一合理的尺度空间核是高斯核(Koenderink, 1984; Lindeberg, 1994)</li>
<li>一个有效的方法是检测高斯差分金字塔的高峰(Burt &amp; Adelson,1983; Crowley &amp; Parker, 1984 - but examining more scales)</li>
<li>有固定比例尺度缩小的高斯差分近似于Lindeberg的尺度归一化拉普拉斯算子（从热扩散方程中得到）</li>
</ul>
<h4 id="Rotation-invariance"><a href="#Rotation-invariance" class="headerlink" title="Rotation invariance"></a>Rotation invariance</h4><p>给定一个关键点以及它在高斯差分(DoG)中的尺度，我们需要选出最具有代表性的特征，并描述所有与该方向相关的特征：</p>
<ol>
<li>用与关键点尺度相关的光滑（模糊）过的图像</li>
<li>在关键点邻居上得到图像梯度</li>
<li>通过负关键点方向，旋转梯度方向和位置。换句话说，描述与方向相关的所有特征。</li>
</ol>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/8.7.png"></p>
<h4 id="SIFT-descriptor-formation"><a href="#SIFT-descriptor-formation" class="headerlink" title="SIFT descriptor formation"></a>SIFT descriptor formation</h4><p>用精确的梯度位置是很脆弱的，所以我们需要一个一般化的相似描述子。我们将创造一个方向直方图矩阵，并将梯度放入8个方向柱（”米”字形）的局部方向直方图中</p>
<p>具体：</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/8.8.png"></p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/8.9.png"></p>
<ol>
<li><p>创建方向直方图矩阵(4x4)</p>
</li>
<li><p>将旋转过的梯度放入局部方向直方图。</p>
<ul>
<li>一个梯度根据距离给附近的直方图贡献值。例如，如果它在两个直方图位置的中间，它给两个直方图一半的贡献。</li>
<li>远离中心的梯度，贡献值将被缩小。</li>
<li>SIFT作者发现8个方向盒子每直方图和一个4x4直方图矩阵将得到最好的结果（见下图）</li>
</ul>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/8.5.png"></p>
</li>
<li><p>对比两张图的每个向量，找到匹配的关键点。一共有8x4x4&#x3D;128个向量（8方向，4x4直方图矩阵）</p>
</li>
<li><p>为了在高对比度的照片中加入对光照改变的鲁棒性，需要在对比前归一化向量。不可靠的3D光照效果（如炫光）会导致图像有很大的梯度，为了减少大梯度带来的影响，我们将向量中的值压缩至0.2以下，然后再次归一化。</p>
</li>
<li><p>个人总结：将图像分割为4x4，每个格子产生一个局部直方图，得到8个方向各自的向量。</p>
</li>
</ol>
<h2 id="HoG-Another-image-region-descriptor"><a href="#HoG-Another-image-region-descriptor" class="headerlink" title="HoG: Another image region descriptor"></a>HoG: Another image region descriptor</h2><h3 id="Histogram-of-Oriented-Gradient-HoG"><a href="#Histogram-of-Oriented-Gradient-HoG" class="headerlink" title="Histogram of Oriented Gradient (HoG)"></a>Histogram of Oriented Gradient (HoG)</h3><p>HoG描述子在图片中找到一个可以被辨别、较为突出的物体。</p>
<p>过程如下：</p>
<ol>
<li><p>将图像窗分割为小空间区域或单元格(cell)，cell可以是圆形或者矩形的。</p>
</li>
<li><p>对每个单元格，积累一个局部直方图。直方图柱（横坐标）是关于梯度方向的均匀间隔，把单元格内每个像素的梯度方向累积至直方图柱。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/8.10.png"></p>
</li>
<li><p>在一个更大的区域上将直方图归一化，称之为由一些单元格组成的一个块(block)。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/8.12.png"></p>
</li>
<li><p>为了更好的消除光照和阴影的遮挡，可以在使用局部响应之前，对局部相应进行对比标准化。并将标准化整个块内的所有单元格。</p>
</li>
</ol>
<p>HoG有一些缺点：</p>
<ol>
<li>在检测时有大的变化和范围</li>
<li>很慢</li>
<li>当背景有不同光照时，不是很有条理。</li>
</ol>
<p>除去这些缺点，HoG是很有效的。下图是使用HoG后的效果。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/8.11.png"></p>
<h3 id="Difference-between-HoG-and-SIFT"><a href="#Difference-between-HoG-and-SIFT" class="headerlink" title="Difference between HoG and SIFT"></a>Difference between HoG and SIFT</h3><ul>
<li>HoG用于在整个图像上找到梯度，而SIFT用于关键点匹配。</li>
<li>SIFT直方图方向朝向自然正梯度方向，而HoG不是。</li>
<li>HoG梯度用邻柱标准化</li>
<li>SIFT用不同的尺度计算多个描述子</li>
</ul>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>CS131</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131 Lecture 6 Edge Detection</title>
    <url>/2018/07/11/Lecture%206%20Edge%20Detection/</url>
    <content><![CDATA[<h1 id="Lecture-6-Edge-Detection"><a href="#Lecture-6-Edge-Detection" class="headerlink" title="Lecture 6 Edge Detection"></a>Lecture 6 Edge Detection</h1><h2 id="Edge-Detection"><a href="#Edge-Detection" class="headerlink" title="Edge Detection"></a>Edge Detection</h2><h3 id="Edge-Basic"><a href="#Edge-Basic" class="headerlink" title="Edge Basic"></a>Edge Basic</h3><p>图片上的边缘主要有四个可能的来源：不连续的表面（表面角度突然改变）、深度不连续（一个表面重叠在另一个上）、表面颜色不连续、光照不连续（明&#x2F;暗）。从数学上来看，梯度大的位置为边缘。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/6.1.png"></p>
<h3 id="Sobel-Noise-Detector"><a href="#Sobel-Noise-Detector" class="headerlink" title="Sobel Noise Detector"></a>Sobel Noise Detector</h3><p>这个算法利用2个$3\times 3$核，与图像卷积，结构近似于原始图像的x和y导数。<br>$$<br>G_{x}&#x3D; \left[<br> \begin{matrix}<br>  1&amp;0&amp;-1\<br>   2&amp;0&amp;-2\<br>   1&amp;0&amp;-1<br>  \end{matrix}<br>  \right] ,<br>  G_{y}&#x3D; \left[<br> \begin{matrix}<br>  1&amp;2&amp;1\<br>   0&amp;0&amp;0\<br>   -1&amp;-2&amp;-1<br>  \end{matrix}<br>  \right]<br>$$<br>这些矩阵代表了光滑化和微分的结果<br>$$<br>G_{x}&#x3D; \left[<br> \begin{matrix}<br>  1&amp;0&amp;-1\<br>   2&amp;0&amp;-2\<br>   1&amp;0&amp;-1<br>  \end{matrix}<br>  \right]&#x3D;\left[<br> \begin{matrix}<br>  1\<br>   2\<br>   1<br>  \end{matrix}<br>  \right]<br>  \left[<br> \begin{matrix}<br>  1&amp;0&amp;-1\<br>  \end{matrix}<br>  \right]<br>$$<br>但是Sobel Filter有许多问题，包括定位差。并且Sobel Filter比起倾斜边缘，更偏向于水平和垂直边缘。</p>
<h3 id="Canny-Edge-Detector"><a href="#Canny-Edge-Detector" class="headerlink" title="Canny Edge Detector"></a>Canny Edge Detector</h3><p>Canny Edge Detector算法有五步：</p>
<ul>
<li><p>抑制噪音</p>
<p>我们可以用一个类似于Sobel Filter的方法，同时减少噪音并计算x、y方向上的导数。</p>
</li>
<li><p>计算梯度大小与方向<br>$$<br>|\nabla f(x,y)|&#x3D;\sqrt{f_{x}^{2}+f_{y}^{2}}\<br>\theta&#x3D;tan^{-1}(f_{y}&#x2F;f_{x})<br>$$</p>
</li>
<li><p>应用non-maximum消除</p>
<p>我们假设只有梯度最大时出现边缘，所以消除掉所有不含最大梯度值的像素。基本上，如果一个像素不是正方向上三个像素和反方向上三个像素中梯度值最大的一个，那么就将这个像素设为0。并且，所有的梯度值都要四舍五入到接近45°</p>
</li>
<li><p>滞后阈值法(Hysteresis thresholding)</p>
<p>所有保留的像素都服从于滞后阈值法。这部分用两个值作为高低阈值。像素值高于高阈值的为强边缘，低于低阈值的设为0，在两个阈值之间的为弱边缘。</p>
</li>
<li><p>通过连通性(Connectivity)分析检测边缘</p>
<p>所有强边缘像素都是边缘。但对于弱边缘像素，只有和强边缘像素相连的弱边缘像素才是边缘。这一部分使用BFS或者DFS来找到所有边缘。</p>
</li>
</ul>
<h2 id="Hough-Transform-霍夫变换"><a href="#Hough-Transform-霍夫变换" class="headerlink" title="Hough Transform(霍夫变换)"></a>Hough Transform(霍夫变换)</h2><h3 id="Intro-to-Hough-Transform"><a href="#Intro-to-Hough-Transform" class="headerlink" title="Intro to Hough Transform"></a>Intro to Hough Transform</h3><p>霍夫变换是检测图像特别结构，也就是线条，的一个方法。无论如何，霍夫变换可以检测任何参数方程已知的结构。它在噪音和部分遮挡下提供了一个强健的检测器。</p>
<h3 id="Goal-of-Hough-Transform-for-detecting-lines"><a href="#Goal-of-Hough-Transform-for-detecting-lines" class="headerlink" title="Goal of Hough Transform for detecting lines"></a>Goal of Hough Transform for detecting lines</h3><p>要使用霍夫变换，首先要确定图像中构成直线的像素集。这项工作要在边缘检测器检测出边缘后进行，这样才能更好的找出组成直线的像素集。</p>
<h3 id="Detecting-lines-using-Hough-Transform-in-a-b-space"><a href="#Detecting-lines-using-Hough-Transform-in-a-b-space" class="headerlink" title="Detecting lines using Hough Transform in a,b space"></a>Detecting lines using Hough Transform in a,b space</h3><p>假设一条穿过像素$x_{i},y_{i}$的线为：<br>$$<br>y_{i}&#x3D;a<em>x_{i}+b<br>$$<br>那么，可以得出将像素转换到$a,b$空间的公式：<br>$$<br>b&#x3D;-a</em>x_{i}+y_{i}<br>$$<br>这个公式代表了一条在$a,b$空间上的线，并且每个在线上的点$a,b$代表了一条穿过点$x_{i},y_{i}$的线。</p>
<p>因此，对于边缘像素集上的每一个像素，我们都将它转换到$a,b$空间上得到一条线。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/6.2.png"></p>
<p>$a,b$空间上两条线交点处的ab值代表了$y_{i}&#x3D;a<em>x_{i}+b$这条线同时穿过几个点。例如：xy空间上有$x_{1},y_{1}&#x3D;(1,1)$和$x_{2},y_{2}&#x3D;(2,3)$，转换得到ab空间上两条线$b&#x3D;-a</em>1+1$和$b&#x3D;-a*2+3$，联立方程解得$a&#x3D;2,b&#x3D;-1$。这种ab空间上的交叉点告诉了我们在xy空间上同时经过两点的线。</p>
<h3 id="Accumulator-Cells"><a href="#Accumulator-Cells" class="headerlink" title="Accumulator Cells"></a>Accumulator Cells</h3><p>为了得到最佳的线，我们将ab空间量化为细胞。对于ab空间中的每一条线，我们都会在这条线经过的每个细胞上添加一个计数。最后，有最多计数的细胞代表ab空间上有最多的交点，即为在xy空间上穿过最多点的线。所以可以代表图像真正的线。</p>
<p>霍夫变换的具体算法为：</p>
<ul>
<li>通过将参数空间(a,b)划分成细胞来进行量化。</li>
<li>量化后的空间常被称为累加器细胞</li>
<li>计算一条线在特定细胞交叉的次数<ul>
<li>针对每一对被检测为边缘的点$(x_{1},y_{1}),(x_{2},y_{2})$，找到其在$(a,b)$空间上的交叉点(a’,b’)</li>
<li>在$(a’,b’)$所属细胞（范围$[[a_{min},a_{max}],[b_{min},b_{max}]]$）上增加值</li>
<li>值超过某个特定界限的细胞可认为是(x,y)空间上的一条线</li>
</ul>
</li>
</ul>
<h3 id="Hough-transform-in-rho-theta-space"><a href="#Hough-transform-in-rho-theta-space" class="headerlink" title="Hough transform in $\rho ,\theta$ space"></a>Hough transform in $\rho ,\theta$ space</h3><p>用ab空间表示线的问题在于它们收到限制而且不能表示垂线。所以，我们考虑用极坐标(polar coordinates)来表示线。<br>$$<br>x<em>cos\theta +y</em>sin\theta &#x3D;\rho<br>$$<br>在极坐标中，线被表示为类正弦波函数。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/6.3.png"></p>
<p>同ab空间，在极坐标中交叉点仍然表示同时穿过几个点的直线。所以，我们同样可以使用上面提到的累加器算法。</p>
<h3 id="Concluding-Remarks"><a href="#Concluding-Remarks" class="headerlink" title="Concluding Remarks"></a>Concluding Remarks</h3><p>霍夫变换有以下优点：</p>
<ul>
<li>概念简单，易于实践</li>
<li>可以很好的处理缺失、遮挡的数据</li>
<li>可以找到有参数方程的结构，不止是线</li>
</ul>
<p>和缺点：</p>
<ul>
<li>参数越多，计算复杂度越高</li>
<li>只能找一种结构（例如，不能同时找线和圆）</li>
<li>不能检测线段的长度和位置</li>
<li>在同一条线上的线段不能分割</li>
</ul>
<h2 id="RANSAC"><a href="#RANSAC" class="headerlink" title="RANSAC"></a>RANSAC</h2><p>随着模型复杂度的提高（例，参数量的提高），霍夫变化失去了效率。这个章节将阐述RAndom SAmple Consensus (RANSAC，随机样本一致性)技术的设计，它提供了一种计算效率高的拟合图像模型的方法。</p>
<h3 id="Introduction-RANSAC-Basics"><a href="#Introduction-RANSAC-Basics" class="headerlink" title="Introduction RANSAC Basics"></a>Introduction RANSAC Basics</h3><p>RANSAC用于估算图像模型的参数。RANSAC背后的基本想法是：用随机选择的数据最小子集来解决拟合问题，并选出最佳拟合。为了实现这个想法，RANSAC尝试反复地识别符合我们想拟合的模型的数据点。</p>
<p>下图a)描述了一个被拟合进数据的线性模型，虽然大部分点可以和线性模型，但是两个偏离的绿点可以极大影响整体拟合精度。RANSAC算法将通过识别数据正常值(inliers)和异常值(outliers)来解决这个问题。</p>
<p>RANSAC随机在数据中选择样本（样本不是点，而是包含许多点的集合），基于一个假设：只要选择足够多的样本，拟合效果差的可能性较低。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/6.4.png"></p>
<h3 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h3><p>RANSAC算法可以估算不同模型的参数。这已经被证明在图像拼接（上图b)）、异常值检测、车道识别（线性模型估计）、立体相机计算方面有效。</p>
<h3 id="The-Algorithm"><a href="#The-Algorithm" class="headerlink" title="The Algorithm"></a>The Algorithm</h3><p>RANSAC算法迭代的采样原始数据的标称子集（例如，线性估计所用的两个点）。模型对每个样本拟合，且计算对应这种拟合的正常值数量，这包括接近于拟合模型的数据点。在两个极限值内的点（例如，两个标准偏差，或者一个先前决定的像素数量）被视作正常值。如果数据的大部分是正常值，那么拟合模型效果较好。在拟合效果好的情况下，模型会使用所有正常值进行重新拟合，而异常值将被丢弃。反复执行以上过程，找出有足够多的正常值的模型，再对比选择出最佳模型。</p>
<p>下图展示了这个过程，图三为最佳拟合。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/6.5.png"></p>
<p>以下是RANSAC的伪代码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Determine:</span><br><span class="line">	n -- the smallest number of points required</span><br><span class="line">	k -- the number of iterations required</span><br><span class="line">	t -- the threshold used to  identify a point that fits well</span><br><span class="line">	d -- the number of nearby points required to assert a model fits well</span><br><span class="line">Until there is a good fit or k iterations have occurred</span><br><span class="line">	draw a sample of n points from the data </span><br><span class="line">	uniformly and at random</span><br><span class="line">	</span><br><span class="line">	fit to that set of n points</span><br><span class="line">	</span><br><span class="line">	for each data points outside the sample</span><br><span class="line">		</span><br><span class="line">		test the distance from the point to the line against t; </span><br><span class="line">		if the distance from the point to the line</span><br><span class="line">		is less than t,the point is close</span><br><span class="line">		</span><br><span class="line">	end</span><br><span class="line">	if there are d or more points close to the line</span><br><span class="line">	then there is a good fit. Refit the line using all</span><br><span class="line">	these points, and terminate</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<p>RANSAC主要循环的步骤如下：</p>
<ol>
<li>随机从数组中选择一组种子</li>
<li>用所选的种子进行参数估计</li>
<li>确定正常值（靠近估计模型的点数量）</li>
<li>（如果存在足够多的正常值，）用这些正常值重新估计模型</li>
<li>重复1-4，最终保证模型有最多的正常值和最佳的拟合。</li>
</ol>
<h3 id="How-Many-Samples-are-Needed"><a href="#How-Many-Samples-are-Needed" class="headerlink" title="How Many Samples are Needed?"></a>How Many Samples are Needed?</h3><p>RANSAC是一种非确定性模型拟合方法，意味着需要足够多的样本来对参数进行可靠的估计。需要的样本数取决于：</p>
<ul>
<li>需要拟合的参数的数量</li>
<li>噪音的数量</li>
</ul>
<p>下图列出了基于$p&#x3D;0.99$和样本大小变化（例如，参数的数量）和异常值的分数（例如，噪音）的所需最小样本数。噪音越多、模型越大，所需的样本数也就越多。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/6.6.png"></p>
<p>需要选择足够多的样本($k$)，这样所有样本都失败的概率($P_{f}$)较低：<br>$$<br>P_{f}&#x3D;(1-W^{n})^{k}&#x3D;1-p<br>$$<br>其中，$W、n$分别表示样本拟合正确的概率和每个样本所含点的数量。所以，最小样本数为：<br>$$<br>k&#x3D;\frac{log(1-p)}{log(1-W^{n})}<br>$$</p>
<h3 id="Advantages-Limitations-and-Considerations"><a href="#Advantages-Limitations-and-Considerations" class="headerlink" title="Advantages, Limitations, and Considerations"></a>Advantages, Limitations, and Considerations</h3><p>优势：</p>
<ul>
<li>实现简单</li>
<li>在模型拟合领域适用范围广</li>
<li>计算效率高</li>
<li>抽样方法为所有可能的特征组合提供了更好的解决问题的方法。</li>
</ul>
<p>在一些情况下，使用霍夫转换更好：</p>
<ul>
<li>特征数小。例如，线性模型估计（2个特征）用霍夫转换，但图像拼接需要像RANSAC一样计算效率更高的方法</li>
<li>噪音数高；如上文所提及，更多的噪音需要更广泛的抽样方法（即更多的样本数），提高了计算成本。更高的噪音也减少了正确参数估计的机会和异常值分类的精度。</li>
</ul>
<p>值得注意的是，RANSAC最大的限制是在高噪下表现较差，而现实世界的问题总是有着较高的异常值。</p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>CS131</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131 Lecture 9 Image Resizing and Segmentation</title>
    <url>/2018/08/09/Lecture%209%20Image%20Resizing%20and%20Segmentation/</url>
    <content><![CDATA[<h1 id="Lecture-9-Image-Resizing-and-Segmentation"><a href="#Lecture-9-Image-Resizing-and-Segmentation" class="headerlink" title="Lecture 9 Image Resizing and Segmentation"></a>Lecture 9 Image Resizing and Segmentation</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>为了适应不同尺寸和形状的播放器，图片或影片需要调整大小，本章介绍如何在调整大小的同时保存重要内容并减少瑕疵。</p>
<h3 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h3><p>输入大小为$n\times m$的原图，输出大小为$n’\times m’​$，能很好代表原图的新图像。我们希望：</p>
<ol>
<li>新图要能符合设备几何限制</li>
<li>新图要展现重要内容和结构</li>
<li>新图要减少瑕疵</li>
</ol>
<h3 id="Importance-Measures"><a href="#Importance-Measures" class="headerlink" title="Importance Measures"></a>Importance Measures</h3><ol>
<li><p>用函数$S:p\rightarrow [0,1]$来确定图像中的重要部分，再用一些操作来改变图像。如下图</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/9.1.png"></p>
</li>
<li><p>除了函数方法，还有注意力模型之类更精巧的方法。</p>
</li>
</ol>
<h2 id="Seam-Carving"><a href="#Seam-Carving" class="headerlink" title="Seam Carving"></a>Seam Carving</h2><h3 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h3><p>人类视觉对边缘更加敏感。因此一个简单而有效的方法是使用基于梯度的能量方程，将光滑区域的内容移除并保存信息量更大边缘。能量公式定义为<br>$$<br>E(I)&#x3D;|\frac{\partial}{\partial x}I|+|\frac{\partial}{\partial y}I|<br>$$<br>不重要的内容即为能量公式值较小的像素。</p>
<h3 id="Pixel-Removal"><a href="#Pixel-Removal" class="headerlink" title="Pixel Removal"></a>Pixel Removal</h3><p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/9.10.png"></p>
<p>下图从左至右用了三种不同的像素移除方案：</p>
<ol>
<li>移除所有低能量像素</li>
<li>移除每行最低能量像素</li>
<li>移除每列最低能量像素</li>
</ol>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/9.2.png"></p>
<p>可以看出第一、二种方法破坏了原图，而第三种方法较前两种好，但仍有大量瑕疵。</p>
<h3 id="A-Seam"><a href="#A-Seam" class="headerlink" title="A Seam"></a>A Seam</h3><ol>
<li><p>缝被定义为从头到尾（或从左到右）连接像素的通道。对于从头到尾的像素，我们需要从每列钟选取一个像素，数学定义为：</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/9.3.png"></p>
</li>
<li><p>最好的缝是基于像素梯度，能够最小化能量函数的缝。<br>$$<br>s^*&#x3D;argmin_sE(s),\space where\space E(I)&#x3D;|\frac{\partial}{\partial x}I|+|\frac{\partial}{\partial y}I|<br>$$<br><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/9.4.png"></p>
</li>
<li><p>可以用递归关系找到最佳缝。如果$M(i,j)$定义为经过像素$(i,j)$的缝的最小能量代价，那么其递归关系为：<br>$$<br>M(i,j)&#x3D;E(i,j)+min(M(i-1,j-1),M(i-1,j),M(i-1,j+1))<br>$$<br>该递归关系可以通过$O(snm)$的动态程序解决，再原始算法钟$s&#x3D;3$。下面为一个图像的能量函数值</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/9.5.png"></p>
<p>递归关系为</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/9.6.png"></p>
</li>
<li><p>为了找到最佳缝，我们引入回溯法。从底层能量函数值最低的像素开始，往上发展。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/9.7.png"></p>
</li>
</ol>
<h3 id="Seam-Carving-Algorithms"><a href="#Seam-Carving-Algorithms" class="headerlink" title="Seam Carving Algorithms"></a>Seam Carving Algorithms</h3><p>算法耗费$O((n-n’)mn)$，每个循环更新$E,s,im$耗费$O(mn)$。如果要垂直调整大小，可以旋转图片用同一算法。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/9.8.png"></p>
<p>图像的平均能量会随着缝剪裁算法移除低能量像素而提高。当调整大小时，我们需要移除横向和纵向的缝。如何动态同时删除横竖的缝？参考SIGGRAPH论文，利用再循环方程<br>$$<br>T(r,c)&#x3D;min(T(r-1,c)+E(s^x(I_{n-r-1\times m-c})),T(r,c-1)+E(s^y(I_{n-r\times m-c-1})))\<br>\min_{s^x,s^y,\alpha}\sum_{i&#x3D;1}^kE(\alpha_is_i^x+(1-\alpha_i)s_i^y)<br>$$<br> 同时剪裁横竖缝，得到更多信息。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/9.9.png"></p>
<h2 id="Advanced-Seam-Carving"><a href="#Advanced-Seam-Carving" class="headerlink" title="Advanced Seam Carving"></a>Advanced Seam Carving</h2><h3 id="Image-Expansion"><a href="#Image-Expansion" class="headerlink" title="Image Expansion"></a>Image Expansion</h3><p>用相似的方法，我们可以提高图片大小。最原始的方法时迭代地找到能量最低的缝，并复制它们来拓展图片。但是，这种方法会形成下图：</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/9.11.png"></p>
<p>注意图像右侧看起来很不自然，这是由于每次算法找到的都是同一条缝，导致同一条缝被复制多次。更有效的方法是一次性找到能量最低的$k$条缝，并复制它们，如下图：</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/9.12.png"></p>
<p>这样形成的图片更加自然。注意这种方法只能以2x来放大图像（没有足够多的缝进行放大）。</p>
<h3 id="Multi-Size-Image-Representation"><a href="#Multi-Size-Image-Representation" class="headerlink" title="Multi-Size Image Representation"></a>Multi-Size Image Representation</h3><p>实际中，许多图片会储存在它们缝的表示的旁边，使得图片更容易调整大小。这些缝表现和图片规模大小一致，但是它们不储存像素强度，而是由多条能量从低到高、路径排序的缝组成。注意为了一次性计算多条缝，图像的能量在模拟每条缝的移除后必须重新计算。下图是缝表示的一个例子：</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/9.13.png"></p>
<p>有了右图的表示，可以轻松的移除$k$条缝，只要移除右图对应标签为$1$到$k$的像素。</p>
<h3 id="Object-Removal"><a href="#Object-Removal" class="headerlink" title="Object Removal"></a>Object Removal</h3><p>通过让用户指定图中物体能量的高低，我们可以保护或移除特定的物体。如下图，红色为保护，绿色为移除物体。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/9.14.png"></p>
<h3 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h3><ol>
<li><p>当图片大小放生巨大变化时，效率的上下限。</p>
</li>
<li><p>图中物体的重要特征可能是低能量的，导致缝误移除。如下图</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/9.15.png"></p>
<p>我们可以看到一些光滑平整的区域虽然梯度小能量低，但对图像十分重要，却会被算法误移除。为了解决这个问题，我们需要通过考虑更多的因素来改变能量。例如：用面部探测或者用户约束，如下图。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/9.16.png"></p>
</li>
</ol>
<h3 id="Forward-Energy"><a href="#Forward-Energy" class="headerlink" title="Forward Energy"></a>Forward Energy</h3><p>如上文提及，缝剪裁会导致图片平均能量上升，产生锯齿状边缘。改进的方法是，不考虑移除最低能量的缝，而是移除插入最小能量到图像的缝。移除一个像素$P(i,j)$会有三种新的边缘情况，这些新边缘花费为<br>$$<br>C_L(i,j)&#x3D;|I(i,j+1)-I(i,j-1)|+|I(i-1,j)-I(i,j-1)|\<br>C_R(i,j)&#x3D;|I(i,j+1)-I(i,j-1)|+|I(i-1,j)-I(i,j+1)|\<br>C_V(i,j)&#x3D;|I(i,j+1)-I(i,j-1)|<br>$$<br><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/9.17.png"></p>
<p>这种方式称为”forward energy”，之前的方法称为”backward energy”。forward energy通过最小化添加的能量，保留了光滑的边缘。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/9.18.png"></p>
<h3 id="Seam-Carving-in-Videos"><a href="#Seam-Carving-in-Videos" class="headerlink" title="Seam-Carving in Videos"></a>Seam-Carving in Videos</h3><p>比起图像，视频剪裁难度更大，有以下几个问题。</p>
<p>第一，时间问题。一个30fps的视频，一分钟有1800帧，利用缝剪裁每帧处理至少需要30小时。</p>
<p>第二，视频连接问题。视频的帧与帧之间是逻辑连续的，而人眼对移动又特别敏感，使得逐帧剪裁的视频产生失真现象。稍改进后的方法是，将视频当作一个3D空间，每个垂直面作为视频的一个图像。从而可以找到整个视频上能量最低的2D缝。剩余操作与2D剪裁相同。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/9.19.png"></p>
<h2 id="Segmentation"><a href="#Segmentation" class="headerlink" title="Segmentation"></a>Segmentation</h2><p>我们有多种原因想让计算机可以切割图像。我们可能需要将图像切割成和现实世界中一样，多个相邻的物体组成。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/9.20.png"></p>
<p>我们可能想将图像基于附近的像素是相似的切割成多组。我们将这些组称为超像素(superpixels)。超像素允许我们将一些独立像素视作一体，因此使得一些计算更快。如下图。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/9.21.png"></p>
<p>各种形式的图像分割帮助我们将一组像素视作一个特征，并从中得到图片信息。切割还能帮助图像效果，像背景移除。</p>
<p>为了解决如何切割图像问题，我们将切割当作聚类。聚类让我们将相似的数据点聚集并用一个值表示它们。我们需要解决两个问题：</p>
<ol>
<li>如何确定两个像素、批(patch)、图像是否相似？</li>
<li>如何从成对的相似处中计算整体的分类？</li>
</ol>
<p>通常有两大类聚类算法：自上而下、自下而上。自上而下将像素和patch分类到一起，因为它们依赖于同一视觉实体。自下而上算法将像素分类到一起，因为它们局部相邻。</p>
<p>我们还可以用人类识别物体的特定视觉规律来作为聚类算法。例如：相似物体、对称、共同命运、接近度等等。共同命运指一组物体似乎会被一起移除，所以他们有共同的命运，如下图。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/9.22.png"></p>
<p>接近度指将物体和它们接近的物体分在一组。如下图中的三个人。</p>
<p><img src="https://bearly.oss-cn-hangzhou.aliyuncs.com/CS131/9.23.png"></p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>CS131</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux 命令</title>
    <url>/2017/08/06/Linux%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<h2 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h2><p>concatenate的缩写，用于访问单个或者多个文件。</p>
<h2 id="cowsay-XXX"><a href="#cowsay-XXX" class="headerlink" title="cowsay XXX"></a>cowsay XXX</h2><p>画一头说XXX的牛</p>
<h2 id="man-命令"><a href="#man-命令" class="headerlink" title="man 命令"></a>man 命令</h2><p>查看命令使用手册，调用了less程序，使内容整页显示。</p>
<h3 id="less程序"><a href="#less程序" class="headerlink" title="less程序"></a>less程序</h3><p>U——向上一整页，D——向下一整页，&lt;——回到开头，&gt;——跳到结尾<br>输入行数+ENTER——跳转到指定行数，&#x2F;字符串——搜索指定字符串（区分大小写），其中n跳到下一条，N上一条</p>
<h2 id="history-x2F-win-R-x2F-上箭头"><a href="#history-x2F-win-R-x2F-上箭头" class="headerlink" title="history&#x2F;win+R&#x2F;上箭头"></a>history&#x2F;win+R&#x2F;上箭头</h2><p>查看所有历史&#x2F;搜索历史&#x2F;调用上一条命令</p>
<h2 id="unzip"><a href="#unzip" class="headerlink" title="unzip"></a>unzip</h2><p>解压文件</p>
<h2 id="ls"><a href="#ls" class="headerlink" title="ls"></a>ls</h2><p>查看当前目录下所有文件,默认不输出以.开头的文件。<br>PS：以.开头的文件通常是系统文件<br>-a 不忽略.开头文件<br>-l 文件类型（目录 or 文件）&#x2F;？&#x2F;字节大小&#x2F;创建时间&#x2F;文件名.后缀</p>
<h2 id="rm"><a href="#rm" class="headerlink" title="rm"></a>rm</h2><p>remove移除文件<br>-rf r——recursive递归，f——force强制。 以递归的方式强制将目录下所有文件删除。</p>
<h2 id="ctrl-c"><a href="#ctrl-c" class="headerlink" title="ctrl+c"></a>ctrl+c</h2><p>停止当前操作</p>
<h2 id="ctrl-D"><a href="#ctrl-D" class="headerlink" title="ctrl+D"></a>ctrl+D</h2><p>表示文件结尾，用于结束输入或者退出程序</p>
<h2 id="nano"><a href="#nano" class="headerlink" title="nano"></a>nano</h2><p>调用Ubuntu自带文本编辑器nano</p>
<h2 id="pwd"><a href="#pwd" class="headerlink" title="pwd"></a>pwd</h2><p>查看当前所在目录</p>
<h2 id="cd"><a href="#cd" class="headerlink" title="cd"></a>cd</h2><p>进入某目录或文件夹，cd ..——返回父目录    .表示当前目录。<br>单独的cd表示返回主目录</p>
<h2 id="echo"><a href="#echo" class="headerlink" title="echo"></a>echo</h2><p>打印信息</p>
<h2 id="mv-x2F-cp"><a href="#mv-x2F-cp" class="headerlink" title="mv&#x2F;cp"></a>mv&#x2F;cp</h2><p>移动文件&#x2F;复制文件</p>
<h2 id="mkdir-目录"><a href="#mkdir-目录" class="headerlink" title="mkdir 目录"></a>mkdir 目录</h2><p>创建新目录</p>
<h2 id="rmdir"><a href="#rmdir" class="headerlink" title="rmdir"></a>rmdir</h2><p>删除目录，但仅用于删除空目录。<br>要删除有内容的目录，需要用到 ** rm -r **命令进行递归删除。</p>
<h1 id="Linux文件系统"><a href="#Linux文件系统" class="headerlink" title="Linux文件系统"></a>Linux文件系统</h1><h2 id="文件名"><a href="#文件名" class="headerlink" title="文件名"></a>文件名</h2><p>可以使用除了\以外的各种符号，但是如果使用了符号，在调用文件的时候就要用’文件名’来调用。</p>
<h2 id="根（root）"><a href="#根（root）" class="headerlink" title="根（root）"></a>根（root）</h2><p>Linux系统不像Windows系统每个分盘有独立的根，Linux所有文件属于一个根。要参考文件只能给出完整路径，用&#x2F;分割目录。</p>
<h2 id="glob（globbing通配符模式）"><a href="#glob（globbing通配符模式）" class="headerlink" title="glob（globbing通配符模式）"></a>glob（globbing通配符模式）</h2><p><em>XX</em> ，其中<em>代表匹配任意字符，可以放在字符串的任意位置，也可使用多个</em>。<br>XXX{A,B} 匹配包含花括号中任一字符串的文件，例如XXXA和XXXB。<br>？匹配一个字符，？？匹配两个字符····<br>[]匹配包含其中字母，注意只能匹配包含其中单个字母的文件，多个字母无需隔开。<br>** 以上匹配均区分大小写，且可跟在各种命令后 **</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Matlab Week3</title>
    <url>/2017/10/17/Matlab-Week3/</url>
    <content><![CDATA[<h2 id="Matrices-and-Operators"><a href="#Matrices-and-Operators" class="headerlink" title="Matrices and Operators"></a>Matrices and Operators</h2><h3 id="The-colon-Operator"><a href="#The-colon-Operator" class="headerlink" title="The colon Operator"></a>The colon Operator</h3><p>a:b:c表示行向量a,a+b,…,a+nb(a+nb&lt;c)，若仅输入a:b则默认c&#x3D;1，若输入的a&gt;b则返回空矩阵</p>
<h3 id="Accessing-Parts-of-a-Matrix"><a href="#Accessing-Parts-of-a-Matrix" class="headerlink" title="Accessing Parts of a Matrix"></a>Accessing Parts of a Matrix</h3><ul>
<li>x(a,b)返回x矩阵中a行b列的元素</li>
<li>X(a,b)&#x3D;c，若X矩阵不存在或者(a,b)超出其行列数，则matlab会拓展出一个最小满足该要求的矩阵，除了(a,b)位置为c其他新增位置为0。X存在则c替换(a,b)原有数字</li>
<li>X([a,b],c)，返回第c列中的a、b行的元素，有各种变式</li>
<li>end是matlab中的一个关键字，不能作为变量名，矩阵索引中可以当作一个表示最后的数字</li>
</ul>
<h3 id="Combining-and-Transforming-Matrices"><a href="#Combining-and-Transforming-Matrices" class="headerlink" title="Combining and Transforming Matrices"></a>Combining and Transforming Matrices</h3><ul>
<li>可以将矩阵作为一个整体进行拼合，例如C&#x3D;[A,B]，注意行拼合行数相同，列拼合列数相同</li>
<li>‘表示矩阵转置</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Matlab</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习 Week10</title>
    <url>/2017/10/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-Week10/</url>
    <content><![CDATA[<h2 id="Gradient-Descent-with-Large-Datasets"><a href="#Gradient-Descent-with-Large-Datasets" class="headerlink" title="Gradient Descent with Large Datasets"></a>Gradient Descent with Large Datasets</h2><h3 id="Learning-With-Large-Datasets"><a href="#Learning-With-Large-Datasets" class="headerlink" title="Learning With Large Datasets"></a>Learning With Large Datasets</h3><p>先使用小的数据集（例如m&#x3D;1000），绘制学习曲线(Jtrain(θ)、Jcv(θ))，看m为小数值的时候是否出现高偏差(high variance)，出现则用大数据集</p>
<h3 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h3><p>随机梯度下降通常用于大数据算法，扫描所有算法，对每个样本分别进行梯度下降。应该提前随机打乱数据，可以让梯度下降更快。第二步的整个循环一般进行1-10次，通常m越大执行次数越小。同时，Jtrain必须执行batch gradient descent（批量梯度下降，为了学习α），而不一定需要执行Stochastic Gradient Descent</p>
<p><img src="https://i.imgur.com/bXec05e.png"></p>
<h3 id="Mini-Batch-Gradient-Descent"><a href="#Mini-Batch-Gradient-Descent" class="headerlink" title="Mini-Batch Gradient Descent"></a>Mini-Batch Gradient Descent</h3><p>取b&#x3D;mini-batch size, b介于1-m之间，步骤同随机梯度下降，只是循环内多出Σ项</p>
<h4 id="为什么选择mini-batch-而不是stochastic"><a href="#为什么选择mini-batch-而不是stochastic" class="headerlink" title="为什么选择mini-batch 而不是stochastic?"></a>为什么选择mini-batch 而不是stochastic?</h4><p>因为mini-batch可以用向量化的方法直接计算</p>
<h3 id="Stochastic-Gradient-Descent-Convergence"><a href="#Stochastic-Gradient-Descent-Convergence" class="headerlink" title="Stochastic Gradient Descent Convergence"></a>Stochastic Gradient Descent Convergence</h3><p>检查算法是否收敛：</p>
<p>在更新θ前，计算cost(θ,(x,y))，每一千次迭代就计算一次平均的cost并画出</p>
<p><img src="https://i.imgur.com/y672TLf.png"></p>
<ul>
<li>左上曲线代表已经收敛，其中红色曲线震荡更小，说明红色曲线的α值更小</li>
<li>右上曲线也已收敛，红色曲线代表增加平均样本数（每5000计算一次）</li>
<li>左下代表没有收敛，同样，红色曲线代表增加平均样本数（每5000计算一次）</li>
<li>右下代表算法发散，需要使用小α</li>
</ul>
<p>如果不改变α，算法最后会在全局最小值处震荡，如果需要接近全局最小值，则需要令α随着迭代次数减小。</p>
<h2 id="Advanced-Topics"><a href="#Advanced-Topics" class="headerlink" title="Advanced Topics"></a>Advanced Topics</h2><h3 id="Online-Learning"><a href="#Online-Learning" class="headerlink" title="Online Learning"></a>Online Learning</h3><p>例如：快递网站提供运费查询，如果用户查询后未寄件则记为负样本y&#x3D;0，若寄出，则记为y&#x3D;1.</p>
<p>算法：用逻辑回归或者神经网络计算P(y&#x3D;1)，每进来一个用户学习一次，不保存数据，学习后直接丢弃该数据。</p>
<p>优点：对大量用户的网站进行偏好优化</p>
<p>应用2：预估点击率CTR，推送给用户最有可能被点击的条目</p>
<h3 id="Map-Reduce-and-Data-Parallelism"><a href="#Map-Reduce-and-Data-Parallelism" class="headerlink" title="Map Reduce and Data Parallelism"></a>Map Reduce and Data Parallelism</h3><p>映射简约(Map Reduce)，可以让多台计算机来计算同一大规模算法。</p>
<p>过程：将数据平分为几个部分，分别交给不同计算机计算，最后汇总给中央处理器计算。</p>
<p>要求：算法可以拆分为函数的求和</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习 Week11</title>
    <url>/2017/10/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-Week11/</url>
    <content><![CDATA[<h2 id="Photo-OCR"><a href="#Photo-OCR" class="headerlink" title="Photo OCR"></a>Photo OCR</h2><h3 id="Problem-Description-and-Pipeline"><a href="#Problem-Description-and-Pipeline" class="headerlink" title="Problem Description and Pipeline"></a>Problem Description and Pipeline</h3><p>Photo OCR全称为photo optical character recognition(照片光学字符识别)，用于读取图片中的文字信息。</p>
<p>photo OCR步骤（又称机器学习流水线）：</p>
<ul>
<li>给定图片，检测有文字的位置</li>
<li>通过得到的矩形文字框，对里面的字母进行独立分割</li>
<li>识别字母，通过修正</li>
</ul>
<h3 id="Sliding-Windows"><a href="#Sliding-Windows" class="headerlink" title="Sliding Windows"></a>Sliding Windows</h3><p>滑动窗分类器用于文字检测(text detection)，确定一个长宽固定的矩形框，在一幅图片中每次一个步长(又称步幅参数stride parameter)移动矩形框遍历图片获得图像块数据，通常步长选择为4-8像素。</p>
<p>在图片识别中，将识别到有文字的图像块周边进行检测，如果周边也有被检测到的图像块，就扩展连接两个图像块。同时可以人工忽略检测后长宽不正常的图像块。</p>
<p>检测后，将识别到两个字母间的图像块设置为正样本y&#x3D;1，识别到完整字母的为负样本y&#x3D;0</p>
<h3 id="Getting-Lots-of-Data-and-Artificial-Data"><a href="#Getting-Lots-of-Data-and-Artificial-Data" class="headerlink" title="Getting Lots of Data and Artificial Data"></a>Getting Lots of Data and Artificial Data</h3><p>使用人工数据合成(artificial data synthesis)收集大量数据，通常有两种方法。</p>
<ol>
<li>在已有数据的基础上拓展其他数据。例如：将收集到含有字母的图像块，用不同的字体替换图中字母，就得到了新的图像块。</li>
<li>将数据变形（图像块扭曲，音频变调、加噪声等等）以获得更多数据</li>
<li>“Crowd source”众包，雇别人帮你收集、标记数据</li>
</ol>
<ul>
<li>需要将已有数据变形才能获得新数据，单纯的复制多个相同的数据是无用的</li>
</ul>
<h3 id="Ceiling-Analysis-What-Part-of-the-Pipeline-to-Work-on-Next"><a href="#Ceiling-Analysis-What-Part-of-the-Pipeline-to-Work-on-Next" class="headerlink" title="Ceiling Analysis: What Part of the Pipeline to Work on Next"></a>Ceiling Analysis: What Part of the Pipeline to Work on Next</h3><p>上限分析(Ceiling Analysis)可以帮助你找到最值得花时间去改进的模块。原理：人工将某一模块全部设置为正确y&#x3D;1，查看整个系统的准确率提高多少，比较所花时间后选择改进性价比最高的模块进行改进。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习 Week6</title>
    <url>/2017/09/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-Week6/</url>
    <content><![CDATA[<h2 id="如何评估一个学习算法"><a href="#如何评估一个学习算法" class="headerlink" title="如何评估一个学习算法"></a>如何评估一个学习算法</h2><h3 id="评估假设函数"><a href="#评估假设函数" class="headerlink" title="评估假设函数"></a>评估假设函数</h3><p>将训练集以7：3分为训练集和测试集，m表示训练集总数，m(test)表示测试集总数。（注意随机选择）</p>
<p>评估步骤：</p>
<ol>
<li><p>用70%的训练集算出θ</p>
</li>
<li><p>计算测试误差Jtest（θ）:a. 对于线性规划<br>$$<br>J_{test}(\Theta) &#x3D; \dfrac{1}{2m_{test}} \sum_{i&#x3D;1}^{m_{test}}(h_\Theta(x^{(i)}<em>{test}) - y^{(i)}</em>{test})^2<br>$$<br>b.对于分类问题</p>
</li>
</ol>
<p>$$<br>err(h_\Theta(x),y) &#x3D; \begin{matrix} 1 &amp; \mbox{if } h_\Theta(x) \geq 0.5\ and\ y &#x3D; 0\ or\ h_\Theta(x) &lt; 0.5\ and\ y &#x3D; 1\newline 0 &amp; \mbox otherwise \end{matrix}<br>$$</p>
<p>​          err函数结果为1&#x2F;0（&gt;&#x3D;0.5则取1，1表示结果正确），接下来，计算平均误差<br>$$<br>\text{Test Error} &#x3D; \dfrac{1}{m_{test}} \sum^{m_{test}}<em>{i&#x3D;1} err(h_\Theta(x^{(i)}</em>{test}), y^{(i)}_{test})<br>$$</p>
<h3 id="模型选择问题"><a href="#模型选择问题" class="headerlink" title="模型选择问题"></a>模型选择问题</h3><p>将数据集分为三部分：训练集 60% &#x2F; 交叉验证集（CV，cross validation set） 20% &#x2F; 测试集 20%</p>
<h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><ol>
<li>对每个多项式次数d通过训练集计算出Θ</li>
<li>用交叉验证集找出test error最小的Θ</li>
<li>通过Jtest（Θd ）评估误差</li>
</ol>
<h2 id="偏差-bias-与方差-variance"><a href="#偏差-bias-与方差-variance" class="headerlink" title="偏差(bias)与方差(variance)"></a>偏差(bias)与方差(variance)</h2><p>算法的表现不理想总与偏差（欠拟合）和方差（过拟合）有关。</p>
<p><img src="https://i.imgur.com/nUACohi.png"></p>
<h3 id="正则化与偏差-x2F-方差"><a href="#正则化与偏差-x2F-方差" class="headerlink" title="正则化与偏差&#x2F;方差"></a>正则化与偏差&#x2F;方差</h3><p>选择正确的λ的方法：</p>
<ol>
<li>选择一系列的λ（例如：0.01，0.02，0.04，0.08········10.24）</li>
<li>写出一些带有不同的多项式次数或者变体的模型</li>
<li>对于每个λ、每个模型都计算出不同的Θ</li>
<li>通过θ计算CV error（with λ），再用CV error计算Jcv(θ)（不带正则化部分）</li>
<li>选择错误最低的λ（在CV上）</li>
<li>用选出的λ与θ计算Jtest</li>
</ol>
<h3 id="学习曲线learning-curve"><a href="#学习曲线learning-curve" class="headerlink" title="学习曲线learning curve"></a>学习曲线learning curve</h3><p>画出Jtrain与Jcv关于m(测试集大小)的函数图像</p>
<ul>
<li>高方差：小λ，大多项式次数，过拟合。随着m增大，Jcv与Jtrain逐渐接近，但有仍有较大差距。</li>
<li>高偏差：小多项式次数，欠拟合。随着m增大，Jcv与Jtrain逐渐接近，最后趋向直线。</li>
</ul>
<h3 id="决定下一步"><a href="#决定下一步" class="headerlink" title="决定下一步"></a>决定下一步</h3><ul>
<li>取得更多样本 —-解决高方差</li>
<li>减少特征数—-解决高方差</li>
<li>增加特征数—-解决高偏差</li>
<li>增加多项式特征（x1²，x2²，x1x2·····）—-解决高偏差</li>
<li>增加λ —高偏差</li>
<li>减少λ —高方差</li>
</ul>
<h3 id="与神经网络联系"><a href="#与神经网络联系" class="headerlink" title="与神经网络联系"></a>与神经网络联系</h3><p>过少单元的神经网络容易造成欠拟合的高偏差，反之，过多单元的神经网络容易造成过拟合的高方差。</p>
<h2 id="建立一个垃圾邮件分类器"><a href="#建立一个垃圾邮件分类器" class="headerlink" title="建立一个垃圾邮件分类器"></a>建立一个垃圾邮件分类器</h2><h3 id="需要着重考虑的问题"><a href="#需要着重考虑的问题" class="headerlink" title="需要着重考虑的问题"></a>需要着重考虑的问题</h3><ol>
<li>决定x&#x3D;邮件的特征？例如：一系列能区分垃圾&#x2F;非垃圾邮件的单词, deal&#x2F;buy·····出现单词X(i)&#x3D;1，否则x(i)&#x3D;0。</li>
<li>如何改进算法？</li>
</ol>
<ul>
<li>收集大量数据，例如：虚假邮箱收集垃圾邮件</li>
<li>用更复杂的变量，例如：判断邮箱名或邮箱标题、邮箱路由信息</li>
<li>更复杂的算法，例如：学会判断dea1与deal是相同的（有意的拼写错误）</li>
</ul>
<p><em>很难确定哪种方式改进分类器最有效，有时候会随机决定一个方法</em></p>
<h3 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h3><h4 id="推荐的方法"><a href="#推荐的方法" class="headerlink" title="推荐的方法"></a>推荐的方法</h4><ul>
<li>从<em>简单、快速的算法</em>开始，并通过交叉验证检查</li>
<li>画出学习曲线，检验高方差、高偏差，进一步改进</li>
<li>误差分析：看被算法错误分析的样本找到分类错误的规律（被分类错的样本都有什么共同特征），进行改进</li>
<li>最好有数值计算的方法来评估（例如：不知道该不该用新方法时，用交叉验证快速计算新旧方法的错误率，选择错误率低的方法）</li>
</ul>
<h2 id="处理偏斜数据-skewed-data"><a href="#处理偏斜数据-skewed-data" class="headerlink" title="处理偏斜数据(skewed data)"></a>处理偏斜数据(skewed data)</h2><h3 id="对偏差类的误差度量值-Error-Metrics-for-Skewed-Classes"><a href="#对偏差类的误差度量值-Error-Metrics-for-Skewed-Classes" class="headerlink" title="对偏差类的误差度量值(Error Metrics for Skewed Classes)"></a>对偏差类的误差度量值(Error Metrics for Skewed Classes)</h3><p>定义：整个样本偏向于某一类，例如：癌症率测试样本，得癌症的为少数，样本偏向未得癌症</p>
<p>这时候，需要另一种评估度量值</p>
<h4 id="查准率-precision-x2F-召回率-recall"><a href="#查准率-precision-x2F-召回率-recall" class="headerlink" title="查准率(precision)&#x2F;召回率(recall)"></a>查准率(precision)&#x2F;召回率(recall)</h4><p>查准率：如果有一个1、0参杂的测试集，如果我们全预测为1，预测正确的概率</p>
<p>召回率：如果有一个全部属于1的测试集，能准确预测出多少1的概率</p>
<p>查准率和召回率都越高越好</p>
<h3 id="保持召回率和查准率的平衡"><a href="#保持召回率和查准率的平衡" class="headerlink" title="保持召回率和查准率的平衡"></a>保持召回率和查准率的平衡</h3><p>查准率和召回率一般为相反变化。需要高查准率时，可以把界限降低，需要高召回率时则相反。</p>
<p>方案：</p>
<ul>
<li><p>画出x&#x3D;召回率，y&#x3D;查准率的图像</p>
</li>
<li><p>F1值：2x（PxR）&#x2F;（R+P）,F值越高越好，表示查准率和召回率均较高</p>
</li>
</ul>
<h2 id="机器学习的数据"><a href="#机器学习的数据" class="headerlink" title="机器学习的数据"></a>机器学习的数据</h2><p>在有足够特征用于预测y的前提下，大量数据可以很好的改进算法。</p>
<p>如何证明有足够的特征值x？找一个人类专家，如果能通过x预测出y，则其有足够特征。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习 Week7</title>
    <url>/2017/09/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-Week7/</url>
    <content><![CDATA[<h2 id="Large-Margin-Classification"><a href="#Large-Margin-Classification" class="headerlink" title="Large Margin Classification"></a>Large Margin Classification</h2><h3 id="Optimization-Objective"><a href="#Optimization-Objective" class="headerlink" title="Optimization Objective"></a>Optimization Objective</h3><p>支持向量机(Support Vector Machine,SVM)：用于学习复杂的非线性方程的算法</p>
<p>可以从线性的代价函数中推导而出，如下：</p>
<p><img src="https://i.imgur.com/CwCUWST.png"></p>
<p>相比线性函数去掉1&#x2F;m：习惯问题，不影响</p>
<p>将A+λB的形式换为CA+B的形式：控制权重的不同方式</p>
<h3 id="Large-Margin-Intuition"><a href="#Large-Margin-Intuition" class="headerlink" title="Large Margin Intuition"></a>Large Margin Intuition</h3><p>SVM在分类时会以一个最大间距来分类数据，C越大，过拟合的可能性越大，即高方差低偏差小λ。</p>
<h2 id="Kernels-核函数"><a href="#Kernels-核函数" class="headerlink" title="Kernels(核函数)"></a>Kernels(核函数)</h2><p>(高斯)核函数 ：</p>
<p><img src="https://i.imgur.com/grPDbkL.png"></p>
<p>其中f1代替了Θ1+Θ2*x1·····中的x1，而σ越大，特征变量值减小越慢。θ的大小决定了标记点的权重，当x靠近标记点时预测值就会为1，否则为0.</p>
<h3 id="如何选择标记点"><a href="#如何选择标记点" class="headerlink" title="如何选择标记点"></a>如何选择标记点</h3><p>选择每一个训练样本作为标记点</p>
<h3 id="各个参数的影响"><a href="#各个参数的影响" class="headerlink" title="各个参数的影响"></a>各个参数的影响</h3><p>C:越大，过拟合的可能性越大，即高方差低偏差小λ。</p>
<p>σ²:越大则特征变量值f减小越慢且变化平缓，同时高偏差低方差</p>
<h2 id="如何使用SVM"><a href="#如何使用SVM" class="headerlink" title="如何使用SVM"></a>如何使用SVM</h2><p>使用好的软件库，而不是自己编写SVM软件（比如：liblinear,libsvm）</p>
<p>需要设置：</p>
<ul>
<li>参数C</li>
<li>选择核函数（相似度函数），例如：</li>
</ul>
<p><img src="https://i.imgur.com/oheT8ce.png"></p>
<ul>
<li>核函数需要满足的条件：Mercer’s Theorem</li>
</ul>
<h3 id="高斯函数"><a href="#高斯函数" class="headerlink" title="高斯函数"></a>高斯函数</h3><p>什么时候选择高斯核函数：特征变量x的维数n比较小，如果取值范围较大，需要进行归一化。</p>
<p>Matlab需要做的：</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">f</span>=<span class="title">kernel</span><span class="params">(x1,x2)</span></span></span><br><span class="line">	f=<span class="built_in">exp</span>(-delta_x^<span class="number">2</span>/(<span class="number">2</span>*sigma^<span class="number">2</span>)); <span class="comment">%delta_x为x1到x2的距离</span></span><br><span class="line"><span class="keyword">return</span></span><br></pre></td></tr></table></figure>

<h3 id="逻辑函数与SVM的选择"><a href="#逻辑函数与SVM的选择" class="headerlink" title="逻辑函数与SVM的选择"></a>逻辑函数与SVM的选择</h3><p>特征数n比较多（n&gt;1000）：逻辑函数或者不是高斯核函数的SVM</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习 Week8</title>
    <url>/2017/09/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-Week8/</url>
    <content><![CDATA[<h2 id="聚类-clustering"><a href="#聚类-clustering" class="headerlink" title="聚类(clustering)"></a>聚类(clustering)</h2><h3 id="无监督学习-unsupervised-learning"><a href="#无监督学习-unsupervised-learning" class="headerlink" title="无监督学习(unsupervised learning)"></a>无监督学习(unsupervised learning)</h3><p>输入没有标记的数据，通过算法找出结构。</p>
<h3 id="K均值算法-K-Means-Algorithm"><a href="#K均值算法-K-Means-Algorithm" class="headerlink" title="K均值算法(K-Means Algorithm)"></a>K均值算法(K-Means Algorithm)</h3><p>选择k个聚类中心（需要聚k个类），K均值要<strong>迭代</strong>的做两件事：簇分配和移动聚类中心。</p>
<ul>
<li>簇分配：遍历所有样本，根据样本更靠近哪个聚类中心来分类。</li>
<li>移动聚类中心：将两个聚类中心移动到同类型的均值处。</li>
</ul>
<p>最终聚类中心不再移动时，算法完成。</p>
<p>详细步骤：</p>
<p><img src="https://i.imgur.com/qxYf1dZ.png"></p>
<p><em>注意图中K的大小写，小写k是聚类的下标，大写K为聚类的总数</em> ，c(i)的值为x(i)最靠近的那个聚类的编号（下标）。如果出现某个簇没有被分配数据，则删除这个簇，或者重新随机分配一个簇。</p>
<h3 id="优化目标函数-Optimization-Objective"><a href="#优化目标函数-Optimization-Objective" class="headerlink" title="优化目标函数(Optimization Objective)"></a>优化目标函数(Optimization Objective)</h3><p><img src="https://i.imgur.com/Ix6Snsn.png"></p>
<p>优化目标函数也称作失真代价函数(distortion cost function)，实际上，K均值算法的第一步簇分配就是选择c(1)到c(m)来最小化函数J。第二步移动聚类中心则是选择能够最小化函数J的μ值。</p>
<p>注意，函数J只会一直下降，如果出现波动则算法错误。</p>
<h3 id="随机初始化-Random-Initialization"><a href="#随机初始化-Random-Initialization" class="headerlink" title="随机初始化(Random Initialization)"></a>随机初始化(Random Initialization)</h3><p>随机挑选K个样本成为K个聚类中心</p>
<p>如何避免局部最优：初始化10次，得到代价函数J最小的那一个.（K一般为2-10个）</p>
<h3 id="选择聚类数目"><a href="#选择聚类数目" class="headerlink" title="选择聚类数目"></a>选择聚类数目</h3><ul>
<li>肘部法则：画出J-K的折线图，选择a—b—c中a—b变化急剧，b—c变化变缓的b点K值（类似于肘部）。</li>
<li>根据后来的需求决定K，例如：T恤的生产，选择3个S M L或者加上XS XL成为5个。</li>
</ul>
<h2 id="维数约减-dimensionality-reduction"><a href="#维数约减-dimensionality-reduction" class="headerlink" title="维数约减(dimensionality reduction)"></a>维数约减(dimensionality reduction)</h2><h3 id="数据压缩"><a href="#数据压缩" class="headerlink" title="数据压缩"></a>数据压缩</h3><p>将数据从2D降到1D：将所有数据投影到一条直线上</p>
<p>将数据从3D降到2D：将所有数据投影到一个平面上</p>
<h3 id="可视化数据"><a href="#可视化数据" class="headerlink" title="可视化数据"></a>可视化数据</h3><p>把数据从高维降至二、三维，画出点图</p>
<h3 id="主成分分析法-Principal-Component-Analysis-PCA"><a href="#主成分分析法-Principal-Component-Analysis-PCA" class="headerlink" title="主成分分析法(Principal Component Analysis,PCA)"></a>主成分分析法(Principal Component Analysis,PCA)</h3><p>定义：构造一个向量（二维中为两个相互垂直的向量），使得投影点到向量延长直线（平面）的距离之和最小。</p>
<p>与线性规划的区别：线性规划最小化的是点到直线的垂线距离（垂直于x轴）的平方和，而PCA则是最小化点到直线的距离（垂直于直线）和。</p>
<h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><p>特征缩放、均值归一化：</p>
<p><img src="https://i.imgur.com/yhCWTJ2.png"></p>
<h4 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h4><p><img src="https://i.imgur.com/Ol5xupO.png"></p>
<ul>
<li>协方差和求和符号Σ重复，注意区分。</li>
<li>U为nxn矩阵，x为nx1矩阵，设Ureduce为U的前k列矩阵（即为nxk，降至k维），z&#x3D;Ureduce‘*x（kx1矩阵）</li>
</ul>
<h2 id="PCA的应用"><a href="#PCA的应用" class="headerlink" title="PCA的应用"></a>PCA的应用</h2><h3 id="解压缩维度"><a href="#解压缩维度" class="headerlink" title="解压缩维度"></a>解压缩维度</h3><p>因为z&#x3D;Ureduce‘*x用于压缩，所有x&#x3D;Ureduce x z即可用于解压</p>
<h3 id="如何选择K值（降到K维）"><a href="#如何选择K值（降到K维）" class="headerlink" title="如何选择K值（降到K维）"></a>如何选择K值（降到K维）</h3><p>平均平方映射误差&#x2F;数据总偏差&lt;&#x3D;0.01，即保留了99%的差异性。另外常用的数值是&lt;&#x3D;0.05或者&lt;&#x3D;0.10</p>
<p><img src="https://i.imgur.com/rJOMzVC.png"></p>
<p>应用：</p>
<p><img src="https://i.imgur.com/ohE1T4t.png"></p>
<h3 id="一些建议"><a href="#一些建议" class="headerlink" title="一些建议"></a>一些建议</h3><ul>
<li>逻辑回归应用PCA只在训练集中，而不用在交叉验证集使用。</li>
<li>不要用PCA减少特征数来避免过拟合</li>
<li>使用PCA之前，先考虑没有PCA数据是否可以训练（PCA的帮助有多大?）</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习 Week9</title>
    <url>/2017/10/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-Week9/</url>
    <content><![CDATA[<h2 id="异常检测-Anomaly-detection"><a href="#异常检测-Anomaly-detection" class="headerlink" title="异常检测(Anomaly detection)"></a>异常检测(Anomaly detection)</h2><p>简介：通过一系列特征变量X建造一个模型p(x)，若p(x)&lt;ε，则可以认为该数据异常，进行进一步检测。</p>
<p>应用：制造业、用户行为检测、数据中心</p>
<h3 id="高斯分布-正态分布"><a href="#高斯分布-正态分布" class="headerlink" title="高斯分布(正态分布)"></a>高斯分布(正态分布)</h3><p>x~N(μ,σ²)</p>
<p>~表示服从····分布，N表示Normal(正态)，μ表示均值，σ²表示方差（σ为标准差）</p>
<p>μ决定中心位置，σ越小曲线越窄高，但曲线所围面积总为1</p>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>Π：累乘标志</p>
<p>步骤：</p>
<ol>
<li>找出可以用于检测错误的特征x，或者可以描述相关属性的特征x</li>
<li>找出m个无标签数据构成的训练集，拟合出期望与方差值</li>
<li>计算p(x)，如果p(X)&lt;ε，则异常</li>
</ol>
<p>公式如下：</p>
<p><img src="https://i.imgur.com/8q7PaN4.png"></p>
<h2 id="如何开发异常检测系统"><a href="#如何开发异常检测系统" class="headerlink" title="如何开发异常检测系统"></a>如何开发异常检测系统</h2><h3 id="如何评价一个异常检测算法"><a href="#如何评价一个异常检测算法" class="headerlink" title="如何评价一个异常检测算法"></a>如何评价一个异常检测算法</h3><p>以6:2:2的比例将好的样本分配给训练集&#x2F;CV&#x2F;测试集，将坏的样本以1:1的比例分配给CV&#x2F;测试集（有时候训练集会参杂一些坏样本，不影响结果）</p>
<p>方法：</p>
<ol>
<li>用训练集拟合p(x)</li>
<li>用CV&#x2F;测试集的x来预测y，p(x)&gt;&#x3D;ε，y&#x3D;0，正常</li>
</ol>
<p>评估(在CV中)：因为y大量偏向于0，所以分类准确度不是一个好的评估方法。</p>
<p>应该使用召回率&#x2F;查准率、F1-score之类的方法评估</p>
<h3 id="与监督学习对比"><a href="#与监督学习对比" class="headerlink" title="与监督学习对比"></a>与监督学习对比</h3><p>异常检测：</p>
<ul>
<li>大量的负样本（正常）与极少数的正样本（异常）</li>
<li>有很多不同类型的正样本（异常）</li>
<li>可能会出现新的异常种类，所以需要对负样本建模，而不是正样本</li>
</ul>
<p>监督学习：</p>
<ul>
<li>大量的正、负样本</li>
<li>正样本特征均类似，不会出现新的类型</li>
</ul>
<h3 id="特征变量的选择"><a href="#特征变量的选择" class="headerlink" title="特征变量的选择"></a>特征变量的选择</h3><p>如果数据不遵循高斯分布，则要通过取指数、对数的方法转换为高斯分布（通过hist指令画出直方图观察）</p>
<p>常见问题：异常样本的p(x)也很大。</p>
<p>解决：观察被错误判断异常样本，找出新的特征变量。同时可以通过整合两个特征变量形成新的特征变量，例如x3&#x3D;(x1)²&#x2F;x2</p>
<h2 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h2><h3 id="基于内容的推荐"><a href="#基于内容的推荐" class="headerlink" title="基于内容的推荐"></a>基于内容的推荐</h3><p>nu&#x3D;用户数量，nm&#x3D;电影数量。</p>
<p>给予内容的不同特征以不同比重值，加上x0&#x3D;1构成特征矩阵x(x∈n+1阶)。对每个用户j构造参数θ(j)(θ∈n+1阶)，其中Θ(0)&#x3D;0，θ中其他值表示用户对该类型的喜爱程度。则可以预测用户j对电影i有θ(j)‘.*x(i)星。</p>
<p><img src="https://i.imgur.com/gTHyOrL.png"></p>
<p><img src="https://i.imgur.com/7IBt0jX.png"></p>
<h3 id="协同过滤-Collaborative-Filtering"><a href="#协同过滤-Collaborative-Filtering" class="headerlink" title="协同过滤(Collaborative Filtering)"></a>协同过滤(Collaborative Filtering)</h3><p>特点：自动学习需要的特征，即一直θ值，用于学习x的值</p>
<p><img src="https://i.imgur.com/c480rkA.png"></p>
<p>结合内容推荐：先计算θ，再计算x-&gt;θ-&gt;x·······，反复迭代</p>
<h3 id="协同过滤算法"><a href="#协同过滤算法" class="headerlink" title="协同过滤算法"></a>协同过滤算法</h3><p>将x与θ的迭代式结合，一步完成，同时<em>不需要x0和θ0</em>：</p>
<p><img src="https://i.imgur.com/CmX3n0P.png"></p>
<h2 id="协同过滤向量法实现"><a href="#协同过滤向量法实现" class="headerlink" title="协同过滤向量法实现"></a>协同过滤向量法实现</h2><p>Y–用户对每部电影的评分矩阵；X–用户向量；Θ–每行是每个用户的θ；预测矩阵：Θ‘*X</p>
<p><em>协同过滤向量法也被称为 低zhi矩阵分解</em></p>
<h3 id="如何衡量两个电影间相似性？"><a href="#如何衡量两个电影间相似性？" class="headerlink" title="如何衡量两个电影间相似性？"></a>如何衡量两个电影间相似性？</h3><p>计算两部电影间的距离||x(i)-x(j)||</p>
<h3 id="细节：均值归一化"><a href="#细节：均值归一化" class="headerlink" title="细节：均值归一化"></a>细节：均值归一化</h3><p>如果出现未评分的新用户，则先根据先前评分计算出平均评分μ，再将每个用户评分都减去平均评分，最后形成新的评分矩阵Y替代先前的Y。而新用户的评分则在新Y中设置为0。</p>
<p>所以预测矩阵也随之更改为Θ‘*X+μ(平均值)</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习Week1</title>
    <url>/2017/08/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0Week1/</url>
    <content><![CDATA[<h2 id="课程中的一些缩写："><a href="#课程中的一些缩写：" class="headerlink" title="课程中的一些缩写："></a>课程中的一些缩写：</h2><p>m：训练集中样本的数目<br>x’s：输入变量&#x2F;特称变量<br>y’s：输出变量&#x2F;目标变量<br>（x(i),y(i)）:第i个变量<br>h:hypothesis（假设），表示算法输出的函数。一个特有名词，没有为什么，没有。</p>
<h2 id="机器学习的定义"><a href="#机器学习的定义" class="headerlink" title="机器学习的定义"></a>机器学习的定义</h2><p>A computer program is said to learn from experience E with respect to some task T and some performance measure P if its performance on T, as measured by P, improves with experience E.<br>Suppose we feed a learning algorithm a lot of historical weather<br>data, and have it learn to predict weather.</p>
<p>T–需要解决的任务，E–大量的数据（经验），P–性能度量值（评判标准）</p>
<h2 id="监督学习（Supervised-Learning）"><a href="#监督学习（Supervised-Learning）" class="headerlink" title="监督学习（Supervised Learning）"></a>监督学习（Supervised Learning）</h2><p>对于数据集中的每个数据， 都有相应的正确答案，（训练集） 算法就是基于这些来做出预测。分为** 回归问题（Regression） <strong>和</strong> 分类问题（Classification） **<br>回归问题：根据大量数据推测其他具体数据的情况<br>分类问题：是与否，例如：predict whether or not it will be raining at 5pm<br>tomorrow. </p>
<h2 id="无监督学习（Unsupervised-Learning）"><a href="#无监督学习（Unsupervised-Learning）" class="headerlink" title="无监督学习（Unsupervised Learning）"></a>无监督学习（Unsupervised Learning）</h2><p>只有一个一样数据集，没有属性或者标签，通过无监督学习可以判定该数据集所属的聚类（cluster）</p>
<h2 id="代价函数（Cost-Function）与梯度下降算法（Gradient-Descent）"><a href="#代价函数（Cost-Function）与梯度下降算法（Gradient-Descent）" class="headerlink" title="代价函数（Cost Function）与梯度下降算法（Gradient Descent）"></a>代价函数（Cost Function）与梯度下降算法（Gradient Descent）</h2><h3 id="代价函数（Cost-Function）"><a href="#代价函数（Cost-Function）" class="headerlink" title="代价函数（Cost Function）"></a>代价函数（Cost Function）</h3><p><img src="http://i.imgur.com/PMEpNDb.png"><br>目标即为求出最小参数值</p>
<h3 id="梯度下降算法（Gradient-Descent）"><a href="#梯度下降算法（Gradient-Descent）" class="headerlink" title="梯度下降算法（Gradient Descent）"></a>梯度下降算法（Gradient Descent）</h3><p><img src="http://i.imgur.com/0xJmq2X.png"><br>符号:&#x3D;表示将右值赋给左值，符号&#x3D;表示判断左右值是否相同<br>正确的更新方法为同步更新，而不是先更新一个参数再更新另一个参数。<br>α：学习速率（learning rate）表示算法的下降速率，过小导致算法速度太慢，太大容易略过最低点导致无法收敛甚至发散。<br>导数部分：由于斜率越接近谷底越小，所以不用改变alpha也可以下降到谷底。</p>
<h3 id="算法实例：用梯度下降的方法来最小化平方误差代价函数"><a href="#算法实例：用梯度下降的方法来最小化平方误差代价函数" class="headerlink" title="算法实例：用梯度下降的方法来最小化平方误差代价函数"></a>算法实例：用梯度下降的方法来最小化平方误差代价函数</h3><p><img src="http://i.imgur.com/A26A0MK.png"><br>梯度下降通常得到的是局部最优值，但由于用于** 线性回归的代价函数 <strong>总是一个</strong> 凸函数 **，所以使用梯度下降时只有一个全局最优解。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习Week2</title>
    <url>/2017/08/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0Week2/</url>
    <content><![CDATA[<h2 id="一些缩写"><a href="#一些缩写" class="headerlink" title="一些缩写"></a>一些缩写</h2><p>n&#x3D;特征量的数目<br>x(i)&#x3D;第i行特征量的向量<br>x下标j上标i&#x3D;第i行第j个特征量<br>向量：nX1矩阵</p>
<h2 id="多变量的线性回归"><a href="#多变量的线性回归" class="headerlink" title="多变量的线性回归"></a>多变量的线性回归</h2><p>将Week1的梯度下降算法推广到多元向量上<br><img src="http://i.imgur.com/6QxnqDR.png"></p>
<h3 id="缩小范围加速梯度下降"><a href="#缩小范围加速梯度下降" class="headerlink" title="缩小范围加速梯度下降"></a>缩小范围加速梯度下降</h3><h4 id="特征缩放（Feature-scaling）"><a href="#特征缩放（Feature-scaling）" class="headerlink" title="特征缩放（Feature scaling）"></a>特征缩放（Feature scaling）</h4><p>由于特征值的范围会导致特征图是一个高瘦的椭圆形，不利于梯度下降的进行。所以通过将特征值除以其范围的上限，使得特征值范围在-1～1附近（不能过大或者过小），可以使得梯度下降更快且特征图接近圆形。</p>
<h4 id="均值归一化（Mean-normalization）"><a href="#均值归一化（Mean-normalization）" class="headerlink" title="均值归一化（Mean normalization）"></a>均值归一化（Mean normalization）</h4><p>将特征值减去平均值再除以总量（或标准差）。例如：房间大小在0～2000<img src="http://i.imgur.com/EWQeiVc.png"><br>使得所有特征量都有0的平均值  </p>
<h4 id="选择合适的学习率alpha"><a href="#选择合适的学习率alpha" class="headerlink" title="选择合适的学习率alpha"></a>选择合适的学习率alpha</h4><p>alpha也指梯度下降算法的更新规则，可以根据代价很熟随迭代步数增加的变化曲线来判断alpha是否过大或者过小。<br><img src="http://i.imgur.com/WT6iwnn.png"><br>图B表示过小，下降缓慢。<br>C为过大，几乎不下降，甚至有时上升。<br>A较为合适，下降较快。</p>
<h2 id="正规方程（Normal-equation）"><a href="#正规方程（Normal-equation）" class="headerlink" title="正规方程（Normal equation）"></a>正规方程（Normal equation）</h2><p>使用以下公式直接计算<br> <img src="http://i.imgur.com/ruRUVIe.png"><br>其中，X&#x3D;每一行为[1,特征向量x（i）的转置]，共有i行的矩阵。y&#x3D;[目标向量]的转置。<br>** 若出现矩阵不可逆，检查是否有多余特征值并删除 **</p>
<h2 id="正规方程与梯度下降法对比"><a href="#正规方程与梯度下降法对比" class="headerlink" title="正规方程与梯度下降法对比"></a>正规方程与梯度下降法对比</h2><h3 id="正规方程优缺点："><a href="#正规方程优缺点：" class="headerlink" title="正规方程优缺点："></a>正规方程优缺点：</h3><ul>
<li>不需要选择alpha</li>
<li>不需要缩小范围</li>
<li>复杂度为n的三次方（在n为10000内可以接受）</li>
<li>在特征量数目n很大的时候计算慢<h3 id="梯度下降法优缺点："><a href="#梯度下降法优缺点：" class="headerlink" title="梯度下降法优缺点："></a>梯度下降法优缺点：</h3></li>
<li>需要选择alpha</li>
<li>需要缩小范围</li>
<li>在特征量数目n很大的时候也可以运行的很好</li>
<li>复杂度为kn的平方</li>
</ul>
<h2 id="Matlab基础"><a href="#Matlab基础" class="headerlink" title="Matlab基础"></a>Matlab基础</h2><p>who：显示当前空间中所有变量名称</p>
<p>whos：显示当前空间中所有变量名称和详细信息</p>
<p>clear:清除</p>
<p>save 文件名.mat&#x2F;.txt 变量名：将变量保存到文件中。</p>
<p>load 文件名.mat&#x2F;.txt：将文件中的变量加载</p>
<p>** save load要注意当前所在文件位置;mat为一种压缩形式 **</p>
<h3 id="基础运算"><a href="#基础运算" class="headerlink" title="基础运算"></a>基础运算</h3><p>～&#x3D;不等于；&#x3D;&#x3D;等于；^n—n次方；&amp;&amp; 逻辑与；|| 逻辑或；XOR(  ,  ) 逻辑异或；abs（）绝对值；exp（）；log（）以10为底</p>
<h3 id="赋值"><a href="#赋值" class="headerlink" title="赋值"></a>赋值</h3><p>加上分号：不打印赋值；</p>
<p>字符串：a&#x3D;‘   ’</p>
<p>disp()：控制输出格式，例如disp(sprintf(‘   ‘,%0.2a))；类似C的格式，可以直接打印字符串</p>
<h3 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h3><h4 id="生成矩阵"><a href="#生成矩阵" class="headerlink" title="生成矩阵"></a>生成矩阵</h4><p>A&#x3D;[1 2; 3 4; 5 6]，分号表示切换到下一行，空格表示本行下一个数字。</p>
<p>递增行向量：a&#x3D;b:c:d，表示a为第一个数字是b，最后一个数字是d，依次以c递增。</p>
<p>c*ones(a,b):生成全为c的a行b列矩阵</p>
<p>zeros(a,b):生成a行b列0矩阵</p>
<p>rand(a,b)：生成随机位于（0,1）之间的a行b列矩阵。randn(a,b)：标准差为1的a行b列矩阵；</p>
<p>eye(a):生成a阶单位阵</p>
<p>A+a：A所有元素均加上a</p>
<p>A&#96;：A的转置矩阵</p>
<p>[a,b]&#x3D;max(A)(A为行向量)：a得到A中最大值，b为最大值所在位置的引索值；max（max（A））：找到整个A矩阵的最大元素</p>
<p>A&lt;a:依次判断每个元素是否小于a，得到一个和a行列数相同由1，0构成的矩阵</p>
<p>find(A&lt;a):找到小于a的元素，返回其引索值（从上到下，从左到右）</p>
<p>magic（a）：返回a阶的魔法矩阵。魔法矩阵：每行、每列、对角线相加都为同一个数</p>
<p>** 可以当作二重数组有A(a,b)或者A(a,b)的用法 **</p>
<h4 id="矩阵运算"><a href="#矩阵运算" class="headerlink" title="矩阵运算"></a>矩阵运算</h4><p>size(A)：返回矩阵大小，即返回一个1Xn矩阵；size(A,1):返回矩阵行数；size(A,2):返回矩阵列数</p>
<p>length(A):返回max(行数，列数)</p>
<p>A(a,b):引索到位于矩阵A的a行b列元素；A(a,:)：引索到A的a行所有元素，即返回一个1Xn矩阵。;A([a b];:)：取得A的第a行和第b行所有元素，返回2Xn矩阵。</p>
<p>A(:,a)&#x3D;B:将A矩阵的第a列替换为B列向量</p>
<p>C&#x3D;[A B]：将A B左右拼接生成C矩阵</p>
<p>C&#x3D;[A;B]：将A B上下拼接生成C</p>
<p>** AB不一定要为变量，可以是直接输入的矩阵，例如C&#x3D;[A;[1;2]] **</p>
<p>A(:)：将A矩阵中所有元素放入一个列向量</p>
<p>A.×B(点乘)：A、B必须为相同行列数的矩阵，生成的矩阵是A、B对应数相乘的结果。* A.^2可以使用，但A^2只能在A是方阵的前提下使用 *</p>
<p>sum(A):将A中所有元素相加；sum（A，1）：每列分别相加；sum（A，2）：每行分别相加</p>
<p>floor（A）：直接砍掉小数；ceil（A）：四舍五入的砍掉小数</p>
<h3 id="绘图"><a href="#绘图" class="headerlink" title="绘图"></a>绘图</h3><p>plot(a,b,c···):a参数表示x轴量，b表y轴，c表颜色之类可选项</p>
<p>xlabel（’ ‘）&#x2F;ylabel（’ ‘）：字符串命名x、y轴</p>
<p>legend（’ ‘,····）:标记曲线</p>
<p>title(‘ ‘):标记图表名字</p>
<p>figure(a):指定绘画</p>
<p>在第a张图上</p>
<p>subplot(a,b,c):将图标分成a*b的格子，显示并使用第c个格子</p>
<p>axis（[a b c d]）:设置x轴范围[a,b]，y轴范围[c,d];</p>
<p>clf：清除所有图像</p>
<p>imagesc(A)：用不同颜色显示A的元素，颜色相同则数值相同。连招：imagesc(A),colorbar,colormap gray:用灰度层显示A的元素。* 用逗号隔开的命令会依次执行 *</p>
<h3 id="控制语句"><a href="#控制语句" class="headerlink" title="控制语句"></a>控制语句</h3><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=a:b,</span><br><span class="line">  something;</span><br><span class="line"><span class="keyword">end</span>;</span><br></pre></td></tr></table></figure>

<p>有break、continue；</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="built_in">i</span>&lt;=a, <span class="comment">%可以直接填写while true/false,与break配合</span></span><br><span class="line">	something;</span><br><span class="line"><span class="keyword">end</span>;</span><br></pre></td></tr></table></figure>

<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> a==b,</span><br><span class="line">	something;</span><br><span class="line"><span class="keyword">elseif</span> a~=b,</span><br><span class="line">	something;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">	something;</span><br><span class="line"><span class="keyword">end</span>;</span><br></pre></td></tr></table></figure>

<h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><p>function [y1,y2,·····]&#x3D;函数名（x1,x2,·····） %y为返回值，x为变量名</p>
<p>something;</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Matlab</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习Week3</title>
    <url>/2017/08/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0Week3/</url>
    <content><![CDATA[<h2 id="分类问题（classification-problems）"><a href="#分类问题（classification-problems）" class="headerlink" title="分类问题（classification problems）"></a>分类问题（classification problems）</h2><p>y&#x3D;0 or 1</p>
<h3 id="回归分析-x2F-逻辑分析-logistic-regression"><a href="#回归分析-x2F-逻辑分析-logistic-regression" class="headerlink" title="回归分析&#x2F;逻辑分析(logistic regression):"></a>回归分析&#x2F;逻辑分析(logistic regression):</h3><p>目标：令h(x)位于[0,1]之间</p>
<h4 id="逻辑函数-x2F-S型函数"><a href="#逻辑函数-x2F-S型函数" class="headerlink" title="逻辑函数&#x2F;S型函数:"></a>逻辑函数&#x2F;S型函数:</h4><p><img src="http://i.imgur.com/ZjWOYQy.png"></p>
<p>图像:</p>
<p><img src="http://i.imgur.com/flCI9zQ.png"></p>
<p>x&#x3D;0，y&#x3D;0.5；x&#x3D;正无穷，y&#x3D;1；x&#x3D;负无穷，y&#x3D;0；</p>
<p>概率角度：P(y&#x3D;0|x;θ)+P(y&#x3D;1|x;θ)&#x3D;1，P(y&#x3D;1|x;θ)表示在给定x数值时y&#x3D;1的概率。</p>
<p>由图可知，要使得h&gt;0(y&#x3D;1),就要z&gt;0，所以是theta定义了决策边界，而训练集用于拟合参数theta</p>
<h3 id="一对多问题（One-vs-all）"><a href="#一对多问题（One-vs-all）" class="headerlink" title="一对多问题（One-vs-all）"></a>一对多问题（One-vs-all）</h3><p>若有个分类，则将问题分成n个1&#x2F;0简单分类问题。每个简单分类问题中，1代表n类中的一类，0代表剩余的n-1类。</p>
<p>计算出每个简单分类问题y&#x3D;1的概率，最终概率即为n个概率中该分类概率最大的。</p>
<p><img src="http://i.imgur.com/TxtyNGW.png"></p>
<h2 id="高级优化算法-Advanced-Optimization"><a href="#高级优化算法-Advanced-Optimization" class="headerlink" title="高级优化算法(Advanced Optimization)"></a>高级优化算法(Advanced Optimization)</h2><p>如：BFGS（变尺度法）、L-BFGS（限制变尺度法）、Conjugate gradient(共轭梯度法)</p>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul>
<li>不用手动选择α，内部有智能的线性搜索算法（line search）,可以自动尝试各种α</li>
<li>速度比梯度下降法快</li>
<li>缺点只有复杂</li>
</ul>
<h3 id="在Matlab使用高级优化算法"><a href="#在Matlab使用高级优化算法" class="headerlink" title="在Matlab使用高级优化算法"></a>在Matlab使用高级优化算法</h3><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[jVal, gradient]</span> = <span class="title">costFunction</span><span class="params">(theta)</span></span></span><br><span class="line">  jVal = [...code to compute J(theta)...];</span><br><span class="line">  gradient = [...code to compute derivative of J(theta)...]; <span class="comment">%J(theta)对theta偏导数</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>

<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">options = optimset(<span class="string">&#x27;GradObj&#x27;</span>, <span class="string">&#x27;on&#x27;</span>, <span class="string">&#x27;MaxIter&#x27;</span>, <span class="number">100</span>); <span class="comment">%储存option的数据结构，&#x27;GradObj&#x27;, &#x27;on&#x27;设置梯度目标参数为on打开，&#x27;MaxIter&#x27;, 100最大迭代次数。</span></span><br><span class="line">initialTheta = <span class="built_in">zeros</span>(<span class="number">2</span>,<span class="number">1</span>); <span class="comment">%theta的最初猜想值</span></span><br><span class="line">   [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options); <span class="comment">%fminunc是无约束最小化函数，@为指向costFunction的指针。</span></span><br></pre></td></tr></table></figure>

<p>运行结果中exitFlag&#x3D;1表示已经收敛，此时functionVal的值接近0</p>
<p><em>θ必须是二维及以上列向量</em></p>
<h2 id="过度拟合-overfitting"><a href="#过度拟合-overfitting" class="headerlink" title="过度拟合(overfitting)"></a>过度拟合(overfitting)</h2><p><img src="http://i.imgur.com/sYHIu2V.png"></p>
<p>图一表示未拟合(underfitting)或者HIgh bios(高偏差)；图三表示表示过度拟合或者高方差(hegh variance)。</p>
<p>定义：在有很多数据的情况下，曲线可能很好的拟合已有曲线。但是无法泛化(generate)新数据。</p>
<p>解决：1.减少选取的变量数。人工选取更重要的变量或者用模型选择算法自动选择变量。</p>
<p>2.正规化(regularization)</p>
<p>保存所有变量，不过减少数量级或者θ(j)的大小</p>
<h3 id="正规化"><a href="#正规化" class="headerlink" title="正规化"></a>正规化</h3><p>通过’惩罚’某些参数，可以使得曲线更加接近合适的曲线。为了使正规化更好的运行，需要选择合适的λ参数。</p>
<p><img src="http://i.imgur.com/wwybISj.png"></p>
<p>同样，正规化改变了J(θ)，也要改变梯度下降和正规方程算法中θ的递归式。在正规方程算法中，只要λ&gt;0，则矩阵可逆。</p>
<hr>
<p>正文结束</p>
<hr>
<p>一点吐槽：用了chrome和印象笔记快两年了，今天才发现印象笔记剪藏chrome插件有多好用，coursera上的文本基本都能一键收藏啊，还不用每次为了做笔记辛苦弄公式传图片。</p>
<p>大概以后就可以少写很多笔记了吧（各种偷懒）。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Matlab</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习Week5</title>
    <url>/2017/09/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0Week5/</url>
    <content><![CDATA[<h2 id="代价函数-Cost-Function-和-反向传播-Backpropagation"><a href="#代价函数-Cost-Function-和-反向传播-Backpropagation" class="headerlink" title="代价函数(Cost Function) 和 反向传播(Backpropagation)"></a>代价函数(Cost Function) 和 反向传播(Backpropagation)</h2><h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h3><h4 id="一些缩写"><a href="#一些缩写" class="headerlink" title="一些缩写"></a>一些缩写</h4><ul>
<li>L &#x3D; 总层数</li>
<li>sl &#x3D; l层的单元数（不包括偏置单元）</li>
<li>K &#x3D; 输出单元数</li>
</ul>
<h4 id="代价函数-1"><a href="#代价函数-1" class="headerlink" title="代价函数"></a>代价函数</h4><p><img src="https://i.imgur.com/MPK5FaW.png"></p>
<p>由逻辑回归的代价函数推广得到神经网络的代价函数。其中，前半部分∑k&#x3D;1 K表示h(x)的所有维（输出单元数）相加，后半部分两个∑表示所有单元的θ之和。</p>
<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>最小化代价函数的一种方式，也是通过计算J(θ)的导数来实现。具体方式如下：</p>
<p><img src="https://i.imgur.com/mdbVRtz.png"></p>
<p>深入理解：<br>$$<br>J(\Theta) &#x3D; - \frac{1}{m} \sum_{t&#x3D;1}^m\sum_{k&#x3D;1}^K \left[ y^{(t)}<em>k \ \log (h_\Theta (x^{(t)}))<em>k + (1 - y^{(t)}<em>k)\ \log (1 - h_\Theta(x^{(t)})<em>k)\right] + \frac{\lambda}{2m}\sum</em>{l&#x3D;1}^{L-1} \sum</em>{i&#x3D;1}^{s_l} \sum</em>{j&#x3D;1}^{s_l+1} ( \Theta</em>{j,i}^{(l)})^2 \<br>cost(t) &#x3D;y^{(t)} \ \log (h_\Theta (x^{(t)})) + (1 - y^{(t)})\ \log (1 - h_\Theta(x^{(t)})) \<br>delta_j^{(l)} &#x3D; \dfrac{\partial}{\partial z_j^{(l)}} cost(t)<br>$$</p>
<h2 id="反向传播算法的实现"><a href="#反向传播算法的实现" class="headerlink" title="反向传播算法的实现"></a>反向传播算法的实现</h2><h3 id="将θ从矩阵展开为向量"><a href="#将θ从矩阵展开为向量" class="headerlink" title="将θ从矩阵展开为向量"></a>将θ从矩阵展开为向量</h3><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">thetaVec=[theta1(:);theta2(:)]; <span class="comment">%矩阵(:)表示将矩阵中元素取出并排列成向量</span></span><br><span class="line">theta1=<span class="built_in">reshape</span>(thetaVec(<span class="number">1</span>:<span class="number">110</span>),<span class="number">10</span>,<span class="number">11</span>); <span class="comment">%将向量thetaVec中1——110元素取出并排列成10*11矩阵</span></span><br></pre></td></tr></table></figure>

<p><img src="https://i.imgur.com/c7Dly86.png"></p>
<h3 id="梯度检验-gradient-checking"><a href="#梯度检验-gradient-checking" class="headerlink" title="梯度检验(gradient checking)"></a>梯度检验(gradient checking)</h3><p>有时候反向传播算法表面上可以计算，实际上存在一些bug，这时候就要哟用到梯度检验来检测以减少错误。</p>
<p><img src="https://i.imgur.com/WsaeBGQ.png"></p>
<p>matlab代码如下：</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">epsilon = <span class="number">1e-4</span>;</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:n,</span><br><span class="line">  thetaPlus = theta;</span><br><span class="line">  thetaPlus(<span class="built_in">i</span>) += epsilon;</span><br><span class="line">  thetaMinus = theta;</span><br><span class="line">  thetaMinus(<span class="built_in">i</span>) -= epsilon;</span><br><span class="line">  gradApprox(<span class="built_in">i</span>) = (J(thetaPlus) - J(thetaMinus))/(<span class="number">2</span>*epsilon)</span><br><span class="line"><span class="keyword">end</span>;</span><br></pre></td></tr></table></figure>

<p>若与反向传播算法计算出的导数结果相似，则正确。</p>
<h4 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h4><ul>
<li>一旦用梯度检验检测算法没有错误后，就要把梯度检验关掉，再正式进行分类器的迭代训练。否则，整个算法会变得很慢。</li>
</ul>
<h3 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h3><p>讨论了θ的初始值(initialtheta)，由于将θ全部初始化为0的时候，θ的值会无法更新且始终为0，同样其他值也永远不会改变，所以要进行θ的随机初始化（也称为打破对称）。</p>
<p><img src="https://i.imgur.com/Wopi5p0.png"></p>
<p>matlab实现：</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="comment">%If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11.</span></span><br><span class="line"></span><br><span class="line">Theta1 = <span class="built_in">rand</span>(<span class="number">10</span>,<span class="number">11</span>) * (<span class="number">2</span> * INIT_EPSILON) - INIT_EPSILON;</span><br><span class="line">Theta2 = <span class="built_in">rand</span>(<span class="number">10</span>,<span class="number">11</span>) * (<span class="number">2</span> * INIT_EPSILON) - INIT_EPSILON;</span><br><span class="line">Theta3 = <span class="built_in">rand</span>(<span class="number">1</span>,<span class="number">11</span>) * (<span class="number">2</span> * INIT_EPSILON) - INIT_EPSILON;</span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="单元以及层数的选择"><a href="#单元以及层数的选择" class="headerlink" title="单元以及层数的选择"></a>单元以及层数的选择</h3><p>输入单元：x的个数</p>
<p>输出单元：分类器y的个数，同时需要将y写成向量形式</p>
<p>隐藏层数：默认选择只有一个隐藏层，如果一定要有多个隐藏层，则每个隐藏层的单元数要求相同</p>
<p>隐藏层的单元数：一般来说略大于输入单元的个数</p>
<h3 id="神经网络训练过程"><a href="#神经网络训练过程" class="headerlink" title="神经网络训练过程"></a>神经网络训练过程</h3><ol>
<li>随机初始化权值(weight)，一般初始化为很小接近于的数</li>
<li>执行向前传播算法，计算h(x)</li>
<li>计算代价函数J(θ)</li>
<li>使用向后传播算法计算J(θ)的偏微分</li>
<li>利用梯度检查检测反向传播算法，并在检测后删除梯度检查代码</li>
<li>用梯度下降法或者其他高级优化算法最小化J(θ)并得到θ值</li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习错题</title>
    <url>/2017/08/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%94%99%E9%A2%98/</url>
    <content><![CDATA[<ol>
<li>Suppose you have the following training set, and fit a logistic regression classifier hθ(x)&#x3D;g(θ0+θ1x1+θ2x2).</li>
</ol>
<p><img src="http://i.imgur.com/hIkTIJ8.png"></p>
<p><img src="http://i.imgur.com/jDYmKyf.png"></p>
<p>Which of the following are true? Check all that apply.</p>
<ul>
<li><p>J(θ) will be a convex function, so gradient descent should converge to the global minimum.</p>
</li>
<li><p>**Adding polynomial features (e.g., instead using hθ(x)&#x3D;g(θ0+θ1x1+θ2x2+θ3x21+θ4x1x2+θ5x22) ) could increase how well we can fit the training data. **</p>
</li>
<li><p>**The positive and negative examples cannot be separated using a straight line. So, gradient descent will fail to converge. **</p>
</li>
<li><p>Because the positive and negative examples cannot be separated using a straight line, linear regression will perform as well as logistic regression on this data.</p>
</li>
<li><p>**Adding polynomial features (e.g., instead using hθ(x)&#x3D;g(θ0+θ1x1+θ2x2+θ3x21+θ4x1x2+θ5x22) ) could increase how well we can fit the training data. **</p>
</li>
<li><p><strong>At the optimal value of θ (e.g., found by fminunc), we will have J(θ)≥0.</strong></p>
</li>
<li><p>Adding polynomial features (e.g., instead using hθ(x)&#x3D;g(θ0+θ1x1+θ2x2+θ3x21+θ4x1x2+θ5x22) ) would increase J(θ)because we are now summing over more terms.</p>
</li>
<li><p>If we train gradient descent for enough iterations, for some examples x(i) in the training set it is possible to obtain hθ(x(i))&gt;1.</p>
<p>​</p>
</li>
</ul>
<ol start="2">
<li>Which of the following statements are true? Check all that apply.</li>
</ol>
<ul>
<li><strong>The cost function J(θ) for logistic regression trained with m≥1 examples is always greater than or equal to zero.</strong></li>
<li>For logistic regression, sometimes gradient descent will converge to a local minimum (and fail to find the global minimum). This is the reason we prefer more advanced optimization algorithms such as fminunc (conjugate gradient&#x2F;BFGS&#x2F;L-BFGS&#x2F;etc).</li>
</ul>
<p><em>【解析】not for this reason, those three ads faster than gradient descent and you don’t need to manully pick alpha.</em></p>
<ul>
<li><strong>The one-vs-all technique allows you to use logistic regression for problems in which each y(i) comes from a fixed, discrete set of values.</strong></li>
<li>Since we train one classifier when there are two classes, we train two classifiers when there are three classes (and we do one-vs-all classification).</li>
</ul>
<p><em>【解析】we train one classifier for each class</em></p>
<ul>
<li><p>Linear regression always works well for classification if you classify by using a threshold on the prediction made by linear regression.</p>
</li>
<li><p><strong>The cost function J(θ) for logistic regression trained with m≥1 examples is always greater than or equal to zero.</strong></p>
</li>
<li><p><strong>The sigmoid function g(z)&#x3D;1&#x2F;(1+e^(−z)) is never greater than one (&gt;1).</strong></p>
<p>​</p>
</li>
</ul>
<ol start="3">
<li><p>Which of the following are reasons for using feature scaling?<br>It speeds up gradient descent by making it require fewer iterations to get to a good solution.<br><em>【解析】Feature scaling speeds up gradient descent by avoiding many extra iterations that are required when one or more features take on much larger values than the rest. The cost function J(θ) for linear regression has no local optima.The magnitude of the feature values are insignificant in terms of computational cost.</em> </p>
<p>​</p>
</li>
<li><p>You are training a classification model with logistic regression. Which of the following statements are true? Check all that apply.【D】</p>
<p>A. Introducing regularization to the model always results in equal or better performance on the training set.</p>
<p><em>【解析】If we introduce too much regularization, we can underfit the training set and have worse performance on the training set.</em></p>
<p>​           B.Adding many new features to the model helps prevent overfitting on the training set.</p>
<p><em>【解析】Adding many new features gives us more expressive models which are able to better fit our training set. If too many new features are added, this can lead to overfitting of the training set.</em></p>
<p>C. Adding a new feature to the model always results in equal or better performance on examples not in<br>the training set.</p>
<p><em>【解析】Adding  more features might result in a model that overfits the training set, and thus can lead to worse performs for examples which are not in the training set.</em></p>
<p><strong>D.Adding a new feature to the model always results in equal or better performance on the training set.</strong></p>
<p><em>【解析】By adding a new feature, our model must be more (or just as) expressive, thus allowing it learn more complex hypotheses to fit the training set.</em></p>
</li>
<li><p>Which of the following statements are true? Check all that apply.【ABD】</p>
<p>A. If a learning algorithm is suffering from high bias, only adding more training examples may <strong>not</strong> improve the test error significantly.</p>
<p>B. If a learning algorithm is suffering from high variance, adding more training examples is likely to improve the test error.</p>
<p>C. We always prefer models with high variance (over those with high bias) as they will able to better fit the training set.</p>
<p>D. When debugging learning algorithms, it is useful to plot a learning curve to understand if there is a high bias or high variance problem.</p>
</li>
<li><p>Which of the following statements are true? Check all that apply.【AC】</p>
<p>The “error analysis” process of manually examining the examples which your algorithm got wrong can help suggest what are good steps to take (e.g.,developing new features) to improve your algorithm’s performance.</p>
<p>It is a good idea to spend a lot of timecollecting a <strong>large</strong> amount of data before buildingyour first version of a learning algorithm.</p>
<p>Using a <strong>very large</strong> training setmakes it unlikely for model to overfit the trainingdata.</p>
<p>After training a logistic regressionclassifier, you <strong>must</strong> use 0.5 as your thresholdfor predicting whether an example is positive ornegative.</p>
<p>If your model is under fitting the training set, then obtaining more data is likely to help.</p>
</li>
<li><p>Suppose you have a dataset with n &#x3D; 10 features and m &#x3D; 5000 examples. After training your logistic regression classifier with gradient descent, you find that it has underfit the training set and does not achieve the desired performance on the training or cross validation sets. Which of the following might be promising steps to take? Check all that apply.</p>
<table>
<thead>
<tr>
<th>Your Answer</th>
<th align="center"></th>
<th align="center">Score</th>
<th align="left">Explanation</th>
</tr>
</thead>
<tbody><tr>
<td>Use a different optimization method since using gradient descent to train logistic regression might result in a local minimum.</td>
<td align="center">Not Correct</td>
<td align="center">0</td>
<td align="left">The logistic regression cost function is convex, so gradient descent will always find the global minimum.</td>
</tr>
<tr>
<td>Try using a neural network with a large number of hidden units.</td>
<td align="center">Correct</td>
<td align="center">0.5</td>
<td align="left">A neural network with many hidden units is a more complex (higher variance) model than logistic regression, so it is less likely to underfit the data.</td>
</tr>
<tr>
<td>Reduce the number of examples in the training set.</td>
<td align="center">Not Correct</td>
<td align="center">0</td>
<td align="left">While you can improve accuracy on the training set by removing examples, doing so results in a worse model that will not generalize as well.</td>
</tr>
<tr>
<td>Use an SVM with a Gaussian Kernel.</td>
<td align="center">Correct</td>
<td align="center">0.5</td>
<td align="left">By using a Gaussian kernel, your model will have greater complexity and can avoid underfitting the data.</td>
</tr>
</tbody></table>
</li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习Week4</title>
    <url>/2017/08/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0Week4/</url>
    <content><![CDATA[<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p> 模拟大脑，用于解决样本量大的非线性分类问题。θ&#x3D;模型参数&#x3D;权重，表示一个波矩阵，起到控制层与层之间的作用。矩阵大小：下一层特征量数X（本层特征量数+1）</p>
<p>layer1输入层（输入特征项x0,x1，…,xn）——&gt;layer2,3,…,n-1隐藏层（激励函数a0,a1,…,an）——&gt;layern输出层(最终的一个神经元)。</p>
<p>其中，x0&#x3D;1,a0&#x3D;1被称为bios unit（偏置单元），可以选择性画出。</p>
<p><img src="https://i.imgur.com/FKg3QPA.png"></p>
<h3 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h3><p>依据cost公式通过向前传播的方法依层计算</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
